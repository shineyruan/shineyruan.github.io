<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>CAD2CAV: Computer Aided Design for Cooperative Autonomous Vehicles - Ryan's Blog</title><meta name=Description content="Yet another documentation of life."><meta property="og:title" content="CAD2CAV: Computer Aided Design for Cooperative Autonomous Vehicles"><meta property="og:description" content="CAD2CAV is a project focusing on multi-agent exploration in unknown environments. It attempts to build a complete system from perception to planning and control, exploring a designated unknown environments with multiple autonomous vehicles. It is built in xLab at the University of Pennsylvania, on multiple F1TENTH race cars.
Useful documents:
GitHub link Spring 2021 Final Report (not complete) Demo This is a sample scenario of a single F1TENTH race car exploring the 2nd floor of Levine Hall at the University of Pennsylvania, right outside xLab."><meta property="og:type" content="article"><meta property="og:url" content="https://zhihaoruan.xyz/cad2cav/"><meta property="og:image" content="https://zhihaoruan.xyz/logo.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-12-27T23:12:37+00:00"><meta property="article:modified_time" content="2023-03-30T01:18:30-07:00"><meta property="og:site_name" content="Ryan's Blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://zhihaoruan.xyz/logo.png"><meta name=twitter:title content="CAD2CAV: Computer Aided Design for Cooperative Autonomous Vehicles"><meta name=twitter:description content="CAD2CAV is a project focusing on multi-agent exploration in unknown environments. It attempts to build a complete system from perception to planning and control, exploring a designated unknown environments with multiple autonomous vehicles. It is built in xLab at the University of Pennsylvania, on multiple F1TENTH race cars.
Useful documents:
GitHub link Spring 2021 Final Report (not complete) Demo This is a sample scenario of a single F1TENTH race car exploring the 2nd floor of Levine Hall at the University of Pennsylvania, right outside xLab."><meta name=application-name content="Ryan's Blog"><meta name=apple-mobile-web-app-title content="Ryan's Blog"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://zhihaoruan.xyz/cad2cav/><link rel=prev href=https://zhihaoruan.xyz/565-final-project/><link rel=next href=https://zhihaoruan.xyz/newcastle/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=/lib/fontawesome-free/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/fontawesome-free/all.min.css></noscript><link rel=preload href=/lib/animate/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=/lib/animate/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"CAD2CAV: Computer Aided Design for Cooperative Autonomous Vehicles","inLanguage":"en","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/zhihaoruan.xyz\/cad2cav\/"},"genre":"posts","keywords":"f1tenth, motion planning, exploration, robotics, Multi-lingual","wordcount":4357,"url":"https:\/\/zhihaoruan.xyz\/cad2cav\/","datePublished":"2021-12-27T23:12:37+00:00","dateModified":"2023-03-30T01:18:30-07:00","license":"This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"Zhihao Ruan"},"description":""}</script></head><body data-header-desktop=auto data-header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="Ryan's Blog">Ryan's Blog</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/categories/projects/>Projects </a><a class=menu-item href=/categories/my-gallery/>My Gallery </a><a class=menu-item href=/posts/>Posts </a><a class=menu-item href=/tags/>Tags </a><a class=menu-item href=/categories/>Categories </a><a class=menu-item href=/about/>About </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder="Search titles or contents..." id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span>
</span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="menu-item language" title="Select Language"><i class="fa fa-globe" aria-hidden=true></i>
<select class=language-select id=language-select-desktop onchange="location=this.value"><option value=/cad2cav/ selected>English</option></select></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="Ryan's Blog">Ryan's Blog</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder="Search titles or contents..." id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>Cancel</a></div><a class=menu-item href=/categories/projects/ title>Projects</a><a class=menu-item href=/categories/my-gallery/ title>My Gallery</a><a class=menu-item href=/posts/ title>Posts</a><a class=menu-item href=/tags/ title>Tags</a><a class=menu-item href=/categories/ title>Categories</a><a class=menu-item href=/about/ title>About</a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class=menu-item title="Select Language"><i class="fa fa-globe fa-fw" aria-hidden=true></i>
<select class=language-select onchange="location=this.value"><option value=/cad2cav/ selected>English</option></select></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">CAD2CAV: Computer Aided Design for Cooperative Autonomous Vehicles</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=/ title=Author rel=author class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>Zhihao Ruan</a></span>&nbsp;<span class=post-category>included in <a href=/categories/projects/><i class="far fa-folder fa-fw" aria-hidden=true></i>Projects</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime=2021-12-27>2021-12-27</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;4357 words&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;21 minutes&nbsp;</div></div><div class=featured-image><img class=lazyload src=/svg/loading.min.svg data-src=/images/posts/cad2cav/cad2cav-overview.png data-srcset="/images/posts/cad2cav/cad2cav-overview.png, /images/posts/cad2cav/cad2cav-overview.png 1.5x, /images/posts/cad2cav/cad2cav-overview.png 2x" data-sizes=auto alt=/images/posts/cad2cav/cad2cav-overview.png title=/images/posts/cad2cav/cad2cav-overview.png width=2972 height=1192></div><div class="details toc" id=toc-static data-kept><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#demo>Demo</a></li><li><a href=#motivation>Motivation</a></li><li><a href=#technical-details>Technical Details</a><ul><li><a href=#overview>Overview</a></li><li><a href=#building-renderer>Building Renderer</a><ul><li><a href=#other-options>Other Options</a></li></ul></li><li><a href=#central-ros-server>Central ROS Server</a><ul><li><a href=#initial-map-construction-from-revit>Initial Map Construction from Revit</a></li><li><a href=#waypoint-identification-and-registration>Waypoint Identification and Registration</a></li><li><a href=#graph-planning-and-routing>Graph Planning and Routing</a><ul><li><a href=#the-capacitated-vehicle-routing-problem-cvrp>The Capacitated Vehicle Routing Problem (CVRP)</a></li><li><a href=#spectral-clustering>Spectral Clustering</a></li><li><a href=#multi-level-k-way-graph-partitioning>Multi-level $k$-Way Graph Partitioning</a></li><li><a href=#graph-planning-summary-and-demo>Graph Planning Summary and Demo</a></li></ul></li></ul></li><li><a href=#f1tenth-onboard-computing-platform>F1TENTH Onboard Computing Platform</a><ul><li><a href=#obstacle-avoiding-planning-and-control>Obstacle-Avoiding Planning and Control</a></li><li><a href=#localization>Localization</a><ul><li><a href=#overview-1>Overview</a></li><li><a href=#localization-1>Localization</a></li><li><a href=#landmark-detection>Landmark Detection</a></li><li><a href=#landmark-projection-and-matching>Landmark Projection and Matching</a></li><li><a href=#code-and-demo-videos>Code and Demo Videos</a></li></ul></li></ul></li></ul></li><li><a href=#references>References</a></li></ul></nav></div></div><div class=content id=content><p>CAD2CAV is a project focusing on multi-agent exploration in unknown environments. It attempts to build a complete system from perception to planning and control, exploring a designated unknown environments with multiple autonomous vehicles. It is built in <a href=https://mlab-upenn.github.io/lab_website/ target=_blank rel="noopener noreffer">xLab at the University of Pennsylvania</a>, on multiple <a href=https://f1tenth.readthedocs.io/en/latest/ target=_blank rel="noopener noreffer">F1TENTH race cars</a>.</p><p>Useful documents:</p><ul><li><a href=https://github.com/mlab-upenn/ISP2021-cad2cav target=_blank rel="noopener noreffer">GitHub link</a></li><li><a href=/files/CAD2CAV_Project_Report.pdf rel>Spring 2021 Final Report (not complete)</a></li></ul><h2 id=demo>Demo</h2><p>This is a sample scenario of a single F1TENTH race car exploring the 2nd floor of Levine Hall at the University of Pennsylvania, right outside xLab.</p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube-nocookie.com/embed/CMwnMAKMueA style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><h2 id=motivation>Motivation</h2><p>Autonomous robots have been widely used in a great number of aspects in our daily life. Specifically, mobile robots have demonstrated great help in unknown environment exploration, i.e., rescue robots exploring debris of a building after an earthquake, scientific robots exploring the world under the sea, etc. We are particularly interested in developing a fleet of mobile robots based on the dynamics of a normal self-driving vehicle (i.e., F1TENTH race cars) that helps people explore normal in-use buildings, or buildings that are still under construction.</p><figure><img src=f1tenth_NX.png alt="Sample image of an F1TENTH race car." width=400><figcaption><p>Sample image of an F1TENTH race car.</p></figcaption></figure><p>Normally, one would quickly think of SLAM (Simultaneous Localization and Mapping) when it comes to robotic exploration (for more details, see my <a href=https://zhihaoruan.xyz/467proj-slam/ rel>SLAM project</a>). Yes, that is one of the most straightforward and simplest solution for a normal robotic exploration task. However, under the assumption of exploring a known building, we are actually given the access an extra layer of structural information for the target environment, i.e. the <em>floor plan</em> of the building. How to exploit that information and set up a multi-agent exploration task efficiently? That becomes the main goal for this project.</p><h2 id=technical-details>Technical Details</h2><h3 id=overview>Overview</h3><p>The general structure of this project is divided into 3 modules: a central ROS server, a building renderer/simulator, and the F1TENTH race car onboard computing platform.</p><figure><img src=cad2cav-overview.png alt="A diagram for the system design of CAD2CAV."><figcaption><p>A diagram for the system design of CAD2CAV.</p></figcaption></figure><p>The figure above demonstrates the general relationship among the 3 modules. Given the prior knowledge of the building&rsquo;s floor plan (which is assumed to come from <a href="https://www.autodesk.com/products/revit/overview?term=1-YEAR&amp;tab=subscription" target=_blank rel="noopener noreffer">Autodesk Revit</a>), the building renderer would parse and construct a rendered 3D model our of it for visualization; the central ROS server would extract necessary structural information and construct 2D occupancy map for motion planning & mapping nodes. As soon as the central ROS server generates a balanced multi-agent routing scheme, it would pass the plan to each F1TENTH race car and the race car would then execute the drive node to drive the car and follow the high-level routing scheme in the real world.</p><p>As the race car is driving in the environment, its perception module would try to localize itself in the occupancy map and update the perceived objects in the map. The map then gets updated in the central ROS server and the server would also pass the updated map to the building renderer. The building renderer would finally render everything and visualize the building in the software.</p><h3 id=building-renderer>Building Renderer</h3><p>As mentioned above, we need a building renderer to render 3D models and visualize everything on the screen. Our current choice is to use Unreal Engine 4. UE4 has a popular plugin called Unreal Datasmith compatible with most of the mainstream model designing softwares, i.e., Autodesk Revit, Cinema 4D (C4D), Rhino, etc.</p><p>Users can install and try the plugin by following the tutorial of <a href=https://docs.unrealengine.com/4.27/en-US/WorkingWithContent/Importing/Datasmith/SoftwareInteropGuides/Revit/ExportingDatasmithContentfromRevit/ target=_blank rel="noopener noreffer">Exporting Datasmith Content from Revit</a> and <a href=https://docs.unrealengine.com/4.27/en-US/WorkingWithContent/Importing/Datasmith/HowTo/ImportingContent/ target=_blank rel="noopener noreffer">Importing Datasmith Content into Unreal Engine 4</a>. We also hosted a pre-imported model of Levine Hall 4th floor on <a href=https://github.com/shineyruan/unreal_levine_4 target=_blank rel="noopener noreffer">GitHub</a>.</p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube-nocookie.com/embed/wf6D9ydkIpc style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><h4 id=other-options>Other Options</h4><p>We are also looking into trying out different options that are more friendly to robotics application development than UE4. One of them is the <a href=https://developer.nvidia.com/isaac-sim target=_blank rel="noopener noreffer">Nvidia Isaac Sim</a> powered by the newly-announced Nvidia Omniverse platform.</p><h3 id=central-ros-server>Central ROS Server</h3><p>The central ROS server is responsible for: 1) parsing floor plan from Revit and constructing initial occupancy grid; 2) generating high-level balanced multi-agent routing scheme from occupancy map; 3) updating occupancy map from the perception data; 4) sending occupancy map information over to building renderer. Each of these tasks is a sub-problem in different fields of robotics, in the general sense.</p><h4 id=initial-map-construction-from-revit>Initial Map Construction from Revit</h4><p>Revit models are encoded in a unique format special to the software. Hence, the only way for us to extract information from a general Revit model is through the <a href=https://www.revitapidocs.com/2022/ target=_blank rel="noopener noreffer">Revit API</a>. Revit API is initially written in C#, while it has been recently transferred to Python by an open-source project called <a href=https://github.com/eirannejad/pyRevit target=_blank rel="noopener noreffer">pyRevit</a>. By following the official installation tutorials of pyRevit, I have managed to create a pyRevit plugin for the project to specifically export the 2D geometry information of all the main elements in a building floor plan. The current supported elements and their geometric representations are:</p><ol><li>Walls, represented as 2D line segments;</li><li>Doors, represented as 2D points with orientation;</li><li>Windows, represented as 2D points with orientation.</li></ol><p>The current pyRevit plugin is hosted on <a href=https://github.com/shineyruan/cad2cav_revit_plugins target=_blank rel="noopener noreffer">GitHub</a> and is written in IronPython 2.7.<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> It runs in Autodesk Revit 2022. Upon one button click, it would try to look for these elements in the model and print their geometric representation information in the console with the following specific format:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-txt data-lang=txt><span class=line><span class=cl>Type,x_1,y_1,z_1,x_2,y_2,z_2,Orientation,Width,Height
</span></span><span class=line><span class=cl>wall,-17.3718,12.2507,0.0,-16.7919,12.8305,0.0,0.0,0.0,0.0
</span></span><span class=line><span class=cl>door,-6.2356,2.5174,0.0,0.0,0.0,0.0,1.5708,0.864,2.032
</span></span><span class=line><span class=cl>window,-0.915,-3.4125,0.9,0.0,0.0,0.0,3.1416,1.05,1.35
</span></span></code></pre></td></tr></table></div></div><p>Users are supposed to save the console outputs into a CSV file and put it manually in the ROS codebase.</p><p>In the CSV format:</p><ul><li><code>Type</code> is a string describing the object type;</li><li><code>x_1</code>, <code>y_1</code>, <code>z_1</code> is the 3D Cartesian coordinate of the 1<sup>st</sup> endpoint of the line segment, or the position of the oriented point;</li><li><code>x_2</code>, <code>y_2</code>, <code>z_2</code> is the 3D Cartesian coordinate of the 2<sup>nd</sup> endpoint of the line segment, or left blank if the type is an oriented point;</li><li><code>Orientation</code> is the orientation of the point, or left blank if the type is a line segment;</li><li><code>Width</code> and <code>Height</code> are the actual width and height of the doors and windows in the Revit model, and are left blank for the walls.</li></ul><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube-nocookie.com/embed/6ka9pvTfP-U style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><p>After we have the necessary structural information in a floor plan contained in a CSV file, we can then proceed to load the file in ROS and construct an initial occupancy map out of it. Since all information are transformed into some basic geometric shapes, it is then very easy for us to draw these shapes on a 2D grid. The only algorithm we use here is the <a href=https://en.wikipedia.org/wiki/Bresenham%27s_line_algorithm target=_blank rel="noopener noreffer">Bresenham&rsquo;s line algorithm</a>, which enables fast grid marching along a certain 2D line segment. We set everything cell in the grid that is part of the geometric shapes to be <code>OCCUPIED</code> and an initial map for the floor plan is then produced.</p><table><thead><tr><th style=text-align:center>Revit</th><th style=text-align:center>Occupancy Grid</th></tr></thead><tbody><tr><td style=text-align:center><figure><img src=revit_floorplan_levine2.png></figure></td><td style=text-align:center><figure><img src=map_floorplan_levine2.png></figure></td></tr></tbody></table><h4 id=waypoint-identification-and-registration>Waypoint Identification and Registration</h4><p>To generate a high-level routing scheme for multiple vehicles to explore, it is important to know which set of locations or areas that the exploration needs to cover. While we&rsquo;re still exploring other options<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>, for simplicity we provide an interface for users to select and register waypoints on the map in a pop-up window. As soon as the waypoints are registered, the server would construct a graph out of all the waypoints, and the routing algorithms will be performed with the scope of an abstract graph.</p><figure><img src=user_waypoint_registration.png alt="A screenshot of a sample user waypoint registration interface." width=600><figcaption><p>A screenshot of a sample user waypoint registration interface.</p></figcaption></figure><h4 id=graph-planning-and-routing>Graph Planning and Routing</h4><p>All the high-level routing algorithms are conducted on an abstract graph. In this task we&rsquo;re interested in assign an exploration route for each vehicle, such that all vehicles gets an exploration path of equal or similar cost to each other.</p><h5 id=the-capacitated-vehicle-routing-problem-cvrp>The Capacitated Vehicle Routing Problem (CVRP)</h5><p>Vehicle Routing Problem (VRP) is a famous set of combinatorial optimization problems widely used in operations research. It is essentially a generalization to the famous Travelling Salesman Problem (TSP), which only focuses on computing the cheapest transportation loop for one single agent. The VRP can be described as the following:<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup></p><blockquote><p>Concerning the service of a delivery company. Things are delivered from one or more <em>depots</em> which has a given set of home <em>vehicles</em> and operated by a set of <em>drivers</em> who can move on a given <em>road network</em> to a set of <em>customers</em>. Determine the set of <em>routes</em> (one route for each vehicle that must start and finish at its own depot) such that all customers&rsquo; requirements and operational constraints are satisfied and the <em>global transportation cost</em> is minimized.</p></blockquote><p>From an optimization problem point of view, VRP essentially tries to minimize the transportation cost under some constraints. Notice that VRP itself does not specify any definition for the transportation costs in the first place. Here we choose to minimize the Euclidean distance as the objective function. The original VRP problem does not specify any constraints either. However, as the optimal solution for an unconstrained VRP would fall back to a TSP loop which is meaningless for our project, we introduce a capacity constraint on each vehicle, which forms the Capacitated VRP (CVRP) problem:</p><blockquote><p>Each vehicle has a fixed <em>capacity</em> defining the maximum number of nodes it could visit in one run. Once the vehicle has visited the maximum number of nodes after it leaves the depot, it must return to the depot immediately.</p></blockquote><p>The formulation of CVRP in our project would be simply minimizing the total Euclidean distance of the set of routes over the graph of waypoints in Revit coordinates. By tuning the capacity of the vehicle as a hyperparameter, we are able to generate a set of routes that covers all waypoints and minimizes the total travelled distance in the same time, for a fixed number of vehicles.</p><p>Solving the CVRP problem in our project involves the use of <a href=https://en.wikipedia.org/wiki/Ant_colony_optimization_algorithms target=_blank rel="noopener noreffer">Any Colony Optimization</a> (ACO) algorithm. As the general CVRP problem is NP-hard, ACO is used as an approximation to the optimal solution that runs in polynomial time.</p><p>A general description of ACO algorithm can be described in the pseudo-code:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>procedure</span> <span class=n>ACO_MetaHeuristic</span> <span class=ow>is</span>
</span></span><span class=line><span class=cl>    <span class=k>while</span> <span class=ow>not</span> <span class=n>terminated</span> <span class=n>do</span>
</span></span><span class=line><span class=cl>        <span class=n>generateSolutions</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>daemonActions</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>pheromoneUpdate</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>repeat</span>
</span></span><span class=line><span class=cl><span class=n>end</span> <span class=n>procedure</span>
</span></span></code></pre></td></tr></table></div></div><p>The essence of ACO is to mimic a colony of exploring ants. During exploration, each ant would leave a pheromone trail behind its body, and other ants would smell it and intend to follow the path of the previous ants. Meanwhile, each ant also has an intention of exploring unvisited places. The combination of two different intentions forms an <a href=https://www.geeksforgeeks.org/epsilon-greedy-algorithm-in-reinforcement-learning/ target=_blank rel="noopener noreffer">$\varepsilon$-greedy strategy</a> for each ant, balancing exploration with exploitation.</p><p>Our implementation of ACO algorithm also involves a timeout variable for terminating the while loop, generating graph search solutions for each ant, determining the next action for the current ant, and updating the pheromone trails. Notice that in any CVRP solution, all vehicles must start off from one common depot.</p><table><thead><tr><th style=text-align:center>CVRP Solution for Vehicle 1</th><th style=text-align:center>CVRP Solution for Vehicle 2</th></tr></thead><tbody><tr><td style=text-align:center><figure><img src=vrp-1.png></figure></td><td style=text-align:center><figure><img src=vrp-2.png></figure></td></tr></tbody></table><h5 id=spectral-clustering>Spectral Clustering</h5><p>Another way to tackle this multi-agent routing task is to divide the entire graph into several partitions. In each partition, we find a TSP path that connects all nodes in the subgraph and minimizes the travel cost. <strong>The advantage of this method is that vehicles are <em>not</em> required to start off from one common depot anymore, which makes the algorithm suitable for replanning at any time during the exploration.</strong></p><p>Spectral clustering is an intuitive way to group graph nodes together based on spectral coordinates. This algorithm initially comes from the theory of unsupervised learning. The spectral coordinates of a graph is defined to be the row vectors of the eigenvector matrix of the Laplacian of the graph. We can obtain the spectral coordinates for each graph node in the following way:</p><ol><li>Construct an adjacency matrix $W$ for the graph;</li><li>Compute the degree matrix $D$ by summing up all elements per row of $W$ and make it a diagonal matrix.</li><li>Compute the graph Laplacian $L$ by $L=D-W$.</li><li>Compute the eigenvalues and eigenvectors of $L$, sort them in ascending order.</li><li>The spectral coordinates of each graph node is the corresponding row of the eigenvector matrix of $L$.</li></ol><figure><img src=spectral-clustering.png alt="Referenced from EECS 445, the University of Michigan" width=600><figcaption><p>Referenced from EECS 445, the University of Michigan</p></figcaption></figure><p>After obtaining the spectral coordinates for each node, we then apply a simple clustering algorithm on the spectral space of the nodes. One popular choice would be <a href=https://en.wikipedia.org/wiki/K-means_clustering target=_blank rel="noopener noreffer">k-means clustering</a>. As for us, we replaced k-means with <a href=https://en.wikipedia.org/wiki/K-means%2B%2B target=_blank rel="noopener noreffer">k-means++</a> as a quick and simple improvement, which improves the initialization of the centroids to make the clustering outcome more stable.</p><p>Spectral clustering tends to group similar nodes together. The definition of similarity lies in the definition of the weight of the edges. That is, the lower the edge weight is between two nodes, the higher chances that they are grouped together. This property of spectral clustering aligns perfectly with our desire, i.e., nodes in the neighborhood form a subpart of the graph. Hence, we could directly apply spectral clustering to our graph partitioning task and generate TSP loop within each graph partition.</p><p>The application of spectral clustering implicitly assume that the graph is complete, i.e., every two nodes are connected. Hence, it cannot generalize to all situations, especially those incomplete graphs generated from some special floor plans.</p><table><thead><tr><th style=text-align:center>Spectral Clustering for Vehicle 1</th><th style=text-align:center>Spectral Clustering for Vehicle 2</th></tr></thead><tbody><tr><td style=text-align:center><figure><img src=spectral-1.png></figure></td><td style=text-align:center><figure><img src=spectral-2.png></figure></td></tr></tbody></table><h5 id=multi-level-k-way-graph-partitioning>Multi-level $k$-Way Graph Partitioning</h5><p>Another way of partitioning the graph is to formulate a $k$-way graph partitioning problem. Traditional graph partitioning problems aims to find a minimum cut to divide the graph subject to the constraint that all resulting subparts are as evenly partitioned as possible. One handy multi-level $k$-way partitioning algorithm<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup> for this builds upon the <a href=https://en.wikipedia.org/wiki/Kernighan%E2%80%93Lin_algorithm target=_blank rel="noopener noreffer">Kernighan-Lin heuristic</a>. For simplicity, we directly incorporated <a href=http://glaros.dtc.umn.edu/gkhome/metis/metis/overview target=_blank rel="noopener noreffer">METIS</a> to perform the graph partitioning task. We then generate TSP loops in each graph subpart using <a href=https://developers.google.com/optimization target=_blank rel="noopener noreffer">Google OR-Tools</a>.</p><table><thead><tr><th style=text-align:center>$k$-Way Graph Partitioning for Vehicle 1</th><th style=text-align:center>$k$-Way Graph Partitioning for Vehicle 2</th></tr></thead><tbody><tr><td style=text-align:center><figure><img src=partitioning-1.png></figure></td><td style=text-align:center><figure><img src=partitioning-2.png></figure></td></tr></tbody></table><h5 id=graph-planning-summary-and-demo>Graph Planning Summary and Demo</h5><p>We explored 3 different ways of distributing different routes to each vehicles based on abstract graphs in the graph planning task in our project. One of them requires a common starting point (initial depot) and the others are based on graph partitioning. All of these methods tries to find a set of routes that covers all the floor plan with minimum overlap and complete loops. Although we have implemented all 3 methods in our code base, our default and most up-to-date choice would be the application of METIS and multi-level $k$-way partitioning algorithm.</p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube-nocookie.com/embed/ufkgAVptQT8 style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><h3 id=f1tenth-onboard-computing-platform>F1TENTH Onboard Computing Platform</h3><p>The latest version of F1TENTH race car is equipped with a nice Nvidia Xavier NX Development Board as the onboard computing platform, which contains both an ARMv8 CPU and a CUDA-enabled GPU. It runs a complete but special Ubuntu system developed and shipped by Nvidia. In general, it is capable of performing part of the computationally heavy tasks in our project. In our design, we run the low-level planning and control node as well as the localization and mapping node onboard.</p><h4 id=obstacle-avoiding-planning-and-control>Obstacle-Avoiding Planning and Control</h4><p>After we have a complete route for each vehicle, we need to design an algorithm for the vehicle to actually reach those waypoints in the real world. Hence, the algorithm is required to be fast, accurate, and responsive. In this task, we choose to apply the <a href=https://arxiv.org/abs/1306.3532 target=_blank rel="noopener noreffer">Fast Marching Tree (FMT*)</a> algorithm as a low-level planner to generate real-time trajectories for each vehicle. As a sampling-based planning algorithm, FMT* is fast enough to replan in a very short amount of time so that the vehicle is able to avoid obstacles.</p><p>As a general overview, FMT* combines the properties of two different sampling-based planning categories: <a href=https://en.wikipedia.org/wiki/Probabilistic_roadmap target=_blank rel="noopener noreffer">PRM (Probabilistic Road Map)</a> and <a href=https://en.wikipedia.org/wiki/Rapidly-exploring_random_tree target=_blank rel="noopener noreffer">RRT (Rapidly-exploring Random Trees)</a>. It first samples a set of points from the planning space (or configuration space), and tries to grow a tree from the start to the end in an RRT-style. It defines an unvisited set $V_\text{unvisited}$, an open set $V_\text{open}$ and a closed set $V_\text{closed}$. As the following figure shows, it involves the following steps:</p><ol><li>Add the starting point to $V_\text{closed}$.</li><li>Add all of the neighboring points of the starting point to $V_\text{open}$. All the other points are in $V_\text{unvisited}$.</li><li>Select the lowest cost node $z\in V_\text{open}$ and finds its neighbors $\mathcal{N}(z)\subseteq V_\text{unvisited}$.</li><li>Given a node $x\in\mathcal{N}(z)$, find the optimal 1-step connection to its neighboring nodes $\mathcal{N}(x)$ in $V_\text{open}$.</li><li>Check collision on all newly-added connections and apply penalties for violations (i.e., remove all the collision connections);</li><li>Add all successful connected nodes to $V_\text{open}$, and move $z$ to $V_\text{closed}$.</li><li>Repeat the operations from Step 3 until the goal is reached.</li></ol><figure><img src=fmt_star.png alt="FMT* algorithm cited from the paper." width=600><figcaption><p>FMT* algorithm cited from the paper.</p></figcaption></figure><p>FMT* works as a low-level planner on the vehicle and generates trajectory towards the next waypoint in the high-level route. For now, the trajectory consists of a set of more fine-grained 2D points. For future work orientation can be added to the trajectory point as an extra information so that the vehicle could be better controlled in terms of steering.</p><p>As for the control algorithm, we currently run the traditional pure-pursuit algorithm to drive the vehicle to follow the trajectory points. For a more detailed overview of the pure-pursuit algorithm please checkout <a href=https://zhihaoruan.xyz/f1tenth-lab6/ rel>my F1TENTH Lab 6 report</a>.</p><p>Future work on the control algorithm involves developing a <a href=https://en.wikipedia.org/wiki/Model_predictive_control target=_blank rel="noopener noreffer">model predictive control (MPC)</a> algorithm, but for the sake of simplicity we still stick to pure-pursuit for the moment.</p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube-nocookie.com/embed/XoU88reSlNA style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><h4 id=localization>Localization</h4><p>In a real-world experiment, each vehicle needs to know where it is in the map. Hence, it is very important for the vehicle to have a localization module onboard. Common localization algorithms in general robotics include all kinds of filtering algorithms, from Bayes filer, <a href=https://en.wikipedia.org/wiki/Kalman_filter target=_blank rel="noopener noreffer">Kalman filter (KF)</a> and its variations (<a href=https://en.wikipedia.org/wiki/Extended_Kalman_filter target=_blank rel="noopener noreffer">EKF</a>, <a href=https://groups.seas.harvard.edu/courses/cs281/papers/unscented.pdf target=_blank rel="noopener noreffer">UKF</a>, etc.), and <a href=https://en.wikipedia.org/wiki/Particle_filter target=_blank rel="noopener noreffer">particle filter</a>. We choose to use particle filter as a starting point and incorporated a CUDA-accelerated package of particle filter<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup> from the MIT race car team.</p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube-nocookie.com/embed/58snKwkVv1g style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div></br><blockquote><p>Thanks to my project partner Shumin for her work on the localization module. <strong>The rest of this section is reposted from</strong> <a href=https://shumin326.github.io/shumin.github.io/jekyll/update/2021/12/21/IndependentStudy.html target=_blank rel="noopener noreffer"><strong>Shumin&rsquo;s article</strong></a>.</p></blockquote><h5 id=overview-1>Overview</h5><p>Mapping and localization algorithms require a manual walkthrough and capture of the environment. Upon loop closure they are ready to navigate with algorithms like SLAM. This project’s goal is to improve this process for indoor spaces with two steps:</p><ol><li>Cheap initial navigable map, e.g. floor plans. Use architectural maps to slow the cost and increase the speed of the initial map capture by eliminating the manual walkthrough. We aim to accomplish this by transforming the building’s blueprint into a 3D navigable world with occupancy map and semantic landmarks within which we do path planning and localization.</li><li>Landmark detection: We implemented a landmark detection model so that robot can recognize landmarks in real world navigation. Those landmarks are further utilized to improve localization performance.</li></ol><p>This independent study developed a complete pipeline for robots navigation under indoor environments, e.g. hospitals and offices, with only an architectural map.</p><p>The overall workflow is shown in the figure below. Given a building blueprint, e.g. floor plans, the Auto Mapping module converts the floor plan into an occupancy map and a list of landmarks with accurate location and orientation. The planning module determines a navigation strategy for robots to navigate and has the potential to apply onto multi-agent navigation applications. It first lets the user manually determine a set of sparse waypoints in order to make sure that LiDAR scans cover every corner of the building. Then it uses a Pure Pursuit controller and FMT* path planner to navigate among those waypoints. Graph partitioning algorithm and closed TSP solution are used to make sure all waypoints are visited only once. The landmark detection module detects landmarks and also shares this information in the format of labeled bounding boxes with the localization module. The Localization module uses particle filter to fuse sensor data, i.e. lidar scan, odometry and landmarks, and obtain an estimated pose.</p><figure><img src=https://i.ibb.co/5cpPZ1V/system-overview.png alt=system-overview></figure><h5 id=localization-1>Localization</h5><p>The localization part is the main focus of this project. It aims at providing a robust and accurate pose estimate for other modules (planning and control, etc) to function properly. The localization module takes as input the laser scans, noisy odometry and landmarks, and fuses them with a particle filter (alg. 1). There are mainly three updates happening inside the loop:</p><ol><li>Motion model update using odometry. This step updates the particle poses by dead reckoning the odometry data and will have accumulated error overtime. Since the odometry used in onboard is quite noisy, the motion model update was manually added some gaussian noise.</li><li>Sensor update with laser scan. This step updates the particle weights by comparing the obtained ranges from laser scan with the ray marching ranges computed by RangeLibc and the reference map.</li><li>Sensor update with landmarks. This step updates the particle weights by doing a landmark projection and matching(shown in Alg.2) algorithm which compares the observed landmarks with reference landmarks. More details will be provided in the next section.</li></ol><figure><img src=https://i.ibb.co/68YNY9z/algo1.png alt=algo1></figure><h5 id=landmark-detection>Landmark Detection</h5><p>The landmark detection is developed using a well known, high performance and highly customizable object detection model: YOLO. Since our indoor environment is vastly different from its training dataset environments where images are collected mostly outdoors and from a human perspective, a customized dataset was created for fine-tuning the YOLO model. The trained model can perform very robust and fast indoor landmark detection which is exactly what we needed for this project.</p><figure><img src=https://i.ibb.co/KWFN2sT/landmark-detection.png alt=landmark-detection></figure><h5 id=landmark-projection-and-matching>Landmark Projection and Matching</h5><p>The detected landmarks as bounding boxes can be used to further enhance the localization performance because we already have certain landmarks positions and orientations embedded in the floor maps, e.g. doors and windows. Thus, this section introduces the landmark projection and matching algorithm (alg. 2) that uses reference landmarks of floor maps and detect landmark in real time to update particle weights and improve the robustness of localization module especially at low-feature places such as the narrow hallway.</p><figure><img src=https://i.ibb.co/GJHNcCd/alg2.png alt=alg2></figure><p>Determining the “seeable” landmarks is to check whether a landmark satisfies the three conditions (also shown in the figure below).</p><ol><li>It’s not blocked by any obstacle</li><li>It’s within camera’s sight distance</li><li>It’s within the camera’s field of view (FOV).</li></ol><p>The first requirement is ensured by comparing the ray marching result from robot to the landmark and the actual distance between them two. If they’re equal(with some tolerance) then the landmark is not blocked by obstacles. The second and third requirements are ensured by computing the distance and angles between robot and landmark and compare with sight distance as well as camera FOV.</p><figure><img src=https://i.ibb.co/3SqYh4n/seeable.png alt=seeable></figure><p>Seeable landmarks are in world frame but the detected landmarks are in camera frame, so we need to transform the seeable landmarks from world to camera frame. For simplicity, the frames are denoted as world ($w$), Robot ($R$) and Camera ($C$):</p><p>$$
L^C=T^C_R\cdot T^R_w\cdot L^w
$$</p><p>Where $L^w$ is the landmark in world frame and $L^C$ is the converted landmarks in the camera frame. The transformation matrices can be obtained using particle pose $\pi=(x,y,\theta)$ and camera extrinsic matrix $E$ and intrinsic matrix $I$.</p><p>$$
T^C_R=E\cdot I
$$</p><p>$$
T^R_W=\begin{bmatrix}
R_{z,\theta} & d \\ \mathbf{0} & 1
\end{bmatrix},\quad d=[x,y,0]^T
$$</p><p>Up to now we’ve found the set of expected landmarks to be seen in the camera frame and we’re ready to compare them with detected landmarks $L$. Since the number of two sets are not necessarily equal, use $k$-nearest neighbor (kNN) to find matches and compute the average distance $d$. Then update the particle weight $w$:</p><p>$$
w=1/(1+d)
$$</p><p>Note that in order to make the computation less stressful so it can be run in real time, the particle filter algorithm is vectorized instead of doing a loop for each particle, the latter is just to make the algorithm easier to understand.</p><h5 id=code-and-demo-videos>Code and Demo Videos</h5><p>The <a href=https://github.com/mlab-upenn target=_blank rel="noopener noreffer">mLab github</a> contains the repos for all the modules including localization, landmark detection, as well as planning module and Auto-mapping module. They&rsquo;re for the whole project &ldquo;CAD2CAV: From Building Blueprints to Scalable Multi-robot navigation&rdquo;.</p><p>in specific, the repos for my independent study project are:</p><ol><li><a href=https://github.com/shineyruan/particle_filter target=_blank rel="noopener noreffer">Localization module</a>.</li><li><a href=https://github.com/Shumin326/darknet_ros target=_blank rel="noopener noreffer">Landmark detection module</a>. It also contains instructions on fine tuning as well as a link to my model.</li></ol><p>The demo videos can be viewed in the repos(listed above) README as well as this <a href="https://www.youtube.com/playlist?list=PL8Fip0E9YRSuZok9umO4fMpGWx_UsfoaL" target=_blank rel="noopener noreffer">youtube video list</a>.</p><h2 id=references>References</h2><ul><li><a href=https://github.com/eirannejad/pyRevit target=_blank rel="noopener noreffer">eirannejad/pyRevit</a>.</li><li><a href=https://github.com/mit-racecar/particle_filter target=_blank rel="noopener noreffer">mit-racecar/particle_filter</a>.</li><li><a href=https://github.com/google/or-tools target=_blank rel="noopener noreffer">google/or-tools</a>.</li><li><a href=https://github.com/AprilRobotics/apriltag target=_blank rel="noopener noreffer">AprilRobotics/apriltag</a>.</li><li><a href=https://github.com/ben-strasser/fast-cpp-csv-parser target=_blank rel="noopener noreffer">ben-strasser/fast-cpp-csv-parser</a>.</li><li><a href=https://github.com/leggedrobotics/darknet_ros target=_blank rel="noopener noreffer">leggedrobotics/darknet_ros</a>.</li><li>A. Y. Ng, M. I. Jordan, and Y. Weiss, &ldquo;On spectral clustering: Analysis and an algorithm,&rdquo; in Proceedings of the 14th International Conference on Neural Information Processing Systems: Natural and Synthetic, ser. NIPS’01. Cambridge, MA, USA: MIT Press, 2001, p. 849–856.</li><li>B. Lau, C. Sprunk, and W. Burgard, &ldquo;Improved updating of euclidean distance maps and Voronoi diagrams,&rdquo; in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems, Taipei, Taiwan, 2010. [Online]. Available: <a href=http://ais.informatik.uni-freiburg.de/publications/papers/lau10iros.pdf target=_blank rel="noopener noreffer">http://ais.informatik.uni-freiburg.de/publications/papers/lau10iros.pdf</a></li><li>B. W. Kernighan and S. Lin, &ldquo;An efficient heuristic procedure for partitioning graphs,&rdquo; The Bell System Technical Journal, vol. 49, no. 2, pp. 291–307, 1970.</li><li>C. Johnson, &ldquo;Topological mapping and navigation in real-world environments,&rdquo; Ph.D. dissertation, University of Michigan, 2018. Available: <a href=https://deepblue.lib.umich.edu/handle/2027.42/144014 target=_blank rel="noopener noreffer">https://deepblue.lib.umich.edu/handle/2027.42/144014</a></li><li>D. Arthur and S. Vassilvitskii, &ldquo;K-means++: The advantages of careful seeding,&rdquo; in Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, ser. SODA ’07. USA: Society for Industrial and Applied Mathematics, 2007, p. 1027–1035.</li><li>E. Olson, &ldquo;AprilTag: A robust and flexible visual fiducial system,&rdquo; 2011 IEEE International Conference on Robotics and Automation, 2011, pp. 3400-3407, doi: 10.1109/ICRA.2011.5979561.</li><li>G. Karypis and V. Kumar, &ldquo;Multilevel k-way partitioning scheme for irregular graphs,&rdquo; Journal of Parallel and Distributed Computing, vol. 48, no. 1, pp. 96–129, 1998. [Online]. Available: <a href=https://www.sciencedirect.com/science/article/pii/S0743731597914040 target=_blank rel="noopener noreffer">https://www.sciencedirect.com/science/article/pii/S0743731597914040</a></li><li>J. Wang and E. Olson, &ldquo;AprilTag 2: Efficient and robust fiducial detection,&rdquo; 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2016, pp. 4193-4198, doi: 10.1109/IROS.2016.7759617.</li><li>L. Janson, E. Schmerling, A. Clark, and M. Pavone, &ldquo;Fast marching tree: a fast marching sampling-based method for optimal motion planning in many dimensions,&rdquo; 2015.</li><li>S. L. Bowman, N. Atanasov, K. Daniilidis and G. J. Pappas, &ldquo;Probabilistic data association for semantic SLAM,&rdquo; 2017 IEEE International Conference on Robotics and Automation (ICRA), 2017, pp. 1722-1729, doi: 10.1109/ICRA.2017.7989203.</li><li>Yang, Zhong & Yao, Baozhen. (2009). &ldquo;An Improved Ant Colony Optimization for Vehicle Routing Problem.&rdquo; European Journal of Operational Research. 196. 171-176. 10.1016/j.ejor.2008.02.028.</li></ul><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href=https://ironpython.net/ target=_blank rel="noopener noreffer">IronPython</a> is the Python extension for Microsoft .Net environment. It is used by pyRevit as the default Python wrapper over C# Revit API.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>Other options include the algorithm of extracting a Voronoi skeleton out of a general occupancy map. This work was initially proposed by Lau et al. in the IROS 2010 paper <a href=http://ais.informatik.uni-freiburg.de/publications/papers/lau10iros.pdf target=_blank rel="noopener noreffer">&ldquo;Improved updating of euclidean distance maps and Voronoi diagrams&rdquo;</a>.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>From Wikipedia: <a href=https://en.wikipedia.org/wiki/Vehicle_routing_problem target=_blank rel="noopener noreffer">Vehicle Routing Problem</a>.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>See <a href=http://glaros.dtc.umn.edu/gkhome/metis/metis/overview target=_blank rel="noopener noreffer">METIS</a> and <a href=https://www.sciencedirect.com/science/article/pii/S0743731597914040 target=_blank rel="noopener noreffer">&ldquo;Multilevel k-way partitioning scheme for irregular graphs&rdquo;</a>.&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p>See <a href=https://github.com/mit-racecar/particle_filter target=_blank rel="noopener noreffer">mit-racecar/particle_filter</a>.&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on 2023-03-30&nbsp;<a class=git-hash href=https://github.com/shineyruan/shineyruan.github.io/commit/d512269a88b05c47da3764a8128cc1dafa864d38 target=_blank title="commit by Zhihao Ruan(shineyruan@gmail.com) d512269a88b05c47da3764a8128cc1dafa864d38: multilingual support">
<i class="fas fa-hashtag fa-fw" aria-hidden=true></i>d512269</a></span></div></div><div class=post-info-line><div class=post-info-md><span><a class=link-to-markdown href=/cad2cav/index.md target=_blank>Read Markdown</a></span></div><div class=post-info-share><span><a href=javascript:void(0); title="Share on Twitter" data-sharer=twitter data-url=https://zhihaoruan.xyz/cad2cav/ data-title="CAD2CAV: Computer Aided Design for Cooperative Autonomous Vehicles" data-hashtags="f1tenth,motion planning,exploration,robotics,Multi-lingual"><i class="fab fa-twitter fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Facebook" data-sharer=facebook data-url=https://zhihaoruan.xyz/cad2cav/ data-hashtag=f1tenth><i class="fab fa-facebook-square fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Hacker News" data-sharer=hackernews data-url=https://zhihaoruan.xyz/cad2cav/ data-title="CAD2CAV: Computer Aided Design for Cooperative Autonomous Vehicles"><i class="fab fa-hacker-news fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Line" data-sharer=line data-url=https://zhihaoruan.xyz/cad2cav/ data-title="CAD2CAV: Computer Aided Design for Cooperative Autonomous Vehicles"><i data-svg-src=/lib/simple-icons/icons/line.min.svg aria-hidden=true></i></a><a href=javascript:void(0); title="Share on 微博" data-sharer=weibo data-url=https://zhihaoruan.xyz/cad2cav/ data-title="CAD2CAV: Computer Aided Design for Cooperative Autonomous Vehicles" data-image=/images/posts/cad2cav/cad2cav-overview.png><i class="fab fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw" aria-hidden=true></i>&nbsp;<a href=/tags/f1tenth/>f1tenth</a>,&nbsp;<a href=/tags/motion-planning/>motion planning</a>,&nbsp;<a href=/tags/exploration/>exploration</a>,&nbsp;<a href=/tags/robotics/>robotics</a>,&nbsp;<a href=/tags/multi-lingual/>Multi-lingual</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/565-final-project/ class=prev rel=prev title="Volume Rendered ReSTIR in Vulkan"><i class="fas fa-angle-left fa-fw" aria-hidden=true></i>Volume Rendered ReSTIR in Vulkan</a>
<a href=/newcastle/ class=next rel=next title="Newcastle, UK">Newcastle, UK<i class="fas fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line>Powered by <a href=https://gohugo.io/ target=_blank rel="noopener noreffer" title="Hugo 0.111.3">Hugo</a> | Theme - <a href=https://github.com/dillonzq/LoveIt target=_blank rel="noopener noreffer" title="LoveIt 0.2.11"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden=true></i> LoveIt</a></div><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw" aria-hidden=true></i><span itemprop=copyrightYear>2019 - 2023</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=/ target=_blank>Zhihao Ruan</a></span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><link rel=stylesheet href=/lib/katex/katex.min.css><script type=text/javascript src=/lib/autocomplete/autocomplete.min.js></script><script type=text/javascript src=/lib/lunr/lunr.min.js></script><script type=text/javascript src=/lib/lazysizes/lazysizes.min.js></script><script type=text/javascript src=/lib/clipboard/clipboard.min.js></script><script type=text/javascript src=/lib/sharer/sharer.min.js></script><script type=text/javascript src=/lib/katex/katex.min.js></script><script type=text/javascript src=/lib/katex/contrib/auto-render.min.js></script><script type=text/javascript src=/lib/katex/contrib/copy-tex.min.js></script><script type=text/javascript src=/lib/katex/contrib/mhchem.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:50},comment:{},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{highlightTag:"em",lunrIndexURL:"/index.json",maxResultLength:10,noResultsFound:"No results found",snippetLength:30,type:"lunr"}}</script><script type=text/javascript src=/js/theme.min.js></script></body></html>