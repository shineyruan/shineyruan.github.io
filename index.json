[{"categories":["photography","nature","flowers","california"],"content":"The winter of 2022‚Äì2023 has been an extremely wet one for California. The cold air from the North Pole rages down into the southern part of U.S. west coast, turning into heavy rain and snow. However, as a positive result, California is very likely to greet a splendid ‚Äúsuper bloom‚Äù after several years of drought. Of all the flowers, California Poppy could be one of the most iconic flower species that worth a visit. It grows only in the State of California (and probably in Baja California as well!) and has been recognized as the official state flower of California. In the Spring of 2023, I took my camera and enjoyed the beauty of California poppy in the wild. ","date":"2023-03-31","objectID":"/california-poppy/:0:0","tags":["My Gallery"],"title":"California Poppy \u0026mdash; A Splendid Spring!","uri":"/california-poppy/"},{"categories":[],"content":"Zhihao Ruan is a robotics student with a main area of focus in motion planning and machine learning. He is interested in building AI-empowered physical objects for the future. Topics: Motion planning (robotics), ROS (robotics), machine learning, artificial intelligence, software development (C++), ray tracing (computer graphics), CUDA (computer graphics) üóí Resume Education M.S.E. in Robotics (GRASP Lab), University of Pennsylvania, 2020‚Äì2022 B.S. in Computer Science Engineering, University of Michigan, 2018‚Äì2020 B.S. in Electrical and Computer Engineering, UM-SJTU Joint Institute (Shanghai Jiao Tong University), 2016‚Äì2020 ","date":"2023-03-30","objectID":"/about/:0:0","tags":[],"title":"About Author","uri":"/about/"},{"categories":["My Gallery"],"content":" An overview of the city of Newcastle. The train station. City of Newcastle in the morning light. The two iconic bridges. ","date":"2021-12-30","objectID":"/newcastle/:0:0","tags":["photography","city representation"],"title":"Newcastle, UK","uri":"/newcastle/"},{"categories":["Projects"],"content":"CAD2CAV is a project focusing on multi-agent exploration in unknown environments. It attempts to build a complete system from perception to planning and control, exploring a designated unknown environments with multiple autonomous vehicles. It is built in xLab at the University of Pennsylvania, on multiple F1TENTH race cars. Useful documents: GitHub link Spring 2021 Final Report (not complete) ","date":"2021-12-27","objectID":"/cad2cav/:0:0","tags":["f1tenth","motion planning","exploration","robotics","Multi-lingual"],"title":"CAD2CAV: Computer Aided Design for Cooperative Autonomous Vehicles","uri":"/cad2cav/"},{"categories":["Projects"],"content":"Demo This is a sample scenario of a single F1TENTH race car exploring the 2nd floor of Levine Hall at the University of Pennsylvania, right outside xLab. ","date":"2021-12-27","objectID":"/cad2cav/:1:0","tags":["f1tenth","motion planning","exploration","robotics","Multi-lingual"],"title":"CAD2CAV: Computer Aided Design for Cooperative Autonomous Vehicles","uri":"/cad2cav/"},{"categories":["Projects"],"content":"Motivation Autonomous robots have been widely used in a great number of aspects in our daily life. Specifically, mobile robots have demonstrated great help in unknown environment exploration, i.e., rescue robots exploring debris of a building after an earthquake, scientific robots exploring the world under the sea, etc. We are particularly interested in developing a fleet of mobile robots based on the dynamics of a normal self-driving vehicle (i.e., F1TENTH race cars) that helps people explore normal in-use buildings, or buildings that are still under construction. Sample image of an F1TENTH race car. Normally, one would quickly think of SLAM (Simultaneous Localization and Mapping) when it comes to robotic exploration (for more details, see my SLAM project). Yes, that is one of the most straightforward and simplest solution for a normal robotic exploration task. However, under the assumption of exploring a known building, we are actually given the access an extra layer of structural information for the target environment, i.e. the floor plan of the building. How to exploit that information and set up a multi-agent exploration task efficiently? That becomes the main goal for this project. ","date":"2021-12-27","objectID":"/cad2cav/:2:0","tags":["f1tenth","motion planning","exploration","robotics","Multi-lingual"],"title":"CAD2CAV: Computer Aided Design for Cooperative Autonomous Vehicles","uri":"/cad2cav/"},{"categories":["Projects"],"content":"Technical Details ","date":"2021-12-27","objectID":"/cad2cav/:3:0","tags":["f1tenth","motion planning","exploration","robotics","Multi-lingual"],"title":"CAD2CAV: Computer Aided Design for Cooperative Autonomous Vehicles","uri":"/cad2cav/"},{"categories":["Projects"],"content":"Overview The general structure of this project is divided into 3 modules: a central ROS server, a building renderer/simulator, and the F1TENTH race car onboard computing platform. A diagram for the system design of CAD2CAV. The figure above demonstrates the general relationship among the 3 modules. Given the prior knowledge of the building‚Äôs floor plan (which is assumed to come from Autodesk Revit), the building renderer would parse and construct a rendered 3D model our of it for visualization; the central ROS server would extract necessary structural information and construct 2D occupancy map for motion planning \u0026 mapping nodes. As soon as the central ROS server generates a balanced multi-agent routing scheme, it would pass the plan to each F1TENTH race car and the race car would then execute the drive node to drive the car and follow the high-level routing scheme in the real world. As the race car is driving in the environment, its perception module would try to localize itself in the occupancy map and update the perceived objects in the map. The map then gets updated in the central ROS server and the server would also pass the updated map to the building renderer. The building renderer would finally render everything and visualize the building in the software. ","date":"2021-12-27","objectID":"/cad2cav/:3:1","tags":["f1tenth","motion planning","exploration","robotics","Multi-lingual"],"title":"CAD2CAV: Computer Aided Design for Cooperative Autonomous Vehicles","uri":"/cad2cav/"},{"categories":["Projects"],"content":"Building Renderer As mentioned above, we need a building renderer to render 3D models and visualize everything on the screen. Our current choice is to use Unreal Engine 4. UE4 has a popular plugin called Unreal Datasmith compatible with most of the mainstream model designing softwares, i.e., Autodesk Revit, Cinema 4D (C4D), Rhino, etc. Users can install and try the plugin by following the tutorial of Exporting Datasmith Content from Revit and Importing Datasmith Content into Unreal Engine 4. We also hosted a pre-imported model of Levine Hall 4th floor on GitHub. Other Options We are also looking into trying out different options that are more friendly to robotics application development than UE4. One of them is the Nvidia Isaac Sim powered by the newly-announced Nvidia Omniverse platform. ","date":"2021-12-27","objectID":"/cad2cav/:3:2","tags":["f1tenth","motion planning","exploration","robotics","Multi-lingual"],"title":"CAD2CAV: Computer Aided Design for Cooperative Autonomous Vehicles","uri":"/cad2cav/"},{"categories":["Projects"],"content":"Central ROS Server The central ROS server is responsible for: 1) parsing floor plan from Revit and constructing initial occupancy grid; 2) generating high-level balanced multi-agent routing scheme from occupancy map; 3) updating occupancy map from the perception data; 4) sending occupancy map information over to building renderer. Each of these tasks is a sub-problem in different fields of robotics, in the general sense. Initial Map Construction from Revit Revit models are encoded in a unique format special to the software. Hence, the only way for us to extract information from a general Revit model is through the Revit API. Revit API is initially written in C#, while it has been recently transferred to Python by an open-source project called pyRevit. By following the official installation tutorials of pyRevit, I have managed to create a pyRevit plugin for the project to specifically export the 2D geometry information of all the main elements in a building floor plan. The current supported elements and their geometric representations are: Walls, represented as 2D line segments; Doors, represented as 2D points with orientation; Windows, represented as 2D points with orientation. The current pyRevit plugin is hosted on GitHub and is written in IronPython 2.7.1 It runs in Autodesk Revit 2022. Upon one button click, it would try to look for these elements in the model and print their geometric representation information in the console with the following specific format: Type,x_1,y_1,z_1,x_2,y_2,z_2,Orientation,Width,Height wall,-17.3718,12.2507,0.0,-16.7919,12.8305,0.0,0.0,0.0,0.0 door,-6.2356,2.5174,0.0,0.0,0.0,0.0,1.5708,0.864,2.032 window,-0.915,-3.4125,0.9,0.0,0.0,0.0,3.1416,1.05,1.35 Users are supposed to save the console outputs into a CSV file and put it manually in the ROS codebase. In the CSV format: Type is a string describing the object type; x_1, y_1, z_1 is the 3D Cartesian coordinate of the 1st endpoint of the line segment, or the position of the oriented point; x_2, y_2, z_2 is the 3D Cartesian coordinate of the 2nd endpoint of the line segment, or left blank if the type is an oriented point; Orientation is the orientation of the point, or left blank if the type is a line segment; Width and Height are the actual width and height of the doors and windows in the Revit model, and are left blank for the walls. After we have the necessary structural information in a floor plan contained in a CSV file, we can then proceed to load the file in ROS and construct an initial occupancy map out of it. Since all information are transformed into some basic geometric shapes, it is then very easy for us to draw these shapes on a 2D grid. The only algorithm we use here is the Bresenham‚Äôs line algorithm, which enables fast grid marching along a certain 2D line segment. We set everything cell in the grid that is part of the geometric shapes to be OCCUPIED and an initial map for the floor plan is then produced. Revit Occupancy Grid Waypoint Identification and Registration To generate a high-level routing scheme for multiple vehicles to explore, it is important to know which set of locations or areas that the exploration needs to cover. While we‚Äôre still exploring other options2, for simplicity we provide an interface for users to select and register waypoints on the map in a pop-up window. As soon as the waypoints are registered, the server would construct a graph out of all the waypoints, and the routing algorithms will be performed with the scope of an abstract graph. A screenshot of a sample user waypoint registration interface. Graph Planning and Routing All the high-level routing algorithms are conducted on an abstract graph. In this task we‚Äôre interested in assign an exploration route for each vehicle, such that all vehicles gets an exploration path of equal or similar cost to each other. The Capacitated Vehicle Routing Problem (CVRP) Vehicle Routing Problem (VRP) is a famous set of combinatorial optimization problems widely used ","date":"2021-12-27","objectID":"/cad2cav/:3:3","tags":["f1tenth","motion planning","exploration","robotics","Multi-lingual"],"title":"CAD2CAV: Computer Aided Design for Cooperative Autonomous Vehicles","uri":"/cad2cav/"},{"categories":["Projects"],"content":"F1TENTH Onboard Computing Platform The latest version of F1TENTH race car is equipped with a nice Nvidia Xavier NX Development Board as the onboard computing platform, which contains both an ARMv8 CPU and a CUDA-enabled GPU. It runs a complete but special Ubuntu system developed and shipped by Nvidia. In general, it is capable of performing part of the computationally heavy tasks in our project. In our design, we run the low-level planning and control node as well as the localization and mapping node onboard. Obstacle-Avoiding Planning and Control After we have a complete route for each vehicle, we need to design an algorithm for the vehicle to actually reach those waypoints in the real world. Hence, the algorithm is required to be fast, accurate, and responsive. In this task, we choose to apply the Fast Marching Tree (FMT*) algorithm as a low-level planner to generate real-time trajectories for each vehicle. As a sampling-based planning algorithm, FMT* is fast enough to replan in a very short amount of time so that the vehicle is able to avoid obstacles. As a general overview, FMT* combines the properties of two different sampling-based planning categories: PRM (Probabilistic Road Map) and RRT (Rapidly-exploring Random Trees). It first samples a set of points from the planning space (or configuration space), and tries to grow a tree from the start to the end in an RRT-style. It defines an unvisited set $V_\\text{unvisited}$, an open set $V_\\text{open}$ and a closed set $V_\\text{closed}$. As the following figure shows, it involves the following steps: Add the starting point to $V_\\text{closed}$. Add all of the neighboring points of the starting point to $V_\\text{open}$. All the other points are in $V_\\text{unvisited}$. Select the lowest cost node $z\\in V_\\text{open}$ and finds its neighbors $\\mathcal{N}(z)\\subseteq V_\\text{unvisited}$. Given a node $x\\in\\mathcal{N}(z)$, find the optimal 1-step connection to its neighboring nodes $\\mathcal{N}(x)$ in $V_\\text{open}$. Check collision on all newly-added connections and apply penalties for violations (i.e., remove all the collision connections); Add all successful connected nodes to $V_\\text{open}$, and move $z$ to $V_\\text{closed}$. Repeat the operations from Step 3 until the goal is reached. FMT* algorithm cited from the paper. FMT* works as a low-level planner on the vehicle and generates trajectory towards the next waypoint in the high-level route. For now, the trajectory consists of a set of more fine-grained 2D points. For future work orientation can be added to the trajectory point as an extra information so that the vehicle could be better controlled in terms of steering. As for the control algorithm, we currently run the traditional pure-pursuit algorithm to drive the vehicle to follow the trajectory points. For a more detailed overview of the pure-pursuit algorithm please checkout my F1TENTH Lab 6 report. Future work on the control algorithm involves developing a model predictive control (MPC) algorithm, but for the sake of simplicity we still stick to pure-pursuit for the moment. Localization In a real-world experiment, each vehicle needs to know where it is in the map. Hence, it is very important for the vehicle to have a localization module onboard. Common localization algorithms in general robotics include all kinds of filtering algorithms, from Bayes filer, Kalman filter (KF) and its variations (EKF, UKF, etc.), and particle filter. We choose to use particle filter as a starting point and incorporated a CUDA-accelerated package of particle filter5 from the MIT race car team. Thanks to my project partner Shumin for her work on the localization module. The rest of this section is reposted from Shumin‚Äôs article. Overview Mapping and localization algorithms require a manual walkthrough and capture of the environment. Upon loop closure they are ready to navigate with algorithms like SLAM. This project‚Äôs goal is to improve this process for indoor spaces with two steps: Chea","date":"2021-12-27","objectID":"/cad2cav/:3:4","tags":["f1tenth","motion planning","exploration","robotics","Multi-lingual"],"title":"CAD2CAV: Computer Aided Design for Cooperative Autonomous Vehicles","uri":"/cad2cav/"},{"categories":["Projects"],"content":"References eirannejad/pyRevit. mit-racecar/particle_filter. google/or-tools. AprilRobotics/apriltag. ben-strasser/fast-cpp-csv-parser. leggedrobotics/darknet_ros. A. Y. Ng, M. I. Jordan, and Y. Weiss, ‚ÄúOn spectral clustering: Analysis and an algorithm,‚Äù in Proceedings of the 14th International Conference on Neural Information Processing Systems: Natural and Synthetic, ser. NIPS‚Äô01. Cambridge, MA, USA: MIT Press, 2001, p. 849‚Äì856. B. Lau, C. Sprunk, and W. Burgard, ‚ÄúImproved updating of euclidean distance maps and Voronoi diagrams,‚Äù in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems, Taipei, Taiwan, 2010. [Online]. Available: http://ais.informatik.uni-freiburg.de/publications/papers/lau10iros.pdf B. W. Kernighan and S. Lin, ‚ÄúAn efficient heuristic procedure for partitioning graphs,‚Äù The Bell System Technical Journal, vol. 49, no. 2, pp. 291‚Äì307, 1970. C. Johnson, ‚ÄúTopological mapping and navigation in real-world environments,‚Äù Ph.D. dissertation, University of Michigan, 2018. Available: https://deepblue.lib.umich.edu/handle/2027.42/144014 D. Arthur and S. Vassilvitskii, ‚ÄúK-means++: The advantages of careful seeding,‚Äù in Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, ser. SODA ‚Äô07. USA: Society for Industrial and Applied Mathematics, 2007, p. 1027‚Äì1035. E. Olson, ‚ÄúAprilTag: A robust and flexible visual fiducial system,‚Äù 2011 IEEE International Conference on Robotics and Automation, 2011, pp. 3400-3407, doi: 10.1109/ICRA.2011.5979561. G. Karypis and V. Kumar, ‚ÄúMultilevel k-way partitioning scheme for irregular graphs,‚Äù Journal of Parallel and Distributed Computing, vol. 48, no. 1, pp. 96‚Äì129, 1998. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S0743731597914040 J. Wang and E. Olson, ‚ÄúAprilTag 2: Efficient and robust fiducial detection,‚Äù 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2016, pp. 4193-4198, doi: 10.1109/IROS.2016.7759617. L. Janson, E. Schmerling, A. Clark, and M. Pavone, ‚ÄúFast marching tree: a fast marching sampling-based method for optimal motion planning in many dimensions,‚Äù 2015. S. L. Bowman, N. Atanasov, K. Daniilidis and G. J. Pappas, ‚ÄúProbabilistic data association for semantic SLAM,‚Äù 2017 IEEE International Conference on Robotics and Automation (ICRA), 2017, pp. 1722-1729, doi: 10.1109/ICRA.2017.7989203. Yang, Zhong \u0026 Yao, Baozhen. (2009). ‚ÄúAn Improved Ant Colony Optimization for Vehicle Routing Problem.‚Äù European Journal of Operational Research. 196. 171-176. 10.1016/j.ejor.2008.02.028. IronPython is the Python extension for Microsoft .Net environment. It is used by pyRevit as the default Python wrapper over C# Revit API.¬†‚Ü©Ô∏é Other options include the algorithm of extracting a Voronoi skeleton out of a general occupancy map. This work was initially proposed by Lau et al. in the IROS 2010 paper ‚ÄúImproved updating of euclidean distance maps and Voronoi diagrams‚Äù.¬†‚Ü©Ô∏é From Wikipedia: Vehicle Routing Problem.¬†‚Ü©Ô∏é See METIS and ‚ÄúMultilevel k-way partitioning scheme for irregular graphs‚Äù.¬†‚Ü©Ô∏é See mit-racecar/particle_filter.¬†‚Ü©Ô∏é ","date":"2021-12-27","objectID":"/cad2cav/:4:0","tags":["f1tenth","motion planning","exploration","robotics","Multi-lingual"],"title":"CAD2CAV: Computer Aided Design for Cooperative Autonomous Vehicles","uri":"/cad2cav/"},{"categories":["Projects"],"content":"This is a project of a Vulkan implementation of Fast Volume Rendering with Spatiotemporal Reservoir Resampling. It achieves the following: Vulkan ray tracing pipeline with nvpro and Vulkan Ray Tracing KHR extension. Volume assets loading and rendering through OpenVDB. ReSTIR algorithm rendering on GLTF scene and volume assets. ","date":"2021-12-27","objectID":"/565-final-project/:0:0","tags":["computer graphics","ray tracing","restir","volume rendering","vulkan","Multi-lingual"],"title":"Volume Rendered ReSTIR in Vulkan","uri":"/565-final-project/"},{"categories":["Projects"],"content":"Authors Zhihao Ruan (ruanzh@seas.upenn.edu), Shubham Sharma (sshubh@seas.upenn.edu), Raymond Yang (rayyang@seas.upenn.edu) Tested on: Windows 10 Home 21H1 Build 19043.1288, Ryzen 7 3700X @ 3.59GHz 48GB, RTX 2060 Super 8GB This project requires an RTX-compatible (Vulkan Ray Tracing KHR-compatible) graphics card to run. ","date":"2021-12-27","objectID":"/565-final-project/:0:1","tags":["computer graphics","ray tracing","restir","volume rendering","vulkan","Multi-lingual"],"title":"Volume Rendered ReSTIR in Vulkan","uri":"/565-final-project/"},{"categories":["Projects"],"content":"Demos Volume-ReSTIR Normal ReSTIR Ray Traced Volume Rendering (with OBJ Models) Ray Traced Volume Rendering Rasterization vs. Path Tracing Rasterization Path Tracing ReSTIR ","date":"2021-12-27","objectID":"/565-final-project/:0:2","tags":["computer graphics","ray tracing","restir","volume rendering","vulkan","Multi-lingual"],"title":"Volume Rendered ReSTIR in Vulkan","uri":"/565-final-project/"},{"categories":["Projects"],"content":"Introduction ReSTIR has been a very successful fast path tracing-based rendering algorithm in recent computer graphics. However, the current state-of-the-art ReSTIR algorithm only works with meshes. Moreover, there exists some common objects in the scene that are not suitable for mesh creation. VDB asset is a special kind of asset that compensates the drawback of meshes and provides a much more accurate description for volume-based objects, such as smokes, clouds, fire flames, etc. Therefore, it is of great importance to migrate current ReSTIR procedures on volume assets so that we could also render the smokes and clouds photo-realistically and efficiently. VDB Data Structures VDB is a special type of data structure for smokes, clouds, fire flames, etc. that is based on hierarchical voxel grids. It essentially holds a set of particles. It also uses a similar tree-like data structure as scene graphs for fast traversal and access that stores all transformations at intermediate nodes, and only the particle positions at leaf nodes. A VDB asset of a pot. A VDB illustration of a smoke asset. A diagram for VDB data structure. ReSTIR Algorithm Source: Spatiotemporal reservoir resampling for real-time ray tracing with dynamic direct lighting Fast Volume Rendering with Spatiotemporal Reservoir Resampling ReSTIR algorithm is a special ray tracing-based rendering algorithm that deals with large number of light source efficiently. It takes advantage of alias tables for Resampled Importance Sampling (RIS) and flexible reservoir data structures. RIS effectively culls images of low weight lights and constructs a PDF (Probability Distribution Function) of lights for the scene. The reservoirs, allocated one for each pixel, map geometry collisions to light sources in the scene. The reservoirs are easily updated for every bounce, each time considering a candidate from a subset of all lights. If the candidate is chosen, the reservoir will map the geometry to the new light. As more samples are considered, it becomes less likely for any candidate to be placed into the reservoir. These reservoirs use a combination of probability and giving up precision to generate result pixel color such that the image converges in near real time. Vulkan Ray Tracing Vulkan is considered as the next-generation API for uniform graphics drivers. It is fast, efficient, light weight, but yet verbose. Vulkan users needs to explicitly set up every little details of the entire rendering pipeline, which is often quite problematic. In this project we provide an explicit example of setting up a working Vulkan rendering pipeline, and it generally involves the following procedures: Set up glfw to work with the latest version of Vulkan. Initialize Vulkan instance, Vulkan physical device, Vulkan logical device, Vulkan swapchain (for passing to frame buffer display), Vulkan graphics command queue. Create all buffers to be passed to device memory. This includes: frame buffer, rendering pipeline description buffer, data buffer (primitive vertices, indices, normals, materials, textures, etc.). Create descriptor set for all device buffers. Vulkan descriptor set defines the stage that GPU reads the data (either in vertex shader, fragment shader, or any stage in the ray tracing pipeline) and the way that GPU reads the data (either as uniform samplers, uniform images, storage buffers, uniform buffers, etc.). Bind the data buffer with the corresponding descriptor set. Create the graphics pipeline. Graphics pipelines are defined in VkPipeline, which specifies the actual procedures of an entire render pass. It can either be defined as a rasterization pipeline (vertex shader ‚Äì\u003e primitive assembly ‚Äì\u003e rasterization ‚Äì\u003e fragment shader), or a ray tracing pipeline (ray generation shader ‚Äì\u003e ray intersection shader ‚Äì\u003e ray closest hit shader/ray miss shader ‚Äì\u003e post processing fragment shader). Get the current command buffer, prepare the frame, and run the pre-defined pipeline with all device buffers p","date":"2021-12-27","objectID":"/565-final-project/:0:3","tags":["computer graphics","ray tracing","restir","volume rendering","vulkan","Multi-lingual"],"title":"Volume Rendered ReSTIR in Vulkan","uri":"/565-final-project/"},{"categories":["Projects"],"content":"References Daqi Lin, Chris Wyman, Cem Yuksel. Fast Volume Rendering with Spatiotemporal Reservoir Resampling. ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia 2021), 40, 6, 2021. Benedikt Bitterli, Chris Wyman, Matt Pharr, Peter Shirley, Aaron Lefohn, and Wojciech Jarosz. 2020. Spatiotemporal reservoir resampling for real-time ray tracing with dynamic direct lighting. ACM Trans. Graph. 39, 4, Article 148 (July 2020), 17 pages. DOI:https://doi.org/10.1145/3386569.3392481 Volume Rendering Volume Rendering (Nvidia) Ray Tracing Gems II Vulkan Ray Tracing Tutorial OpenVDB for VDB data loading. nvpro for Vulkan Ray Tracing KHR setup. spdlog for fast C++ logging. nvpro-samples/vk_mini_path_tracer nvpro-samples/vk_raytracing_tutorial_KHR dipmizu914/ReSTIR_on_Vulkan Common graphics utilities glfw, glm, stb. These are all included in the nvpro. ","date":"2021-12-27","objectID":"/565-final-project/:0:4","tags":["computer graphics","ray tracing","restir","volume rendering","vulkan","Multi-lingual"],"title":"Volume Rendered ReSTIR in Vulkan","uri":"/565-final-project/"},{"categories":["Projects"],"content":" Tested on: Windows 10 Home 21H1 Build 19043.1288, Ryzen 7 3700X @ 3.59GHz 48GB, RTX 2060 Super 8GB ","date":"2021-12-27","objectID":"/565-vulkan-grass-rendering/:0:0","tags":["computer graphics","rendering","vulkan","Multi-lingual"],"title":"Vulkan Grass Rendering","uri":"/565-vulkan-grass-rendering/"},{"categories":["Projects"],"content":"Highlights This project implements physically-based grass rendering \u0026 culling with Vulkan compute shaders: Physically-based real-time rendering of grass blades; 3 different culling tests: orientation culling, view-frustum culling, distance culling; Tessellating Bezier curves into grass blades with GLSL tessellation shader A detailed instruction of this project can be found here. ","date":"2021-12-27","objectID":"/565-vulkan-grass-rendering/:0:1","tags":["computer graphics","rendering","vulkan","Multi-lingual"],"title":"Vulkan Grass Rendering","uri":"/565-vulkan-grass-rendering/"},{"categories":["Projects"],"content":"Vulkan Vulkan is considered as the next-generation graphics API developed by Khronos group, in replacement for the old OpenGL. It is fast, high-performance, and cross-platform. However, the downside of this API is that it exposes all details of GPU hardware to users for possible customization, so that it generally takes a significantly large amount of code for users to set up the rendering pipeline than OpenGL. This project sets up a standard Vulkan graphics pipeline with compute shaders \u0026 tessellation shaders. The goal of this project is to implement the paper Responsive Real-Time Grass Rendering for General 3D Scenes for effective grass simulation in 3D rendering. ","date":"2021-12-27","objectID":"/565-vulkan-grass-rendering/:0:2","tags":["computer graphics","rendering","vulkan","Multi-lingual"],"title":"Vulkan Grass Rendering","uri":"/565-vulkan-grass-rendering/"},{"categories":["Projects"],"content":"Responsive Real-Time Grass Rendering for General 3D Scenes Grass is a critical component in 3D graphics. Almost every modern video games have some scenes that involve the rendering of grass. It is also a typical example of massive parallelization, as with the power of GPU one can easily think of ways to render every grass blade with a single GPU thread for photorealistic effects. However, approximations are still needed, and the paper chooses Bezier curves. Bezier curves are a special family of curve that produces smooth splines geometry in 3D space. The generation of Bezier curve requires pre-defining multiple control points, and the curve would then be generated following a fixed function that interpolates between adjacent control points (for more details, see Wikipedia). Basic Geometry From the paper, a single grass blade can be defined as a Bezier curve with 3 control points: v0, v1, v2. v0 is the base point specifying the 3D position of the base of the grass blade, v1 is the guide for the curve that is always ‚Äúabove‚Äù v0 and defines the blade‚Äôs up direction, and v2 is the control points where we execute all kinds of simulation forces. The paper provides a very detailed usage for tweaking basic 3D geometry shapes for rendering with Bezier curves (see Section 6.3 of paper). Physics Simulation As for physical simulation, we define the following basic forces: Gravity force that pulls v2 to the ground, creating bends on the blade; Recovery force that tends to recover the up-straight pose of the blade; Wind force that produces random displacement on the tip of the blade (specifically, on v2). Blade Culling Blade culling is an optimization technique that the paper mentions to improve FPS in real-time rendering. By culling we do not wish to render the grass blades that: are too far away; are outside the camera‚Äôs view frustum; are not facing towards the camera. Culling tests are conducted in the compute shader so that the number of grass blades that need rendering varies with the position and orientation of the camera. ","date":"2021-12-27","objectID":"/565-vulkan-grass-rendering/:0:3","tags":["computer graphics","rendering","vulkan","Multi-lingual"],"title":"Vulkan Grass Rendering","uri":"/565-vulkan-grass-rendering/"},{"categories":["Projects"],"content":"Visual Effects The following images demonstrates effects due to different culling operations: Orientation Culling View-Frustum Culling Distance Culling The following image demonstrates the up-straight pose of the blade (no physics forces, no culling): ","date":"2021-12-27","objectID":"/565-vulkan-grass-rendering/:0:4","tags":["computer graphics","rendering","vulkan","Multi-lingual"],"title":"Vulkan Grass Rendering","uri":"/565-vulkan-grass-rendering/"},{"categories":["Projects"],"content":"Performance Analysis The following FPS tests on different culling options is conducted with NUM_BLADES = 1 \u003c\u003c 18: We can see that orientation culling improves the rendering FPS the most, while all the others also have some improvements on the FPS. ","date":"2021-12-27","objectID":"/565-vulkan-grass-rendering/:0:5","tags":["computer graphics","rendering","vulkan","Multi-lingual"],"title":"Vulkan Grass Rendering","uri":"/565-vulkan-grass-rendering/"},{"categories":["Projects"],"content":" Tested on: Ubuntu 20.04 LTS, Ryzen 3700X @ 2.22GHz 48GB, RTX 2060 Super @ 7976MB ","date":"2021-12-27","objectID":"/565-cuda-path-tracer/:0:0","tags":["CUDA","computer graphics","ray tracing","Multi-lingual"],"title":"CUDA Path Tracer with √Ä-Trous Denoiser","uri":"/565-cuda-path-tracer/"},{"categories":["Projects"],"content":"CUDA Path Tracer ","date":"2021-12-27","objectID":"/565-cuda-path-tracer/:1:0","tags":["CUDA","computer graphics","ray tracing","Multi-lingual"],"title":"CUDA Path Tracer with √Ä-Trous Denoiser","uri":"/565-cuda-path-tracer/"},{"categories":["Projects"],"content":"Highlights Finished path tracing core features: diffuse shaders perfect specular reflection 1st-bounce ray intersection caching radix sort by material type path continuation/termination by Thrust stream compaction Finished Advanced Features: Refraction with Fresnel effects using Schlick‚Äôs approximation Stochastic sampled anti-aliasing Physically-based depth of field OBJ mesh loading with tinyobjloader ","date":"2021-12-27","objectID":"/565-cuda-path-tracer/:1:1","tags":["CUDA","computer graphics","ray tracing","Multi-lingual"],"title":"CUDA Path Tracer with √Ä-Trous Denoiser","uri":"/565-cuda-path-tracer/"},{"categories":["Projects"],"content":"Background: Ray Tracing Ray tracing is a technique commonly used in rendering. Essentially it mimics the actual physical behavior of light: shoots a ray from every pixel in an image and calculates the final color of the ray by bouncing it off on every surface it hits in the world, until it reaches the light source. In practice a maximum number of depth (bouncing times) would be specified, so that we would not have to deal with infinitely bouncing rays. Ray tracing is one of the applications considered as ‚Äúembarrassingly parallel‚Äù, given the fact that each ray is completely independent of other rays. Hence, it is best to run on an GPU which is capable of providing hundreds of thousands of threads for parallel algorithms. This project aims at developing a CUDA-based application for customized ray tracing, and a detailed instruction can be found here. BSDF: Bidirectional Scattering Distribution Functions BSDF is a collection of models that approximates the light behavior in real world. It is commonly known to consist of BRDF (Reflection) models and BTDF (Transmission) models. Some typical reflection models include: Ideal specular (perfect mirror, glm::reflect) Ideal diffuse. It is a model that the direction of the reflected ray is randomly sampled from the incident hemisphere, along with other advanced sampling methods. Microfacet, such as Disney model. Glossy specular. Subsurface scattering. ‚Ä¶ Some typical transmission models include: Fresnel effect refraction. It consists of a regular transmission model based on Snell‚Äôs law and a partially reflective model based on Fresnel effect and its Schlick‚Äôs approximations. ‚Ä¶ CUDA Optimization for Ray Tracing In order to better utilize CUDA hardware for ray tracing, it is not suggested to parallelize each pixel, as it would lead to a huge amount of divergence. Instead, one typical optimization people use is to parallelize each ray, and uses stream compaction to remove those rays that terminates early. By this means we could better recycle the early ending warps for ray tracing other pixels. ","date":"2021-12-27","objectID":"/565-cuda-path-tracer/:1:2","tags":["CUDA","computer graphics","ray tracing","Multi-lingual"],"title":"CUDA Path Tracer with √Ä-Trous Denoiser","uri":"/565-cuda-path-tracer/"},{"categories":["Projects"],"content":"Results and Demos Ray Refraction for Glass-like Materials Perfect Specular Reflection Glass-like Refraction Stochastic Sampled Anti-Aliasing 1x Anti-Aliasing (Feature OFF) 4x Anti-Aliasing Physically-Based Depth of Field Pinhole Camera Model (Feature OFF) Thin-Lens Camera Model Mesh Loading Mesh loading has not been fully supported due to an incorrect normal vector parsing issue in tinyobjloader. The runtime is also unoptimized and takes an unreasonable amount of time to run. TODO: Bounding volume culling with AABB box/OBB box. ","date":"2021-12-27","objectID":"/565-cuda-path-tracer/:1:3","tags":["CUDA","computer graphics","ray tracing","Multi-lingual"],"title":"CUDA Path Tracer with √Ä-Trous Denoiser","uri":"/565-cuda-path-tracer/"},{"categories":["Projects"],"content":"Performance Analysis Throughout the project two optimizations were done: Cache ray first bounce. For every iteration in a rendering process, the first rays that shoot from camera to the first hit surface is always the same. Therefore we can cache the first shooting ray during the first iteration and reuse the results in all the following iterations. Radix-sort hit attributes by material type. The calculation of resulting colors for each ray depends on the material of the object it hits, and thus we can sort the rays based on material type before shading to enforce CUDA memory coalescence. The following experiments are conducted on a Cornell box scene as shown in cornell_profiling.txt with varying iterations from 300 to 2000. From the performance analysis we can see that caching the first bounce for the 1st iteration has a slight improvement on the performance, while radix-sorting the material type before shading have a great negative impact on the performance. This is possibly due to the reason that radix-sorting itself takes a lot amount of time in each iteration. ","date":"2021-12-27","objectID":"/565-cuda-path-tracer/:1:4","tags":["CUDA","computer graphics","ray tracing","Multi-lingual"],"title":"CUDA Path Tracer with √Ä-Trous Denoiser","uri":"/565-cuda-path-tracer/"},{"categories":["Projects"],"content":"CUDA Denoiser ","date":"2021-12-27","objectID":"/565-cuda-path-tracer/:2:0","tags":["CUDA","computer graphics","ray tracing","Multi-lingual"],"title":"CUDA Path Tracer with √Ä-Trous Denoiser","uri":"/565-cuda-path-tracer/"},{"categories":["Projects"],"content":"Physically-Based Ray Traced (PBRT) Image with √Ä-Trous Denoising PBRT, 5000 iterations PBRT, 5000 iterations, 4x anti-aliasing PBRT, 100 iterations PBRT, 100 iterations, √Ä-Trous Denoising ","date":"2021-12-27","objectID":"/565-cuda-path-tracer/:2:1","tags":["CUDA","computer graphics","ray tracing","Multi-lingual"],"title":"CUDA Path Tracer with √Ä-Trous Denoiser","uri":"/565-cuda-path-tracer/"},{"categories":["Projects"],"content":"Highlights Implemented Edge-Avoiding √Ä-Trous Wavelet Transform denoising techniques with 5x5 Gaussian blur kernel. ","date":"2021-12-27","objectID":"/565-cuda-path-tracer/:2:2","tags":["CUDA","computer graphics","ray tracing","Multi-lingual"],"title":"CUDA Path Tracer with √Ä-Trous Denoiser","uri":"/565-cuda-path-tracer/"},{"categories":["Projects"],"content":"Introduction Physically-Based Ray Tracing (PBRT) is considered as one of the best methods that produce the most photorealistic images. The essence of PBRT is to approximate the rendering equation with Monte-Carlo integration, sampling multiple rays from each pixel of an image and bouncing them off multiple times from various surfaces of different kinds and different textures according to the ray directions, accumulating the colors along the way. However, in reality it is very hard to run PBRT in real time, as we often need a large amount of rays to obtain a reasonable good approximation for the rendering equation. Hence, people come up with multiple ways of applying denoising techniques on partially ray-traced images, hoping that with denoising we could get reasonably good photorealistic images while terminating PBRT early. In this project, we explored Edge-Avoiding √Ä-Trous Wavelet Transform for image denoising. For more details, please checkout the project instruction. The word ‚Äú√Ä-Trous‚Äù with meaning ‚Äúwith holes‚Äù comes from Algorithme √Ä-Trous, which is a stationary wavelet transform commonly used in computer graphics to approximate the effect of a Gaussian filter with much faster performance. It starts from a fix-sized Gaussian filter, performs convolution on the image while expanding out each element of the filter at every iteration, filling all missing entries with 0s. The ‚ÄúEdge-Avoiding‚Äù part of the algorithm incorporates the use of a pixel-wise GBuffer, storing positions and normals of the first hit for each ray. When the image is denoised, information of the GBuffer will be used to avoid blurring edges in the image, while the noisy surfaces are smoothed. Pure √Ä-Trous Filtering √Ä-Trous Filtering with Edge-Avoiding GBuffer ","date":"2021-12-27","objectID":"/565-cuda-path-tracer/:2:3","tags":["CUDA","computer graphics","ray tracing","Multi-lingual"],"title":"CUDA Path Tracer with √Ä-Trous Denoiser","uri":"/565-cuda-path-tracer/"},{"categories":["Projects"],"content":"Qualitative Analysis Visual Results vs. Filter Size The following experiments are run with c_phi=132.353, n_phi=0.245, p_phi=1.324. Filter Size = 6 Filter Size = 15 Filter Size = 32 Filter Size = 100 From the results we can see that the visual results does not vary uniformly with the filter size. When the filter reaches some size threshold, it is no longer the filter size that stops the smoothing process but the weights of the GBuffer instead. Visual Results vs. Material Type The following experiments are run with c_phi=132.353, n_phi=0.245, p_phi=1.324, filter_size=100. Diffuse Reflective Refractive From the results we can see that the filter works best with diffuse materials, reasonably well with refractive materials, and the worst with reflective materials. This is mainly because the current implementation only caches the position \u0026 normal vectors of the first hit, while this property does not really apply to reflective materials (colors on a pure-reflective material depend heavily on the material properties from the 2nd hit). As a result, the filter is blurring the reflection on the reflective materials. Visual Results vs. Light Conditions The following experiments are run with c_phi=132.353, n_phi=0.245, p_phi=1.324, filter_size=100. Cornell Box Cornell Box with Large Lights From the results we can see that the filter works better in brighter lighting conditions. This is because in brighter lighting conditions the color tends to be more similar locally, while with point lights the color differs much from its adjacent pixels, making it more difficult to smooth. ","date":"2021-12-27","objectID":"/565-cuda-path-tracer/:2:4","tags":["CUDA","computer graphics","ray tracing","Multi-lingual"],"title":"CUDA Path Tracer with √Ä-Trous Denoiser","uri":"/565-cuda-path-tracer/"},{"categories":["Projects"],"content":"Quantitative Analysis Denoising Time For a standard scene as shown in Physically-Based Ray Traced (PBRT) Image with √Ä-Trous Denoising, the denoising time for a 800x800 image with c_phi=132.353, n_phi=0.245, p_phi=1.324, filter_size=100 is: ----- Begin image denoising ----- elapsed time: 31.2983ms (CUDA Measured) which is a reasonably fast denoising time. Number of Iterations Needed for a Smooth Image The following experiments are run with c_phi=132.353, n_phi=0.245, p_phi=1.324, filter_size=100. Iteration = 1 Iteration = 10 Iteration = 25 Iteration = 50 Iteration = 100 From the results we can see that subjectively, with the parameters specified above, we can get a reasonably smooth image with number of iterations at least 50. Denoising Runtime vs. Resolution, Filter Size The following experiments are run with c_phi=132.353, n_phi=0.245, p_phi=1.324. For varying resolution, filter_size = 100; for varying filter size, resolution = 800x800. Runtime vs. Resolution Runtime vs. Filter Size From the results we can see that the runtime increases quadratically with resolution as the number of pixels increases quadratically with resolution; the runtime increases linearly with filter size, as increasing filter size would only increase the number of iterations that √Ä-Trous wavelet transform needs to run. ","date":"2021-12-27","objectID":"/565-cuda-path-tracer/:2:5","tags":["CUDA","computer graphics","ray tracing","Multi-lingual"],"title":"CUDA Path Tracer with √Ä-Trous Denoiser","uri":"/565-cuda-path-tracer/"},{"categories":["Projects"],"content":"References [PBRT] Physically Based Rendering, Second Edition: From Theory To Implementation. Pharr, Matt and Humphreys, Greg. 2010. Antialiasing and Raytracing. Chris Cooksey and Paul Bourke, http://paulbourke.net/miscellaneous/aliasing/ Sampling notes from Steve Rotenberg and Matteo Mannino, University of California, San Diego, CSE168: Rendering Algorithms Path Tracer Readme Samples (non-exhaustive list): https://github.com/byumjin/Project3-CUDA-Path-Tracer https://github.com/lukedan/Project3-CUDA-Path-Tracer https://github.com/botforge/CUDA-Path-Tracer https://github.com/taylornelms15/Project3-CUDA-Path-Tracer https://github.com/emily-vo/cuda-pathtrace https://github.com/ascn/toki https://github.com/gracelgilbert/Project3-CUDA-Path-Tracer https://github.com/vasumahesh1/Project3-CUDA-Path-Tracer Edge-Avoiding A-Trous Wavelet Transform for fast Global Illumination Filtering Spatiotemporal Variance-Guided Filtering A Survey of Efficient Representations for Independent Unit Vectors ocornut/imgui - https://github.com/ocornut/imgui ","date":"2021-12-27","objectID":"/565-cuda-path-tracer/:3:0","tags":["CUDA","computer graphics","ray tracing","Multi-lingual"],"title":"CUDA Path Tracer with √Ä-Trous Denoiser","uri":"/565-cuda-path-tracer/"},{"categories":["My Gallery"],"content":"In this winter, I finally got a chance to visit and spend a nice Christmas holiday in UK. Trafalgar Square Tower Bridge Christmas at Regent Street London is famous for its theaters at West End. Along with New York City‚Äôs Broadway theatre, West End theatre is usually considered to represent the highest level of commercial theatre in the English-speaking world. Seeing a West End show is a common tourist activity in London. London musicals at West End ","date":"2021-12-26","objectID":"/london/:0:0","tags":["photography","city representation"],"title":"London, UK","uri":"/london/"},{"categories":["Projects"],"content":"F1Tenth Course Lab 6","date":"2021-01-29","objectID":"/f1tenth-lab6/","tags":["F1Tenth","F1Tenth Racing","Multi-lingual"],"title":"My F1TENTH Journey ‚Äî Lab 6, Pure Pursuit (Waypoint Tracker)","uri":"/f1tenth-lab6/"},{"categories":["Projects"],"content":"This series of blogs marks the journey of my F1/10 Autonomous Racing Cars. All my source codes can be accessed here. Previous post: My F1TENTH Journey ‚Äî Lab 1, Introduction to ROS My F1TENTH Journey ‚Äî Lab 2, Automatic Emergency Braking My F1TENTH Journey ‚Äî Lab 3, PID-Controlled Wall Follower My F1TENTH Journey ‚Äî Lab 4, Reactive Planning Methods for Obstacle Avoidance ","date":"2021-01-29","objectID":"/f1tenth-lab6/:0:0","tags":["F1Tenth","F1Tenth Racing","Multi-lingual"],"title":"My F1TENTH Journey ‚Äî Lab 6, Pure Pursuit (Waypoint Tracker)","uri":"/f1tenth-lab6/"},{"categories":["Projects"],"content":"Overview In autonomous driving development, recording waypoints for future testing is very important. One way to record waypoints efficiently is only remembering the x, y, z coordinates of the vehicle regularly for some constant time gap. However, as ground vehicles are generally non-holonomic (i.e., the vehicle cannot make a turn in place), it is non-trivial to control an autonomous vehicle to exactly follow these pre-recorded waypoints. Hence, we need to design a path tracker (waypoint tracker) so that it can calculate and send commands to the vehicle to make it follow the pre-recorded waypoints. That is how ‚ÄúPure Pursuit‚Äù algorithm comes into place. ","date":"2021-01-29","objectID":"/f1tenth-lab6/:1:0","tags":["F1Tenth","F1Tenth Racing","Multi-lingual"],"title":"My F1TENTH Journey ‚Äî Lab 6, Pure Pursuit (Waypoint Tracker)","uri":"/f1tenth-lab6/"},{"categories":["Projects"],"content":"The Pure Pursuit Algorithm The Pure Pursuit algorithm accounts for the non-holonomic constraints and the speed of the vehicle when it follows the waypoints. Given the current position of the vehicle, what it does is to always steer towards the next appropriate waypoint according to some lookahead distance $L$. In real practice, Pure Pursuit algorithm is often used with localization methods, i.e., a particle filter. But since the vehicle is non-holonomic, how can we steer towards the next waypoint properly? Setting the exact direction of the next waypoint from the current vehicle position is not feasible, as the vehicle is always in high speed and might overshoot the desired goal. One alternative option tries to interpolate the trajectory by finding an arc between the current position and the next waypoint (goal), which can be showed as the figure below. Arc interpolation between current position and goal. However there are infinitely many arcs between two points, and each of them has a different radius (curvature). Therefore we would like to add one more constraint to the arc: its center of radius must lie on the $y$ axis. In such way we can always find a unique arc interpolation for our trajectory, and it can be solved by simple math. After solving the geometry we can get the radius of the arc: $$r=\\frac{L^2}{2|y|}$$ Finding the steering angle from the arc interpolation. Finally, we can simply set the steering angle proportional to the curvature of the arc: double steering_angle = K_p * curvature; K_p works exactly the same as a P controller and its value can also be tuned using PID tuning theory. In summary, the Pure Pursuit algorithm can be implemented in the following steps: Find the next waypoint (goal) from the current position and the list of all waypoints. Interpolate an arc between current position and goal, and calculate its radius \u0026 curvature. Set the steering angle proportional to the curvature. ","date":"2021-01-29","objectID":"/f1tenth-lab6/:2:0","tags":["F1Tenth","F1Tenth Racing","Multi-lingual"],"title":"My F1TENTH Journey ‚Äî Lab 6, Pure Pursuit (Waypoint Tracker)","uri":"/f1tenth-lab6/"},{"categories":["Projects"],"content":"Actual Implementation This section summarizes a list of issues when I was implementing the pure pursuit algorithm in ROS and C++. Running the particle filter. In real practice we can never know the ground truth position of the vehicle. The position of the vehicle can only be measured using some kind of sensors and algorithms. For this lab I made use of a decent particle filter ROS package written by MIT car racing team. As I was using ROS Noetic with Python 3 while this package was written in Python 2, I made a fork of their project and spent some time converting their code into Python 3 to fit in my workspace. For future reference, the Python 3 version of this particle filter can be accessed here. It is worth mentioning that this particle filter provides several options for laser beam ray computing, and all of them depends on a ray-marching library called RangeLibc. The original RangeLibc was also written in Python 2 and I also made a fork of it and converted it into Python 3. With my modification the entire particle filter pipeline should now be able to run successfully in Python 3 and ROS Noetic. Choosing the goal waypoint from current position and lookahead distance. In general, there are multiple ways to choose the goal waypoint for the vehicle. Some options include choosing the waypoint in the list that has distance closest to $L$ from the current vehicle position, interpolating a waypoint with distance exactly equal to $L$ from the closest waypoint in the range and out of the range, and choosing the waypoint closest to $L$ from those out of the range. As for my implementation, I chose to implement in the following way: Starting from the current position, pick the first waypoint that has a distance larger than $L$ from the current position. Notice that my implementation is current not the best option, as it has not incorporate the current orientation of the car and it might cause troubles when the next goal is somewhere behind the vehicle. Convert the goal coordinates to local frame before calculating the curvature of the arc. As the curvature calculation assumes that our vehicle is always facing in the positive $x$ direction, we must convert the goal waypoint into local frame before calculations. Use signed curvature for the steering angle. In real practice the next waypoint could be either in the front-left or front-right of the vehicle, resulting in the fact that the curvature could either be concave or convex. Hence it is important for us to consider the sign of the curvature so that we can steer our car in the correct direction. ","date":"2021-01-29","objectID":"/f1tenth-lab6/:3:0","tags":["F1Tenth","F1Tenth Racing","Multi-lingual"],"title":"My F1TENTH Journey ‚Äî Lab 6, Pure Pursuit (Waypoint Tracker)","uri":"/f1tenth-lab6/"},{"categories":["Projects"],"content":"Demo Videos The Pure Pursuit algorithm with the vehicle running in 5m/s. Green trajectory is the pre-recorded waypoints; red dot is the goal waypoint for the vehicle; blue dot is the current position of the vehicle (using ground truth position in simulator). The Pure Pursuit algorithm was also successfully run in the real world. The following experiment was conducted in the 2nd floor of Levine Hall at the University of Pennsylvania. ","date":"2021-01-29","objectID":"/f1tenth-lab6/:4:0","tags":["F1Tenth","F1Tenth Racing","Multi-lingual"],"title":"My F1TENTH Journey ‚Äî Lab 6, Pure Pursuit (Waypoint Tracker)","uri":"/f1tenth-lab6/"},{"categories":["Projects"],"content":"References F1/10 Autonomous Racing Lecture: Pure Pursuit ","date":"2021-01-29","objectID":"/f1tenth-lab6/:5:0","tags":["F1Tenth","F1Tenth Racing","Multi-lingual"],"title":"My F1TENTH Journey ‚Äî Lab 6, Pure Pursuit (Waypoint Tracker)","uri":"/f1tenth-lab6/"},{"categories":["Projects"],"content":"F1Tenth Course Lab 4","date":"2021-01-27","objectID":"/f1tenth-lab4/","tags":["F1Tenth","F1Tenth Racing","Multi-lingual"],"title":"My F1TENTH Journey ‚Äî Lab 4, Reactive Planning Methods for Obstacle Avoidance","uri":"/f1tenth-lab4/"},{"categories":["Projects"],"content":"This series of blogs marks the journey of my F1/10 Autonomous Racing Cars. All my source codes can be accessed here. Previous post: My F1TENTH Journey ‚Äî Lab 1, Introduction to ROS My F1TENTH Journey ‚Äî Lab 2, Automatic Emergency Braking My F1TENTH Journey ‚Äî Lab 3, PID-Controlled Wall Follower ","date":"2021-01-27","objectID":"/f1tenth-lab4/:0:0","tags":["F1Tenth","F1Tenth Racing","Multi-lingual"],"title":"My F1TENTH Journey ‚Äî Lab 4, Reactive Planning Methods for Obstacle Avoidance","uri":"/f1tenth-lab4/"},{"categories":["Projects"],"content":"Overview Obstacle avoidance is one of the critical parts of autonomous driving. Crashing into obstacles could possibly bring serious damages to both the car and the object that it runs into. Overtime, researchers and developers have come up with multiple ways to avoid obstacles. This lab focuses on implementing a reactive way (also passive way) of avoiding obstacles to drive the car around in free space, provided with only real-time 2D LiDAR sensor data. ","date":"2021-01-27","objectID":"/f1tenth-lab4/:1:0","tags":["F1Tenth","F1Tenth Racing","Multi-lingual"],"title":"My F1TENTH Journey ‚Äî Lab 4, Reactive Planning Methods for Obstacle Avoidance","uri":"/f1tenth-lab4/"},{"categories":["Projects"],"content":"‚ÄúFollow the Gap‚Äù Algorithm With 2D laser scan data, we can easily tell whether there is an obstacle in front of us by looking into the range (length) of each beam. If the range of a certain beam is small, we know that the laser travelling in this direction hits an obstacle in close range; conversely, if the range of a beam is large, we know that the laser beam is able to travel very far before hitting on an obstacle, and thus we can consider this direction as ‚Äúfree space‚Äù. ‚ÄúFollow the Gap‚Äù algorithm states that, for each incoming LaserScan LiDAR data, we first figure out the direction of the closest obstacle, and then try to maneuver the vehicle in the direction of ‚Äúfree space‚Äù to avoid this obstacle. In theory, ‚ÄúFollow the Gap‚Äù planner can be done in the several steps: Find the nearest LiDAR endpoint and put a ‚Äúsafety bubble‚Äù of radius $r$ around it. Set all endpoints inside the bubble to distance $0$. All non-zero endpoints are considered ‚Äúfree space‚Äù. Find maximum length sequence of consecutive non-zero endpoints and identify the ‚Äúwidest‚Äù free space. (The max-gap.) Choose the furthest endpoint in the max-gap, and steer the vehicle in that direction with some certain speed. ‚ÄòFollow the Gap‚Äô Algorithms in diagrams. ","date":"2021-01-27","objectID":"/f1tenth-lab4/:1:1","tags":["F1Tenth","F1Tenth Racing","Multi-lingual"],"title":"My F1TENTH Journey ‚Äî Lab 4, Reactive Planning Methods for Obstacle Avoidance","uri":"/f1tenth-lab4/"},{"categories":["Projects"],"content":"Notes in Real Practice Laser scan data must be smoothed and de-noised. In actual implementation, we can never obtain a clean laser scan data from LiDAR sensor. Due to hardware issues the raw data would contain lots of noises and glitches. To smooth the data, I applied a sliding window of size 5 to average out each of the endpoints. I also clip the data within [range_min, range_max) according to the parameters in the incoming LaserScan ROS message. Calculate the distance between two laser scan endpoints with trigonometric formulas. In the algorithm we wish to draw a ‚Äúsafety bubble‚Äù around the nearest endpoint by traversing all laser beams and setting all endpoints within the radius of that bubble to $0$. But with laser scan data, how can we compute the distance between two endpoints? Trigonometry provides us with a power tool called the Law of Cosines. With laser beams of range $r_1,r_2$ and their angle $\\theta$, we can draw a triangle among the vehicle, endpoint 1, endpoint 2, and apply: $$d=\\sqrt{r_1^2+r_2^2-2r_1r_2\\cos\\theta},$$ which gives us the distance between the two endpoints. Impose a ‚Äúsafety angle‚Äù around the safety bubbles. One of the corner case in testing the algorithm is the corner of the wall. As the figure indicates below, the car detects the corner of the wall and has marked it as an obstacle, setting all points within the safety bubble as 0. However, the laser beam direction that detects the farthest endpoint in the max-gap happens to be right next to the obstacle, causing the car to make a left turn rather than right turn, and directly hits the wall. Demonstration of a corner case without safety angle. To address this issue, I added a ‚Äúsafety angle‚Äù around the bubble, which basically sets the laser beams too close to the direction of obstacles to distance 0. For instance, a safety angle of 20 degrees would set additional laser beams in the direction leftwards of the bubble by 20 degrees and rightwards of that bubble by 20 degrees also to 0. This simple method would prevent the vehicle to get too close to the obstacle, even if the best direction of free space happens to be next to the obstacle. ","date":"2021-01-27","objectID":"/f1tenth-lab4/:1:2","tags":["F1Tenth","F1Tenth Racing","Multi-lingual"],"title":"My F1TENTH Journey ‚Äî Lab 4, Reactive Planning Methods for Obstacle Avoidance","uri":"/f1tenth-lab4/"},{"categories":["Projects"],"content":"Demo Videos ","date":"2021-01-27","objectID":"/f1tenth-lab4/:2:0","tags":["F1Tenth","F1Tenth Racing","Multi-lingual"],"title":"My F1TENTH Journey ‚Äî Lab 4, Reactive Planning Methods for Obstacle Avoidance","uri":"/f1tenth-lab4/"},{"categories":["Projects"],"content":"References F1/10 Autonomous Racing Lecture: Reactive Methods for Planning ","date":"2021-01-27","objectID":"/f1tenth-lab4/:3:0","tags":["F1Tenth","F1Tenth Racing","Multi-lingual"],"title":"My F1TENTH Journey ‚Äî Lab 4, Reactive Planning Methods for Obstacle Avoidance","uri":"/f1tenth-lab4/"},{"categories":["Projects"],"content":"F1Tenth Course Lab 3","date":"2021-01-27","objectID":"/f1tenth-lab3/","tags":["F1Tenth","F1Tenth Racing","Multi-lingual"],"title":"My F1TENTH Journey ‚Äî Lab 3, PID-Controlled Wall Follower","uri":"/f1tenth-lab3/"},{"categories":["Projects"],"content":"This series of blogs marks the journey of my F1/10 Autonomous Racing Cars. All my source codes can be accessed here. Previous post: My F1TENTH Journey ‚Äî Lab 1, Introduction to ROS My F1TENTH Journey ‚Äî Lab 2, Automatic Emergency Braking ","date":"2021-01-27","objectID":"/f1tenth-lab3/:0:0","tags":["F1Tenth","F1Tenth Racing","Multi-lingual"],"title":"My F1TENTH Journey ‚Äî Lab 3, PID-Controlled Wall Follower","uri":"/f1tenth-lab3/"},{"categories":["Projects"],"content":"Lab Materials The lab materials can be accessed here. A PDF version is also attached here. The lab was built on the F1Tenth Simulator, which can be accessed here. ","date":"2021-01-27","objectID":"/f1tenth-lab3/:1:0","tags":["F1Tenth","F1Tenth Racing","Multi-lingual"],"title":"My F1TENTH Journey ‚Äî Lab 3, PID-Controlled Wall Follower","uri":"/f1tenth-lab3/"},{"categories":["Projects"],"content":"PID Control Overview PID (Proportional-Integral-Derivative) control is a way to maintain certain parameters of a system around a specific set point. It is probably one of the most widely-used yet simple method to maintain a stable system. This lab focuses on implementing a wall-following controller of the simulated F1/10 car using PID control algorithms. The general PID controller in the time domain can be written in the following way: $$u(t)=K_p e(t) + K_i \\int_0^t e(\\tau) d\\tau + K_d \\frac{d}{dt}e(t).$$ where $K_p$, $K_i$ and $K_d$ are tunable weight parameters of the controller, $e(t)$ is the error of current state to the desired state, and $u(t)$ is the control output. ","date":"2021-01-27","objectID":"/f1tenth-lab3/:2:0","tags":["F1Tenth","F1Tenth Racing","Multi-lingual"],"title":"My F1TENTH Journey ‚Äî Lab 3, PID-Controlled Wall Follower","uri":"/f1tenth-lab3/"},{"categories":["Projects"],"content":"Wall Following Distance and orientation of the car relative to the wall. In the context of our car, we would like to maintain the distance between the wall and our car at a certain desired value. In the image above, we can see that the parameter we would like to control is $D_t$, while we only have LiDAR laser scans in the local car coordinate frame (positive $x$ as the forward direction of the car). In order to calculate the distance from laser scans, we choose a laser ray $a$ and a laser ray $b$ such that $b$ is 90 degrees from the orientation of our car, and $a$ is of some other angle between 0 degree and 70 degrees. Then, from basic trigonometry, we have $$\\alpha=\\tan^{-1}\\left(\\frac{a\\cos(\\theta)-b}{a\\sin(\\theta)}\\right),$$ and then we can express $D_t$ as $$D_t=b\\cos\\alpha.$$ As we wish to maintain $D_t$ around a certain set point, we define the error term of our PID controller as $e(t)=D_0-D_t$. What‚Äôs more, we would also like to define a look-ahead distance as in real practice our car will be running in high speed and there might be some delays for the car to maneuver as our control command is sent. Finding the look-ahead distance from the car to the wall. The look-ahead distance $D_{t+1}$ can be defined as $D_{t+1}=D_t+L\\sin\\alpha$, where $L$ is some tunable parameters. ","date":"2021-01-27","objectID":"/f1tenth-lab3/:3:0","tags":["F1Tenth","F1Tenth Racing","Multi-lingual"],"title":"My F1TENTH Journey ‚Äî Lab 3, PID-Controlled Wall Follower","uri":"/f1tenth-lab3/"},{"categories":["Projects"],"content":"Implementing the PID Controller With the error term $e(t)$ properly defined, it is not difficult for us to implement the controller. Classical PID controllers are often implemented in a discrete, recursive way, which has the following: // double integral = 0; // defined at initialization double pid_control() { double dt = current_time - prev_time; integral += error * dt; double derivative = (error - prev_error) / dt; return K_p * error + K_i * integral + K_d * derivative; } ","date":"2021-01-27","objectID":"/f1tenth-lab3/:4:0","tags":["F1Tenth","F1Tenth Racing","Multi-lingual"],"title":"My F1TENTH Journey ‚Äî Lab 3, PID-Controlled Wall Follower","uri":"/f1tenth-lab3/"},{"categories":["Projects"],"content":"PID Tuning Tuning the PID controller is never an easy task. Sometimes the three parameters $K_p, K_i, K_d$ can be very problematic to tune. From my experience I would try in the following way: First, adjust $K_p$ to the maximum gain value (value with max performance), keeping $K_i=0, K_d=0$. Second, tune $K_i$ to the maximum gain value. Last, tune $K_d$ to eliminate the system oscillations. We can also use the Ziegler‚ÄìNichols method to tune PID controller step by step. ","date":"2021-01-27","objectID":"/f1tenth-lab3/:5:0","tags":["F1Tenth","F1Tenth Racing","Multi-lingual"],"title":"My F1TENTH Journey ‚Äî Lab 3, PID-Controlled Wall Follower","uri":"/f1tenth-lab3/"},{"categories":["Projects"],"content":"Demo video This video demos the wall follower without any look-ahead distance. The car would run into the wall at the bottom side of the corridor. This video demonstrates the wall follower with proper look-ahead distance adjustment, and the car is able to finish an entire loop with no collisions. ","date":"2021-01-27","objectID":"/f1tenth-lab3/:6:0","tags":["F1Tenth","F1Tenth Racing","Multi-lingual"],"title":"My F1TENTH Journey ‚Äî Lab 3, PID-Controlled Wall Follower","uri":"/f1tenth-lab3/"},{"categories":["Projects"],"content":"References F1/10 Autonomous Racing Lecture: PID Controller \u0026 Laplacian Domain ","date":"2021-01-27","objectID":"/f1tenth-lab3/:7:0","tags":["F1Tenth","F1Tenth Racing","Multi-lingual"],"title":"My F1TENTH Journey ‚Äî Lab 3, PID-Controlled Wall Follower","uri":"/f1tenth-lab3/"},{"categories":["Projects"],"content":"F1Tenth Course Lab 2","date":"2021-01-25","objectID":"/f1tenth-lab2/","tags":["F1Tenth","F1Tenth Racing","Multi-lingual"],"title":"My F1TENTH Journey ‚Äî Lab 2, Automatic Emergency Braking","uri":"/f1tenth-lab2/"},{"categories":["Projects"],"content":"This series of blogs marks the journey of my F1/10 Autonomous Racing Cars. All my source codes can be accessed here. Previous post: My F1TENTH Journey ‚Äî Lab 1, Introduction to ROS ","date":"2021-01-25","objectID":"/f1tenth-lab2/:0:0","tags":["F1Tenth","F1Tenth Racing","Multi-lingual"],"title":"My F1TENTH Journey ‚Äî Lab 2, Automatic Emergency Braking","uri":"/f1tenth-lab2/"},{"categories":["Projects"],"content":"Overview This lab focuses on implementing an AEB (Automatic Emergency Braking) in ROS node for F1/10 racing cars. AEB is widely used in autonomous vehicles as a basic safety guarantee to avoid collisions with objects. The lab materials can be accessed here. A PDF version is also attached here. The lab was built on the F1Tenth Simulator, which can be accessed here. ","date":"2021-01-25","objectID":"/f1tenth-lab2/:1:0","tags":["F1Tenth","F1Tenth Racing","Multi-lingual"],"title":"My F1TENTH Journey ‚Äî Lab 2, Automatic Emergency Braking","uri":"/f1tenth-lab2/"},{"categories":["Projects"],"content":"Demo ","date":"2021-01-25","objectID":"/f1tenth-lab2/:2:0","tags":["F1Tenth","F1Tenth Racing","Multi-lingual"],"title":"My F1TENTH Journey ‚Äî Lab 2, Automatic Emergency Braking","uri":"/f1tenth-lab2/"},{"categories":["Projects"],"content":"Time-To-Collision TTC (Time-To-Collision) is the time it would take for the vehicle to collide with an obstacle given its current heading and velocity. TTC can be calculated with the following format: $$ TTC = \\frac{r}{[-\\dot{r}]+} $$ where $r$ is the distance between vehicle and obstacle, $\\dot{r}$ is its 1st derivative with respect to time, and the symbol $[x]+$ denotes $\\max(0,x)$. In practice, we use LiDAR results to calculate TTC for each beam. Specifically, we project the current vehicle velocity onto the direction of each beam as $\\dot{r}$, namely $\\dot{r}=v\\cos(\\theta)$. sensor_msgs/LaserScan provides us with LiDAR beams in all directions. In each LaserScan message, we have angle_min, angle_max and angle_increment, which corresponds to the starting angle, the ending angle, and the step angle between two adjacent laser beams. All the angles are given in the local coordinate frame (positive $x$ direction is the vehicle‚Äôs heading) and positive $x$ direction is marked as angle value $0$. We also have float32[] ranges in each LaserScan message, which gives us the distance to obstacle in the corresponding direction angle. Namely, ranges[i] \u003c--\u003e angle_min + i * angle_increment. Given the above information, it is not difficult for us to write our AEB ROS node. ","date":"2021-01-25","objectID":"/f1tenth-lab2/:3:0","tags":["F1Tenth","F1Tenth Racing","Multi-lingual"],"title":"My F1TENTH Journey ‚Äî Lab 2, Automatic Emergency Braking","uri":"/f1tenth-lab2/"},{"categories":["Projects"],"content":"References F1/10 Autonomous Racing Lecture recordings: ","date":"2021-01-25","objectID":"/f1tenth-lab2/:4:0","tags":["F1Tenth","F1Tenth Racing","Multi-lingual"],"title":"My F1TENTH Journey ‚Äî Lab 2, Automatic Emergency Braking","uri":"/f1tenth-lab2/"},{"categories":["Projects"],"content":"F1Tenth Course Lab 1","date":"2021-01-24","objectID":"/f1tenth-lab1/","tags":["F1Tenth","F1Tenth Racing","Multi-lingual"],"title":"My F1TENTH Journey ‚Äî Lab 1, Introduction to ROS","uri":"/f1tenth-lab1/"},{"categories":["Projects"],"content":"This series of blogs marks the journey of my F1/10 Autonomous Racing Cars. All my source codes can be accessed here. ","date":"2021-01-24","objectID":"/f1tenth-lab1/:0:0","tags":["F1Tenth","F1Tenth Racing","Multi-lingual"],"title":"My F1TENTH Journey ‚Äî Lab 1, Introduction to ROS","uri":"/f1tenth-lab1/"},{"categories":["Projects"],"content":"Overview In Spring 2021 I finally got a chance to join the F1/10 Autonomous Racing Car Lab at Penn, supervised by Prof. Rahul Mangharam. To start my autonomous racing journey, I followed the official tutorials and studied the labs \u0026 projects there. ROS (Robotics Operating System), is a robotics project development platform widely-used all across the world. Although it is called an ‚Äúoperating system‚Äù, it is actually a chain of build tools that can be used to build mixed C++/Python projects as well as provide nice \u0026 convenient communication functionalities between processes. F1/10 Autonomous Racing Car dev stack is built upon ROS and C++. Lab 1 also mainly focuses on introducing ROS as well as providing some small hands-on exercises for ROS coding. ","date":"2021-01-24","objectID":"/f1tenth-lab1/:1:0","tags":["F1Tenth","F1Tenth Racing","Multi-lingual"],"title":"My F1TENTH Journey ‚Äî Lab 1, Introduction to ROS","uri":"/f1tenth-lab1/"},{"categories":["Projects"],"content":"Lab Materials The lab specs can be access here. A copy of PDF is also shown here. The lab was built on the F1Tenth Simulator, which can be accessed here. ","date":"2021-01-24","objectID":"/f1tenth-lab1/:2:0","tags":["F1Tenth","F1Tenth Racing","Multi-lingual"],"title":"My F1TENTH Journey ‚Äî Lab 1, Introduction to ROS","uri":"/f1tenth-lab1/"},{"categories":["Projects"],"content":"Workspaces and Packages This section mainly uses the official ROS tutorials to create a Catkin workspace and a ROS package. I followed the tutorials in ROS C++, set up my workspace, and created a package. ","date":"2021-01-24","objectID":"/f1tenth-lab1/:3:0","tags":["F1Tenth","F1Tenth Racing","Multi-lingual"],"title":"My F1TENTH Journey ‚Äî Lab 1, Introduction to ROS","uri":"/f1tenth-lab1/"},{"categories":["Projects"],"content":"Answers to Section 3 Written Questions What is a CMakeList? Is it related to a make file used for compiling C++ objects? If yes then what is the difference between the two? A CMakeList is a script offering instructions on how to build a CMake workspace. With a proper CMakeList, we can use CMake to build the workspace by cmake .. make \u0026\u0026 make install As for its relationship to a Makefile, CMakeLists can be viewed as a high-level wrapper of Makefile. In Unix systems, a CMakeList will produce a Makefile. Are you using CMakeList.txt for Python in ROS? Is there a executable object being created for Python? CMakeList for Python in ROS does not compile anything. It just imports Catkin macros and copies the corresponding Python scripts to the bin folder, if necessary. In which directory would you run catkin_make? catkin_make should be run in Catkin workspaces, aka. catkin_ws. The following commands were used in the tutorial source /opt/ros/kinetic(melodic)/setup.bash source devel/setup.bash Why do we need to source setup.bash? What does it do? Why do we have to different setup.bash files here and what is the difference? /opt/ros/kinetic(melodic)/setup.bash provides necessary Catkin commands \u0026 env variables across workspaces, such as catkin_make, catkin_init_workspace, etc. devel/setup.bash provides workspace-specific environment variables, such as the Python libraries it builds, the ROS packages it creates, etc. ","date":"2021-01-24","objectID":"/f1tenth-lab1/:3:1","tags":["F1Tenth","F1Tenth Racing","Multi-lingual"],"title":"My F1TENTH Journey ‚Äî Lab 1, Introduction to ROS","uri":"/f1tenth-lab1/"},{"categories":["Projects"],"content":"Publishers and Subscribers In this section I wrote a simple subscriber to subscribe the LiDAR message broadcasted from F1Tenth simulator. The ROS message was a sensor_msgs/LaserScan it was published on channel /scan. What‚Äôs more, I also processed the LiDAR data and found out its maximum/minimum range through a simple linear search, and published the messages on /closest_point and /farthest_point with data type std_msgs::Float64. I also tried to wrap up the processor in a C++ class. The entire code is shown below. #include \u003cros/ros.h\u003e #include \u003csensor_msgs/LaserScan.h\u003e #include \u003cstd_msgs/Float64.h\u003e class LidarProcessor { public: // Read-only public member variable of closest/farthest range // within a LaserScan const std_msgs::Float64\u0026 range_closest; const std_msgs::Float64\u0026 range_farthest; // constructor LidarProcessor() : range_closest(_closest), range_farthest(_farthest) {} // callback function for LaserScan subscriber void handleLaserScan(const sensor_msgs::LaserScanConstPtr\u0026 new_scan) { std::vector\u003cfloat\u003e ranges = new_scan-\u003eranges; // find the min/max range from linear search float range_closest = std::numeric_limits\u003cfloat\u003e::infinity(); float range_farthest = 0.f; for (const float\u0026 r : ranges) { if (std::isnan(r) || std::isinf(r)) continue; if (r \u003c range_closest) range_closest = r; if (r \u003e range_farthest) range_farthest = r; } // clip the data if it exceeds the required min/max range _closest.data = (range_closest \u003c new_scan-\u003erange_min) ? new_scan-\u003erange_min : range_closest; _farthest.data = (range_farthest \u003e new_scan-\u003erange_max) ? new_scan-\u003erange_max : range_farthest; ROS_INFO(\"LaserScan message received: closest %f, farthest %f\", _closest.data, _farthest.data); } private: std_msgs::Float64 _closest; std_msgs::Float64 _farthest; }; int main(int argc, char** argv) { ros::init(argc, argv, \"laser_scan_subscriber\", ros::init_options::AnonymousName); ros::NodeHandle nh; ros::Rate loop_rate(10); LidarProcessor processor; ros::Subscriber laserScan_subscriber = nh.subscribe(\"/scan\", 1, \u0026LidarProcessor::handleLaserScan, \u0026processor); ros::Publisher closestPoint_publisher = nh.advertise\u003cstd_msgs::Float64\u003e(\"/closest_point\", 1); ros::Publisher farthestPoint_publisher = nh.advertise\u003cstd_msgs::Float64\u003e(\"/farthest_point\", 1); while (ros::ok()) { closestPoint_publisher.publish(processor.range_closest); farthestPoint_publisher.publish(processor.range_farthest); loop_rate.sleep(); ros::spinOnce(); } return 0; } ","date":"2021-01-24","objectID":"/f1tenth-lab1/:4:0","tags":["F1Tenth","F1Tenth Racing","Multi-lingual"],"title":"My F1TENTH Journey ‚Äî Lab 1, Introduction to ROS","uri":"/f1tenth-lab1/"},{"categories":["Projects"],"content":"Answers to Section 4 Written Questions What is a nodehandle object? Can we have more than one nodehandle objects in a single node? A ros::NodeHandle is an extra layer in C++ that provides RAII-style startup \u0026 shutdown of the ROS node. It also offers extra functionalities for the node, such as sending/receiving ROS messages, waiting on a specific ROS message, etc. We can indeed have multiple NodeHandles in a single node for creating different namespaces, but they all refer to the same ROS node. What is ros::spinOnce()? How is it different from ros::spin()? ros::spinOnce() iterates the loop once more while keeping the node active, while ros::spin() puts the entire node into sleep and only handles data from callback functions. What is ros::rate()? ros::Rate specifies the rate of spinning loop of this ROS node. ","date":"2021-01-24","objectID":"/f1tenth-lab1/:4:1","tags":["F1Tenth","F1Tenth Racing","Multi-lingual"],"title":"My F1TENTH Journey ‚Äî Lab 1, Introduction to ROS","uri":"/f1tenth-lab1/"},{"categories":["Projects"],"content":"Implementing Custom Messages In this section I focused on implementing a custom ROS message type scan_range, which includes the min/max range in the laser scan. According to the tutorials of creating custom ROS messages, I created the following ROS message scan_range.msg: std_msgs/Header header std_msgs/Float64 range_min std_msgs/Float64 range_max After resolving all Catkin package dependencies in CMakeList \u0026 ROS package manifest file (package.xml), I finally reached the following output. \u003e rosmsg show lab1/scan_range std_msgs/Header header uint32 seq time stamp string frame_id std_msgs/Float64 range_min float64 data std_msgs/Float64 range_max float64 data ","date":"2021-01-24","objectID":"/f1tenth-lab1/:5:0","tags":["F1Tenth","F1Tenth Racing","Multi-lingual"],"title":"My F1TENTH Journey ‚Äî Lab 1, Introduction to ROS","uri":"/f1tenth-lab1/"},{"categories":["Projects"],"content":"Answers to Section 5 Written Questions Why did you include the header file of the message file instead of the message file itself? According to ROS mechanisms, any customized ROS messages will be generated into a templated message header file to be included in the C++ code. The message file itself is not part of standard C++ and it cannot be parsed by C++ compilers. In the documentation of the LaserScan message there was also a data type called Header. What is that? Can you also include it in your message file? What information does it provide? Include Header in your message file too. The Header provides information on the sequential order, the timestamp, and the frame ID of the instant when the message is published. It is very useful when one would like to make use of time information of the messages received. The header file has been included in my custom ROS message. ","date":"2021-01-24","objectID":"/f1tenth-lab1/:5:1","tags":["F1Tenth","F1Tenth Racing","Multi-lingual"],"title":"My F1TENTH Journey ‚Äî Lab 1, Introduction to ROS","uri":"/f1tenth-lab1/"},{"categories":["Projects"],"content":"Recording and Publishing Bag Files This section aims at playing with rosbag and bag files in ROS. ROS bag files are a certain kind of files that targets at recording ROS messages, so that it can be replayed elsewhere afterwards. ROS Bag is extremely helpful when one would like to collect data in real-world experiments. ","date":"2021-01-24","objectID":"/f1tenth-lab1/:6:0","tags":["F1Tenth","F1Tenth Racing","Multi-lingual"],"title":"My F1TENTH Journey ‚Äî Lab 1, Introduction to ROS","uri":"/f1tenth-lab1/"},{"categories":["Projects"],"content":"Answers to Section 6 Written Questions Where does the bag file get saved? How can you change where it is saved? By default the bag file is saved at the current directory. We can also change the directory by adding -o such as: rosbag record -o ../bagfiles/my_rosbag_recordings.bag Where will the bag file be saved if you were launching the recording of bagfile record through a launch file. How can you change where it is saved? By default the bag file is saved at the current directory. We can also change the directory by appending args=../bagfiles/my_rosbag_recordings.bag to the corresponding node in the launch file. ","date":"2021-01-24","objectID":"/f1tenth-lab1/:6:1","tags":["F1Tenth","F1Tenth Racing","Multi-lingual"],"title":"My F1TENTH Journey ‚Äî Lab 1, Introduction to ROS","uri":"/f1tenth-lab1/"},{"categories":["Projects"],"content":"References F1/10 Autonomous Racing Lecture: Course Introduction F1/10 Autonomous Racing Lecture: Using the Simulator ","date":"2021-01-24","objectID":"/f1tenth-lab1/:7:0","tags":["F1Tenth","F1Tenth Racing","Multi-lingual"],"title":"My F1TENTH Journey ‚Äî Lab 1, Introduction to ROS","uri":"/f1tenth-lab1/"},{"categories":["Projects"],"content":"In the fall of 2020, I finally had a chance to explore the field of computer graphics at University of Pennsylvania, and did a very interesting course project which builds a simplified Minecraft from scratch. This game is built on OpenGL 3.2 with Qt 5.15.0. Through building this game, I learned a lot about OpenGL rendering pipeline, game engine \u0026 texture mapping. ","date":"2020-12-09","objectID":"/560proj/:0:0","tags":["computer graphics","Multi-lingual"],"title":"Minecraft Game Programming","uri":"/560proj/"},{"categories":["Projects"],"content":"Demo Video ","date":"2020-12-09","objectID":"/560proj/:1:0","tags":["computer graphics","Multi-lingual"],"title":"Minecraft Game Programming","uri":"/560proj/"},{"categories":["Projects"],"content":"Overview Minecraft is originally well-known for its block-like structure. Everything is made of blocks, including mountains, trees, grasses, people, cows, zombies, etc. Although it might seem a little bit weird, this actually creates a really amazing, retro vibe as if you were playing those good old, low-resolution video games in the 70s and 80s. What‚Äôs more, I personally think that using different types of blocks to create a huge variety of objects from scratch is one of the essence of Minecraft. It is exactly because of the set-up of blocks that players can easily make things on their own. Minecraft (Java edition) main menu. ","date":"2020-12-09","objectID":"/560proj/:2:0","tags":["computer graphics","Multi-lingual"],"title":"Minecraft Game Programming","uri":"/560proj/"},{"categories":["Projects"],"content":"Blocks Minecraft is a game only built upon Blocks. Each block is defined to be a 1x1 cube in 3D space. All Blocks can only be arranged adjacent to each other without any translation or rotation. Now you may wonder: aren‚Äôt there items such as leaves, torches, ladders, sticks in Minecraft that don‚Äôt look like a cube? Well, yes, but these are considered to be specially-shaped blocks. There are still many regular-shaped blocks such as stones, dirt, ice that look exactly like a 3D cube. Those specially-shaped blocks are only treated in their special shapes when rendering game graphics, but when it comes to the deep low-level gaming logic, such as finding the state of the block (occupied/unoccupied) or the center position of the block, etc., these specially-shaped blocks are treated just like the regular ones! Minecraft has lots of different types of blocks. Passionate Minecraft gamers also keeps adding new types of blocks to the original game to make it more playable. For my own Minecraft game, I have currently only created the following types of blocks: Regular blocks (3D cubic shape): CLOUD GRASS DIRT STONE SNOW (actually corresponds to snow blocks in original Minecraft) ICE REDSTONE_LAMP ‚Ä¶ Specially-shaped blocks: REDSTONE_TORCH LEVER REDSTONE_WIRE ‚Ä¶ In additional to the regular biomes such as grasslands, mountains, forests in original Minecraft, we also created a new biome called Heaven. Heaven consists of CLOUD as the ground, and we used QUARTZ, GOLD and QUARTZ_BRICK to build a very large temple for God in Heaven. For details, please see our demo video! ","date":"2020-12-09","objectID":"/560proj/:2:1","tags":["computer graphics","Multi-lingual"],"title":"Minecraft Game Programming","uri":"/560proj/"},{"categories":["Projects"],"content":"Chunks Minecraft has a very huge map. It is impossible for the game to load all the maps into PC memory every time on start-up. Hence, it is critical for Minecraft to come up with a plan to only load the surrounding map at the player‚Äôs location. In fact, Minecraft actually divides the entire map into Chunks, where each Chunk consists of 16x256x16 blocks (x, y, z direction, respectively; y is the direction pointing ‚Äúupwards‚Äù i.e., to the sky). Upon rendering, each Chunk would be the smallest unit for rendering. This mean that Minecraft doesn‚Äôt render the surface of all blocks one by one. Instead, only rendering the surface (the boundary between an opaque block (STONE, DIRT, SNOW, etc.) and a transparent block (EMPTY, WATER, ICE, etc.)) of an entire Chunk would be much more efficient and make the game more responsive. ","date":"2020-12-09","objectID":"/560proj/:2:2","tags":["computer graphics","Multi-lingual"],"title":"Minecraft Game Programming","uri":"/560proj/"},{"categories":["Projects"],"content":"Terrains, Terrain Generation Zones When it comes to map loading, it is easy to discover that only loading one chunk (16x16 blocks in x, z direction) at a time reveals way too little information of the player‚Äôs surroundings. A simple solution is that we can group Chunks together and form Terrains. As a result, each Terrain is defined to have 4x4 Chunks in the x, z direction (64x64 blocks; 64x256x64 blocks for x, y, z). Furthermore, 5x5 Terrains form a terrain generation zone. In this way, we can now always keep loading the terrain generation zone centered at the chunk where player is standing, which means we load 20x20 Chunks surrounding the player. Once the player moves off a Chunk, we keep loading new neighboring Chunks to the PC memory and remove those chunks that are no longer considered to be in the new terrain generation zone from the PC memory. By such means we maintain a constant amount of memory usage throughout the game while keeping the player well informed of the surrounding terrain. To be continued‚Ä¶ ","date":"2020-12-09","objectID":"/560proj/:2:3","tags":["computer graphics","Multi-lingual"],"title":"Minecraft Game Programming","uri":"/560proj/"},{"categories":["Projects"],"content":"This project was served as the Undergraduate Major Design Experience at UM-SJTU Joint Institute, Shanghai Jiao Tong University. ","date":"2020-08-29","objectID":"/450proj/:0:0","tags":["computer vision","raspberry pi","Multi-lingual"],"title":"Real-Time On-Device Flow Statistics Detection and Prediction","uri":"/450proj/"},{"categories":["Projects"],"content":"Background: The Impact of COVID-19 The outbreak of novel coronavirus COVID-19 in 2020 has brought huge attention to the administration of human society. ‚ÄúSocial distancing‚Äù has been advocated as a very effective method to fight the virus. In order to facilitate the implementation of social distancing, we propose an idea of building a portable device that is capable of detect and predict flow statistics in real time, in order to provide insights and suggestions for traffic management. With the development of computer vision technology, nowadays it has become much easier and cheaper to put cameras into all parts of industry to perform a variety of detection tasks. In order to make our product cost-efficient, we also decided to take the advantage of computer vision and developed a vision-based solution for real-time flow detection. To ensure that our product is portable to use, we also settled down the plan that our whole system should be running on a Raspberry Pi. ","date":"2020-08-29","objectID":"/450proj/:1:0","tags":["computer vision","raspberry pi","Multi-lingual"],"title":"Real-Time On-Device Flow Statistics Detection and Prediction","uri":"/450proj/"},{"categories":["Projects"],"content":"Concept Generation Morphological chart. What constitutes to our product? First, in order to satisfy the basic requirements, our final product must be capable of detecting human traffic flow (i.e. pedestrians) through cameras. Therefore, there must be some part transforming the live video stream into detection results. As a result, we name this part an object detector. Second, we need to make sure that our system is capable of assigning ‚Äúidentities‚Äù to objects in the camera frame so that it could detect which person leaves the frame and which person enters the frame and we call this part an object tracker. Third, we wish to store a fixed amount of history data so that we could utilize it to predict future flow statistics changes. This means that we also need a back-end server. Last but not least, we‚Äôll have to display our flow analysis results somewhere. We decided to host them on a self-designed front-end website. From the above morphological analysis, we settled down the final configuration of our product: an object detector, an object tracker, a back-end server, and a front-end website. ","date":"2020-08-29","objectID":"/450proj/:2:0","tags":["computer vision","raspberry pi","Multi-lingual"],"title":"Real-Time On-Device Flow Statistics Detection and Prediction","uri":"/450proj/"},{"categories":["Projects"],"content":"Key Technologies After we settled down the big picture of our project, we then proceeded to conduct tests and develop the final product. After several months of discussions and developments, our final product have utilized the following technologies. ","date":"2020-08-29","objectID":"/450proj/:3:0","tags":["computer vision","raspberry pi","Multi-lingual"],"title":"Real-Time On-Device Flow Statistics Detection and Prediction","uri":"/450proj/"},{"categories":["Projects"],"content":"Object Detector: MobileNet-SSD Deep Neural Networks (DNNs) are the first solution we have thought of for object detection. It has multiple advantages over other techniques such as traditional computer vision methods, including better robustness and lower implementational cost. However, not all DNNs are suitable for our project. In fact, most of the state-of-the-art DNN detectors require a lot of computing resources such as high-performance deep learning graphics cards and are generally run on big enterprise servers. Therefore, we have to select the DNN with a relatively high performance and the least required computing resources. Finally, MobileNet v2-based Single Shot MultiBox Detector (MobileNet-SSD) becomes our final solution, because it is accurate, lightweight, and was specifically designed for mobile devices. ","date":"2020-08-29","objectID":"/450proj/:3:1","tags":["computer vision","raspberry pi","Multi-lingual"],"title":"Real-Time On-Device Flow Statistics Detection and Prediction","uri":"/450proj/"},{"categories":["Projects"],"content":"Object Tracker: Kalman Filter Kalman filter is a probabilistic model that has been widely used in control theories and applications. Particularly, it has its own states that are hidden from the outer world and only certain state properties can be observed by outer worlds through sensors. The main task of the model is to use a certain amount of observations to predict the next states of the model. Hence, we can utilize this property of Kalman filter and construct one Kalman filter for each object that we‚Äôre going to track. Once the Kalman filter is constructed, we can utilize the prediction of its position in the next frame and cross-check with the detection results of the next frame to determine whether this object is ‚Äúdisappeared.‚Äù ","date":"2020-08-29","objectID":"/450proj/:3:2","tags":["computer vision","raspberry pi","Multi-lingual"],"title":"Real-Time On-Device Flow Statistics Detection and Prediction","uri":"/450proj/"},{"categories":["Projects"],"content":"Source Code Unfortunately, due to confidentiality agreement, the source code for this project is not available for now. However, users can always visit our front-end website https://www.umji-flowdetection.com/ for further reference. ","date":"2020-08-29","objectID":"/450proj/:4:0","tags":["computer vision","raspberry pi","Multi-lingual"],"title":"Real-Time On-Device Flow Statistics Detection and Prediction","uri":"/450proj/"},{"categories":["Projects"],"content":"Acknowledgements For the entire work, I would like to express special thanks to: our sponsors: Allen Zhu and Allan Zhu from UM-SJTU Joint Institute in Shanghai Jiao Tong University my teammates: Jiayu Yi, Zekun LI, Zhikai Chen and Zihao Shen my own passion! ","date":"2020-08-29","objectID":"/450proj/:5:0","tags":["computer vision","raspberry pi","Multi-lingual"],"title":"Real-Time On-Device Flow Statistics Detection and Prediction","uri":"/450proj/"},{"categories":["My Gallery"],"content":"As COVID-19 virus spread around the world causing serious pandemic and deaths, it also has seriously impacted countless people‚Äôs lives. As a resident in the United States, my everyday life has also been affected by this pandemic in multiple aspects. After making sure that social distancing can be properly kept when I was outdoors, I decided to take my camera and witness the changes of my daily life. ","date":"2020-05-25","objectID":"/covid-19/:0:0","tags":["photography","covid-19","documentary"],"title":"Witnessing the COVID-19 Pandemic","uri":"/covid-19/"},{"categories":["My Gallery"],"content":"Ann Arbor, MI ","date":"2020-05-25","objectID":"/covid-19/:1:0","tags":["photography","covid-19","documentary"],"title":"Witnessing the COVID-19 Pandemic","uri":"/covid-19/"},{"categories":["My Gallery"],"content":"‚ÄúBlue Hour,‚Äù by definition, is the time period in a day when the world is covered in blue lights from the dusk or the dawn. It is usually formed in the period of twilight in the morning or in the evening when the sun is right below the horizon (more details check Wikipedia). Blue hour is so famous due to its beauty. During this period, all the surroundings would be covered in the romantic blue, indicating the end of the day and the start of the night. Many famous movies, such as ‚ÄúLa La Land,‚Äù features the beauty of the blue hour. ","date":"2020-05-25","objectID":"/blue-hour/:0:0","tags":["photography","blue hour"],"title":"Capturing the \"Blue Hour\"","uri":"/blue-hour/"},{"categories":["My Gallery"],"content":"Blue Hour in Ann Arbor, MI ","date":"2020-05-25","objectID":"/blue-hour/:1:0","tags":["photography","blue hour"],"title":"Capturing the \"Blue Hour\"","uri":"/blue-hour/"},{"categories":["My Gallery"],"content":"Hong Kong has been one of the fastest developing metropolis in China. When I was young, my impression of Hong Kong came with the Hong Kong TV shows. Hongkongers were so good at joking and acting that their movies and TV shows have had significant influences on me, especially with the fact that I can understand Cantonese :-). Not only me but also Chinese people in the 1980s and 1990s were greatly influenced by Hong Kong cultures. Hong Kong is famous for its fast-developing metropolitan scenery, with a local nickname as ‚ÄúForest of Steel/Urban Forest‚Äù (Chinese: Èí¢ÈìÅÊ£ÆÊûó). Personally I think the nickname really describes the essence of Hong Kong ‚Äî a place full of so many skyscrapers that almost covers the sky! Hong Kong is also extremely crowded and busy. It is a tiny piece of land filled with millions of people. As a matter of fact, Hongkongers are experts in fully utilizing vertical spaces in every corner of the city. Nathan Rd (Chinese: ‰ΩêÊï¶ÈÅì) Temple Street (Chinese: Â∫ôË°ó). It is famous for its crowded stalls, selling all kinds of food, clothes and handicrafts. A Tsui Wah Restaurant near Temple Street. Tsui Wah Restaurant is a famous Hong Kong traditional restaurant with a long history. Staring from 1960s, Tsui Wah Restaurant made their way to success by making exquisite Cantonese cuisine. Iconic Hong Kong red taxi. A big Chinese character 'Eat' near Nathan Rd Every metropolis has its dark side. Behind the crowd and business, a small alley in Hong Kong was filled with all kinds of trash. Besides the dirtiness, a big banner of ‚ÄúWelcome‚Äù hanging across the alley seems to have some kind of deep meaning behind the words. ","date":"2020-05-24","objectID":"/hong-kong/:0:0","tags":["photography","city representation"],"title":"Hong Kong, China","uri":"/hong-kong/"},{"categories":["My Gallery"],"content":"As a travelling enthusiast, there is no doubt that the state where I lived must somehow appear on my wishlist. During the two years in Ann Arbor, I also spent some time to explore Michigan. What represents Michigan? It is very easy to think of the slogan of the State of Michigan ‚Äî ‚ÄúPure Michigan.‚Äù However, in what way does the State of Michigan look pure? Clearly it shouldn‚Äôt be the way in which Detroit and Lansing look. Although Michigan doesn‚Äôt seem to be among the most popular states for tourism to foreigners, there are a lot of beautiful state parks as well as national parks in Michigan that make the state ‚Äúpure.‚Äù I would like to categorize these places in two sections: the west part of Michigan (Western Michigan) and the north part of Michigan (Northern Michigan). ","date":"2020-05-17","objectID":"/michigan/:0:0","tags":["photography","city representation"],"title":"Michigan, USA","uri":"/michigan/"},{"categories":["My Gallery"],"content":"Western Michigan The views in the west part of Michigan mainly comes from Lake Michigan, which is one of the five great lakes between the US-Canada border. On the west coast of the lake lies the State of Illinois and the State of Wisconsin, whereas on the east coast lies the State of Michigan. Lake Michigan is such a great lake that a large number of beaches has formed along its coast. It also produces strong wind due the combination of its location and the climate, which is the reason why Chicago is known as the ‚ÄúWindy City.‚Äù Generally, Lake Michigan can be viewed as a sea, and spending holidays by Lake Michigan is very much similar to the feelings of enjoying a seaside life. Lake Michigan and the surrounding states During the past summer, I have visited several towns by Lake Michigan, including Muskegon, Pentwater, Manistee, Traverse City, and the famous Sleeping Bear Dunes. Sleeping Bear Dunes is a famous national lakeshore at Western Michigan. There is indeed a number of great sand dunes by the lake. Although it might seem weird to most people that sand dunes should not be formed in such a wet climate, it is actually the result of special geological movements \u0026 climates. The lake there is pure blue, in a sharp contrast with the yellowish sand dunes, overall producing a spectacular view. Traverse City is one of the few big cities in Western Michigan. The city has become a place for holidays. Countless tourists come here to enjoy a great view of the Great Lakes. I captured a beautiful sunset that day in the city. ","date":"2020-05-17","objectID":"/michigan/:1:0","tags":["photography","city representation"],"title":"Michigan, USA","uri":"/michigan/"},{"categories":["My Gallery"],"content":"Last summer, I went to Ireland for a trip. As a fan of the Irish boy band Westlife, I was really lucky to be able to get a ticket to their concert at Croke Park in Dublin. Meanwhile, I also had a chance to explore Dublin, a city with bars and Irish folk music. ","date":"2020-02-28","objectID":"/dublin/:0:0","tags":["photography","city representation"],"title":"Ireland","uri":"/dublin/"},{"categories":["My Gallery"],"content":"Street Performers in Downtown Dublin To my great surprise, street performing seems to be a very popular activity in Dublin. Walking in the streets in the city you could find a lot of street performers playing all kinds of musical instruments. Most of them are performing with great passion and happiness, as if they were just playing music on the street not for their living, but for fun. ","date":"2020-02-28","objectID":"/dublin/:1:0","tags":["photography","city representation"],"title":"Ireland","uri":"/dublin/"},{"categories":["My Gallery"],"content":"Westlife ‚ÄúThe Twenty Tour‚Äù @Croke Park, Dublin ‚ÄúThe Twenty Tour‚Äù is one of the major concert tours of Westlife after their reunion. I could hardly find a proper word to describe my excitement at the concert due to my poor vocabulary. It was really exciting, magnificent, and wonderful. ","date":"2020-02-28","objectID":"/dublin/:2:0","tags":["photography","city representation"],"title":"Ireland","uri":"/dublin/"},{"categories":["My Gallery"],"content":"Cork Along with Dublin, I also visited the second-largest city in Ireland, Cork, and the famous Cliffs of Moher, during my trip to Ireland last summer. Although Cork has been the second largest city of Ireland, it is still relatively small and looks more like a peaceful town. In Cork there is a famous English market where all kinds of Atlantic seefood, meat, fruits, and flowers are sold. The city was small but really peaceful, with a small river passing through the city center. Hello, Cork! ","date":"2020-02-28","objectID":"/dublin/:3:0","tags":["photography","city representation"],"title":"Ireland","uri":"/dublin/"},{"categories":["My Gallery"],"content":"Cliffs of Moher Cliffs of Moher faces towards the great Atlantic Ocean. It is a wonderful place with magnificent views. ","date":"2020-02-28","objectID":"/dublin/:4:0","tags":["photography","city representation"],"title":"Ireland","uri":"/dublin/"},{"categories":["Projects"],"content":"Simultaneous Localization And Mapping, also known as SLAM, is a set of very important techniques in robotics, and it‚Äôs still in active research. It aims at developing different algorithms for figuring out the position of the robot in the real world by the robot itself. ","date":"2020-01-13","objectID":"/467proj-slam/:0:0","tags":["robotics","Multi-lingual"],"title":"SLAM ‚Äî Simultaneous Localization And Mapping","uri":"/467proj-slam/"},{"categories":["Projects"],"content":"Background: What is SLAM? Think about a scenario where you were led to an unknown building, and you were standing at the beginning of a corridor. Yes, right now you were completely lost, and you wanted to figure out your position in the building. With initially zero information, you now began to look around at the surroundings. Very soon, you acquired a general sense of where you are with respect to all the objects around you. Then you realized that keeping standing at the same place won‚Äôt offer you more information, and you began to move. While you were moving, you kept checking out the new surroundings, obtained information about your new position, and updated your prior positions with the new information. Finally, you reached the end of the corridor, with a complete local map of the general shape of the corridor in your head. If you kept going, you would obtain more and more information about the new environment and also get your prior information updated. At some point you would be familiar enough with the building to figure out where you were in it. Although it might seem natural for readers as you to get familiar with an unknown environment by looking and walking around, this process is actually complicated when it comes to applying it on a robot. In the field of robotics, this process is called SLAM (Simultaneous Localization And Mapping). So far, there have been a number of different models that accomplish this task, with one of the most basic model called Markov Localization. In the fall of 2019, I took the course Autonomous Robotics (EECS 467), in which I implemented a complete SLAM algorithm with occupancy grid mapping and Markov Localization for a robot car with 3 other teammates Gregory Meyer, Nathan Brown, and Martin Deegan. ","date":"2020-01-13","objectID":"/467proj-slam/:1:0","tags":["robotics","Multi-lingual"],"title":"SLAM ‚Äî Simultaneous Localization And Mapping","uri":"/467proj-slam/"},{"categories":["Projects"],"content":"Demo Videos The following is a demo video of our algorithm visualization on a laptop. The following video is a demo of our robot car running in the unknown environment. ","date":"2020-01-13","objectID":"/467proj-slam/:2:0","tags":["robotics","Multi-lingual"],"title":"SLAM ‚Äî Simultaneous Localization And Mapping","uri":"/467proj-slam/"},{"categories":["Projects"],"content":"EECS 373 Final Project","date":"2019-05-03","objectID":"/373proj/","tags":["Embedded Systems","Multi-lingual"],"title":"An Interactive Game ‚Äî Step On the White Tiles!","uri":"/373proj/"},{"categories":["Projects"],"content":" Probably one of the best interactive game projects I have seen in the class. ‚Äî Prof. Alanson Sample and Prof. Matt Smith, University of Michigan \u003e This project is developed together with Regina (Jingliang Ren), Kun Huang and Shiyu Liu. \u003e This post is written in shared effort with Regina (Jingliang Ren). \u003e Special thanks to them for all their contributions! In the passing winter semester, I took a very interesting course: Introduction to Embedded System Design (EECS 373). I‚Äôve learned memory-mapped I/O, interrupt, timer and many other important concepts in embedded system. The target of this course is to build an embedded system using a specific kind of development board ‚Äì Actel SmartFusion¬Æ SoC FPGA. Our final project is ‚ÄúStep on the White Tiles‚Äù, which is inspired by a popular mobile game Don‚Äôt Tap the White Tiles (Chinese: Âà´Ë∏©ÁôΩÂùó). This is one of my proudest projects, so I‚Äôd like to write a blog to record this project experience and share these fun technical details with more people! ","date":"2019-05-03","objectID":"/373proj/:0:0","tags":["Embedded Systems","Multi-lingual"],"title":"An Interactive Game ‚Äî Step On the White Tiles!","uri":"/373proj/"},{"categories":["Projects"],"content":"Background: Interactive Gaming Interactive gaming system has been developing fast for the past decades. From the Wii Remote controller created by Nintendo to the PlayStation series controller invented by Sony, it seems that people has become more and more fascinated about interactive gaming. Inspired by the existing interactive gaming system solutions, I, along with Regina (Jingliang Ren), Ken (Kun Huang) and Shiyu Liu planned to develop a simple interactive gaming system based on Actel SmartFusion¬Æ SoC FPGA, which is a simple implementation of a popular mobile game Don‚Äôt Tap the White Tiles into the real world. A basic user interface for the original mobile game, Don‚Äôt Tap the White Tiles. ","date":"2019-05-03","objectID":"/373proj/:1:0","tags":["Embedded Systems","Multi-lingual"],"title":"An Interactive Game ‚Äî Step On the White Tiles!","uri":"/373proj/"},{"categories":["Projects"],"content":"Game Rules There are 3 modes for our game: slow, medium and fast. Different modes have different numbers and speeds of tiles. At the beginning of the game, player will have 5 lives (health points). üç≠ When left foot steps on a white tile, white tile becomes green and score++ üç≠ When left foot steps on a white tile, white tile becomes red and score++ üå∂ When player misses a white tile, he/she will lose one life (health point) ‚ò†Ô∏è When all lives go, game over! ","date":"2019-05-03","objectID":"/373proj/:2:0","tags":["Embedded Systems","Multi-lingual"],"title":"An Interactive Game ‚Äî Step On the White Tiles!","uri":"/373proj/"},{"categories":["Projects"],"content":"Demo Videos Here is a video captured during EECS373 Project Expo üòé! ","date":"2019-05-03","objectID":"/373proj/:3:0","tags":["Embedded Systems","Multi-lingual"],"title":"An Interactive Game ‚Äî Step On the White Tiles!","uri":"/373proj/"},{"categories":["Projects"],"content":"Technical Details In the following part, I will be introducing in detail how to build this cool interactive gaming system! As mentioned before, in this project we‚Äôre using Actel SmartFusion¬Æ SoC FPGA. ","date":"2019-05-03","objectID":"/373proj/:4:0","tags":["Embedded Systems","Multi-lingual"],"title":"An Interactive Game ‚Äî Step On the White Tiles!","uri":"/373proj/"},{"categories":["Projects"],"content":"Component Overview As indicated in the graph, we use one SmartFusion¬Æ board to control 5 main peripherals: Nintendo controller and LCD display for calibration Pixy camera, setting game mode, controlling start and pause Projector controlled by VGA signal for projecting moving tiles, score, left health point Pixy camera for detecting if player‚Äôs feet are on white tiles Sound board for playing background music and sound effect ","date":"2019-05-03","objectID":"/373proj/:4:1","tags":["Embedded Systems","Multi-lingual"],"title":"An Interactive Game ‚Äî Step On the White Tiles!","uri":"/373proj/"},{"categories":["Projects"],"content":"Code Environments As for the project developing environment, we‚Äôre using Libero SoC Design Software as well as Microsemi SoftConsole. The specific versions are: Libero SoC v11.9 SP3 Microsemi SoftConsole v3.4 Windows 10 Pro 1809 (operating system) Note that Libero SoC is neither forward nor backward compatible. A Libero SoC v11.9 project is not compatible with a Libero SoC v11.9 SP3 project. ","date":"2019-05-03","objectID":"/373proj/:4:2","tags":["Embedded Systems","Multi-lingual"],"title":"An Interactive Game ‚Äî Step On the White Tiles!","uri":"/373proj/"},{"categories":["Projects"],"content":"Memory-mapped I/O \u0026 Advanced Peripheral Bus In common sense, every memory address in a CPU corresponds to some place which can store data (aka ‚Äúreal memory‚Äù). However, in embedded systems, each input/output (I/O) device also corresponds to a specific memory address in the core. For these addresses, each of them has been mapped to an I/O device instead of ‚Äúreal memory‚Äù, such as Analog-to-Digital Converter (ADC), Digital-to-Analog Converter (DAC), General-Purpose I/O (GPIO) pins, etc. Accessing these addresses through CPU is actually accessing their corresponding I/O devices. APB (Advanced Peripheral Bus) is used as an interface to access these peripherals. A typical APB consists of the following signal (by naming convention): Signal Name Meaning PRDATA Data line where CPU reads from the peripheral. PWDATA Data line where CPU writes to the peripheral. PWRITE Indicates whether it is a read or a write transaction. PENABLE Indicates whether CPU has started the transaction. PSEL Indicates whether CPU has selected this peripheral. PADDR Indicates what address CPU has chosen to interact with. PCLK Clock signal from CPU. PREADY Controlled by peripheral; indicates whether peripheral has been prepared for the transaction. By convention, in terms of APB, the CPU is called ‚Äúmaster‚Äù while the peripherals are called ‚Äúslaves‚Äù. While signals are generally high/low voltages which is in essence a hardware issue, APB makes generating these signals with C code possible. A diagram of how we‚Äôre making use of APB in our interactive game. Implementation in Verilog While in most microcontrollers APB is not able to be modified because it is a very complicated circuit network soldered onto the PCB, with an FPGA we can take full advantage of that and customize our own APB. In Verilog, we can simply make use of an unused memory address and create the following module (APB interface): module apb_interface( input PCLK, // clock input PRESERN, // system reset // APB3 BUS INTERFACE input PSEL, // peripheral select input PENABLE, // distinguishes access phase output wire PREADY, // peripheral ready signal output wire PSLVERR, // error signal input PWRITE, // read/write control bit input [31:0] PADDR, // IO address input wire [31:0] PWDATA, // (processor) bus is writing data to // this device 32 bits output reg [31:0] PRDATA, // (processor) bus is reading data from this device /*** Your Code (inputs \u0026 outputs) ***/ ); assign PSLVERR = 0; // assumes no error generation assign PREADY = 1; // assumes zero wait wire write_enable = (PENABLE \u0026\u0026 PWRITE \u0026\u0026 PSEL); //decodes APB3 write cycle wire read_enable = (!PWRITE \u0026\u0026 PSEL); //decode APB3 read cycle // ****** Your code ****** endmodule We can see that all the I/O ports in the Verilog module has to strictly follow the APB convention mentioned above. ","date":"2019-05-03","objectID":"/373proj/:4:3","tags":["Embedded Systems","Multi-lingual"],"title":"An Interactive Game ‚Äî Step On the White Tiles!","uri":"/373proj/"},{"categories":["Projects"],"content":"VGA VGA is short for Video Graphics Array. Although it is a relatively aged display method (introduced by IBM in 1987), its frequency (60Hz per frame) is good enough for our display purpose. Basics A VGA cable has 15 pins, 6 of them are used in the project: Blue, Red, Green represent color for one pixel Horizontal sync and vertical sync are used to define the ends of each line and the whole frame GND for ground A VGA video is a stream of frames. Each frame is made up of a series of horizontal lines, and each line is made up of a series of pixels. At the beginning, there will be a scan point on top left corner of the frame. In each clock period, we should output correct RGB value for current pixel. And the scan point will go to the next pixel. When it comes to the end of a line, the Horizontal-Sync signal will go low. Then the scan point goes line by line. When it comes to the end of the frame, the Vertical-Sync will go low. Till now, we refresh the whole frame for one time. In a word, to project a desired video, we just need to output correct RGB signal for each pixel in sequence. Implementation The main logic for VGA display is implemented in C code: it determines the positions of tiles, scores and health point; then writes these information in specific APB addresses. Verilog code is only for analyzing information from these APB addresses and generating corresponding signals. A 60-Hz interrupt will be generated periodically in fabric. In main.c, __attribute__((interrupt)) void Fabric_IRQHandler(void){} is interrupt handler, which is responsible for refresh of the whole frame. Moving Tiles There are 8 tiles in maximum in one frame. To make them look as random as possible, I randomly generate 3 attributes for each tile: length the column inside which it will move delay for interval between the time when a tile goes out of the frame and when it reappears at the top of the frame. In vga.c, void random_mode(int k) {} controls the new position of kth tile based on its old position. Besides, the attribute, left_on, right_on, will control color of the tile. When it is set to true inside pixy.c, the color will turn green or red. Score \u0026 Health Point Once left_on or right_on is set to true inside pixy.c, the variable, score, will add one. If left_on and right_on are false until the tile goes out of the frame, one life will lose. The display of score üî¢ and heart ‚ù§Ô∏è are hard-coded in Verilog. Another way to show them is using Sprites + SRAM (see below). Showing More Complicated Graphs Although the display for this game is fairly simple, we tried using Sprites and SRAM, which is used for complicated graphics (but didn‚Äôt use it in final version of project). The idea behind Sprites is that generate RGB values for each pixel in a picture; and then store them in SRAM of microcontroller; read RGB value for corresponding pixel from SRAM when scan point comes to the pixel. The SRAM module inside SmartFusion¬Æ is shown above. Pull up r_en when you are reading from it, pull up w_en when you are writing to it. However, there are two annoying issues when I implemented it: Timing. When use APB and SRAM, timing of these two things need to be considered carefully. The PREADY cannot be set to high all the time since SRAM need one clock period to finish reading/writing. So PREADY should be delayed for one clock period. Memory limitation. The total size of SRAM inside SmartFusion¬Æ is only 64kB. ","date":"2019-05-03","objectID":"/373proj/:4:4","tags":["Embedded Systems","Multi-lingual"],"title":"An Interactive Game ‚Äî Step On the White Tiles!","uri":"/373proj/"},{"categories":["Projects"],"content":"Nintendo Controller For the controller for the user menu selection, we‚Äôre using a classic Nintendo¬Æ NES controller. This is a relatively aged controller, but its signal is easy to decode and it‚Äôs sufficient for our design. We can see that it consists of 8 buttons: Up, Down, Left, Right, Select, Start, A and B. For more details, a document that I have referred to when programming the NES controller says that: The Nintendo Entertainment System or NES, also known as the Nintendo Family Computer (Famicom) was released in Japan in 1983 and shortly after in the USA in 1985. The system was even more successful than the Atari 2600 and more or less put the last stake in the heart of Atari home consoles forever. The NES sold more than 60,000,000 units worldwide in its heyday and still sells to this day (I just bought 4 of them from eStarland.com)! There are over 200 clones of the system in circulation and the word ‚ÄúNintendo‚Äù is used in many cultures as a youth slang term with many ‚Äúvaried‚Äù meanings. The system itself was nothing compared to the Propeller chip‚Äôs computing power, the NES was powered by the 8-bit 6502 running at 1.79 MHz, but the NES did have quite a bit of dedicated graphics hardware making it quite a contender in the 8-bit era, it sported multiple tile maps, graphics planes, sprites, sound, and a lot more. ‚Äî ‚ÄúGame Programming for the Propeller Powered HYDRA‚Äù, pp. 95 Interfacing the Nintendo NES controller On the back of the NES controller, it is actually organized as the following figure shows. An inside view of an NES controller. We can see that there‚Äôre five wires: brown, red, orange, yellow, white. The functions of these wires are summarized below. Signal Name Meaning GND Brown clock Red latch Orange data_out Yellow 5V White The states of the 8 buttons (pressed/not pressed) will be provided at the data_out line, bit by bit. The latch and the clock line are used to control the clocking of the data_out data line. In order to get the button states from the NES controller, we need to perform the following steps. Set the clock of the MCU to be greater than or equal to $12\\mu\\rm{s}$. Set latch line to LOW. Set clock line to LOW. Now, we‚Äôre in a known state. Pulse latch line HIGH for at least 2 cycles. This will latch the data into the shift register. Read the bits in from data_out line. The first bit will be available after the latching process. Pulse clock line 7 times to read the remaining bits. Note. The bits are inverted from the data line (LOW for pressed, HIGH for not pressed). Based on the above rules, we can create a logic in Verilog that keeps reading the button states of the NES controller in the background. To keep things simple, we use APB interface to interact with the C coding parts, and we organize the 8 button states to be in the form of an 8-bit integer in a specific APB address that can be accessed with C code. This figure shows the 8-bit integer data format we‚Äôre using for NES controller. As we‚Äôre using 25MHz clock on our SmartFusion¬Æ board, we need to extend our clock signal to $12\\mu\\rm{s}$. Thus, we need a clock divider. reg [9:0] clock_divider; assign clock_divider_max = (clock_divider == 10'd150); always @(posedge PCLK) begin if (~PRESERN) begin clock_divider \u003c= 10'd0; end else if (clock_divider_max) begin clock_divider \u003c= 0; end else begin clock_divider \u003c= clock_divider + 1; end end Then, we need to create our logic to pulse the latch and clock signal, and get the bits from data_out. Whenever we read a new bit in, we need to shift all the old bits left by 1. reg [7:0] data_in, data_complete; reg [2:0] read_count; reg start, finish; always @(posedge PCLK) begin if (~PRESERN) begin latch \u003c= 0; clock \u003c= 0; start \u003c= 1; finish \u003c= 0; count \u003c= 5'd0; data_in \u003c= 8'd0; data_complete \u003c= 8'd0; read_count \u003c= 3'd0; end if (finish == 1) begin data_complete \u003c= data_in; finish \u003c= 0; start \u003c= 1; clock \u003c= 0; data_in \u003c= 8'd0; end if (clock_divider_max) begin if (start == 1) begin // latch the first data latch \u003c= 1; sta","date":"2019-05-03","objectID":"/373proj/:4:5","tags":["Embedded Systems","Multi-lingual"],"title":"An Interactive Game ‚Äî Step On the White Tiles!","uri":"/373proj/"},{"categories":["Projects"],"content":"LCD Display We use Sparkfun‚Äôs $20\\times4$ serial enabled LCD display in our project. The interface of this LCD is actually really simple ‚Äî just UART. If you send a character a to it through UART, it will display an ‚Äúa‚Äù on the screen; if you send a string Hello World to it, it will display ‚ÄúHello World‚Äù. A typical Sparkfun‚Äôs 20x4 LCD display. It is worth noticing that this LCD module displays characters based on the position of a cursor. This is very similar to writing on a text file. The character sent to the module will always be written at the position of the cursor. For instance, if the current cursor position is at the first position of the first line, the incoming character a will then be displayed at the first position in the first line. What is worth mentioning is that there are some special ASCII character pairs that have special meanings and have been hard-coded into the LCD module. For example, some of these special characters are: Character Value Meaning 0xFE, 0x01 Clear the entire display. 0xFE, 0x80 Set cursor position to be the first position in the 1st line. The 20 values starting from 0x80 correspond to the cursor positions from the beginning of the 1st line to the end of the first line. For the first position in the 2nd line, add 64 to 0x80; for the first position of the 3rd line, add 20 to 0x80; for the first position of the 4th line, add 84 to 0x80. 0x12 Reset the whole module. Note. Two bytes separated by a comma means a consecutive ASCII character pair. For instance, 0xFE, 0x80 means 0xFE should be first sent to the LCD module, followed by a 0x80. Constructing the User Menu In terms of constructing a complete user menu, we use \u003e as the prompt character for the selected option. We develop a data structure called LCD_Display to store the current contents of the 4 lines as well as the line index of the prompt character, and another Menu for multiple layers of the menu. For simplicity, we set each layer of the menu has a maximum size of 6. For more detailed information, please visit our GitHub repo and take a look at the menu.[ch] library files. ","date":"2019-05-03","objectID":"/373proj/:4:6","tags":["Embedded Systems","Multi-lingual"],"title":"An Interactive Game ‚Äî Step On the White Tiles!","uri":"/373proj/"},{"categories":["Projects"],"content":"Pixy Camera Pixy camera is a small and responsive open-source camera developed by CREATE Lab, a part of the Robotics Institute at Carnegie Mellon University. It is equipped with color recognition and it can tell the location of the object in real time within its field of view. It has a very nice interface for MCU, which we can integrate into our SmartFusion¬Æ board. Particularly, it is able to transmit its color recognition results to MCU via three interfaces: I2C, SPI, and UART. It is known that SPI has the highest data transmission rate among the three popular protocols, so we decide to utilize its SPI interface. Porting Guidelines According to the official documentation, Pixy camera has the following available ports: The available ports and its corresponding functionality on Pixy camera. with the numbering convention (looking at the back of Pixy): 1 2 3 4 5 6 7 8 9 10 The Serial Protocol Signature is an important feature of Pixy. A signature represents a particular color: say you may want to set a signature for color red, another signature for color blue, and another signature for color yellow, etc. An object is the biggest object with a corresponding signature appearing in Pixy‚Äôs field of view. That is, Pixy can hold as many objects as the signatures. The whole information containing in one single image is called a frame. A data frame can have several objects corresponding to several signatures. Pixy‚Äôs protocol enables itself to convey information for multiple objects of different colors at a time. More specifically, its serial protocol goes as follows: The information for each object is 14 bytes in little endian, with the first 2 bytes starting with 0xaa55. Two consecutive 0xaa55 starts a new frame, with the second 0xaa55 starts the new object. Bytes 16-bit word Description ---------------------------------------------------------------- 0, 1 y sync: 0xaa55=normal object, 0xaa56=color code object 2, 3 y checksum (sum of all 16-bit words 2-6, that is, bytes 4-13) 4, 5 y signature number 6, 7 y x center of object 8, 9 y y center of object 10, 11 y width of object 12, 13 y height of object Above are the information needed for us to parse Pixy‚Äôs data. But how can we transmit data to Pixy to set the signatures? Fortunately, Pixy‚Äôs group have developed a very nice software called PixyMon for us to accomplish this goal. This software sets up a UART communication between PC and Pixy camera, and it can transmit data to it as soon as we click the corresponding button in the software. By the means mentioned above, we‚Äôre able to get the center of the objects with specific color within Pixy‚Äôs field of view, which is a very nice method to locate the player‚Äôs feet in our project. Meanwhile, there are also other advanced features of Pixy that we haven‚Äôt explored. For details, please visit their official website. ","date":"2019-05-03","objectID":"/373proj/:4:7","tags":["Embedded Systems","Multi-lingual"],"title":"An Interactive Game ‚Äî Step On the White Tiles!","uri":"/373proj/"},{"categories":["Projects"],"content":"Sound Board We use Adafruit Audio FX Mini Sound Board for playing background music. By plugging in the sound board into computer using USB cable, it will show up as a new USB key. Just copy music into it and music will be written properly in its memory. To trigger audio, pull down one of trigger pins - named #0 thru #10. To stop audio, pull down Rst. The control of this board also uses the Verilog APB interface. ","date":"2019-05-03","objectID":"/373proj/:4:8","tags":["Embedded Systems","Multi-lingual"],"title":"An Interactive Game ‚Äî Step On the White Tiles!","uri":"/373proj/"},{"categories":["Projects"],"content":"Source Code All source codes with the latest updates can be accessed in my GitHub repository. A project directory structure is also provided below. Special thanks to Regina for her drawing the project directory structure! ","date":"2019-05-03","objectID":"/373proj/:5:0","tags":["Embedded Systems","Multi-lingual"],"title":"An Interactive Game ‚Äî Step On the White Tiles!","uri":"/373proj/"},{"categories":["Projects"],"content":"Acknowledgements For the entire work, I would like to express special thanks to: Prof. Sample and Prof. Smith‚Äôs support throughout the semester ‚òïÔ∏èü•É my teammates: Shiyu, Regina and Ken‚Äôs efforts on this project üç¶üçª my own passion üçóüç∫ ","date":"2019-05-03","objectID":"/373proj/:6:0","tags":["Embedded Systems","Multi-lingual"],"title":"An Interactive Game ‚Äî Step On the White Tiles!","uri":"/373proj/"},{"categories":["My Gallery"],"content":"Ann Arbor is the place for me to pursue my second bachelor‚Äôs degree in the University of Michigan. It is a small but lovely town located in the south of the State of Michigan. While I was taking classes and working on the campus, I would also like to record those tiny little pieces of happiness in daily life with my lenses. These pictures help to keep my precious memories. ","date":"2019-04-09","objectID":"/ann-arbor/:0:0","tags":["photography","city representation"],"title":"Ann Arbor, MI","uri":"/ann-arbor/"},{"categories":["My Gallery"],"content":"Story of My Daily Life The crossing of Plymouth Rd and Upland Rd on North Campus. There was a beautiful sunset one afternoon, and at this crossing there was a broad view of the scenery. Watching the busy cars running on the street gives me a sense of life. Route 23 of the Ann Arbor city bus arriving at CCTC (Centra Campus Transit Center). Every time it arrives, it basically tells me I'am finally able to go home. Sunset shining on a oversized truck passing through North Campus on Plymouth Rd. A U-M campus bus passing through Northwood on North Campus during the summer. A tiny piece of sunset shining on the stem of a big tree. A nice contrast of orange and green. ","date":"2019-04-09","objectID":"/ann-arbor/:1:0","tags":["photography","city representation"],"title":"Ann Arbor, MI","uri":"/ann-arbor/"},{"categories":["My Gallery"],"content":"Ann Arbor Summer Festival It was really wonderful for me to find that, during the summer, the town of Ann Arbor was getting more and more busy. One of the major activities in Ann Arbor in summer is the Summer Festival, in which a lot of artists and artisans would come and show off their great works. With excitement, I went to the festival, and received lots of great memories. A very interesting reflection of two visitors in a small puddle, reflecting the business of the summer festival. A lot of decorative CDs and records were on sale in the Ann Arbor Summer Festival. All of them are crafted into different shapes. A tricycle man is riding several passengers on the street with his tricycle full of ads. A dancer showing off his dancing skills. ","date":"2019-04-09","objectID":"/ann-arbor/:2:0","tags":["photography","city representation"],"title":"Ann Arbor, MI","uri":"/ann-arbor/"},{"categories":["My Gallery"],"content":"For√ßa, Bar√ßa In August, FC Barcelona came to Michigan Stadium and had a match with S.S.C. Napoli. Why having a soccer match at Michigan? I would say, it was definitely because Michigan Stadium is the largest stadium in the US, the third largest stadium across the world! Although it was not an important match for the two soccer clubs, it was still really exciting. With no surprise, FC Barcelona finally won the game. soccer players are doing their final practice before the match. FC Barcelona ‚Äî goal! ","date":"2019-04-09","objectID":"/ann-arbor/:3:0","tags":["photography","city representation"],"title":"Ann Arbor, MI","uri":"/ann-arbor/"},{"categories":["My Gallery"],"content":"‚ÄúBlue Hour‚Äù in Ann Arbor As I always finish my classes in the late afternoon, witnessing the beauty of the blue hour every day has been one of my favorite things. When the summer comes and the sky becomes clear, I finally got some chance to capture the romantic feelings in the blue hour. ","date":"2019-04-09","objectID":"/ann-arbor/:4:0","tags":["photography","city representation"],"title":"Ann Arbor, MI","uri":"/ann-arbor/"},{"categories":["My Gallery"],"content":"Of all the places I‚Äôve been to, Zhuhai, by no doubt, is the most special one to me. I have been to almost everywhere in this city. As I grew up, countless changes have taken place in this city. People went and people came, and the city developed fast. As a photographer, I have the feeling that it is my responsibility to record her changes every time I return. I really hope that my hometown can become better and better. A view of Zhuhai from where I live. ","date":"2019-03-05","objectID":"/zhuhai/:0:0","tags":["photography","city representation"],"title":"Zhuhai, China ‚Äî Home, Sweet Home","uri":"/zhuhai/"},{"categories":["My Gallery"],"content":"Zhuhai, in the City Zhuhai Railway Station (Chinese: ÂüéËΩ®Áè†Êµ∑Á´ô) Zhuhai Railway Station (Chinese: ÂüéËΩ®Áè†Êµ∑Á´ô). Zhuhai Railway Station is the biggest and busiest train station connected to the high-speed CRH train in China. Dozens or even hundreds of trains leave and enter Zhuhai every day. Zhuhai Railway Station is also the endpoint of Beijing‚ÄìGuangzhou‚ÄìShenzhen‚ÄìHong Kong high-speed railway ÔºàChinese: ‰∫¨ÂπøÊ∑±È´òÈìÅÁ∫ø), which is a railway of over 2,000 kilometers connecting Beijing and Zhuhai. For me, Zhuhai Railway Station is a must-go if I‚Äôm going anywhere by train. Gongbei Port/Gongbei Port of Entry (Chinese: Êã±ÂåóÂè£Â≤∏) Gongbei Port (Chinese: Êã±ÂåóÂè£Â≤∏) is the biggest and the main port on the Zhuhai-Macau border. Every day, there are hundreds of thousands of people traveling to Macau thorugh this port. This picture was captured on the Chinese New Year‚Äôs Eve in 2018. On that particular day, the total passenger flow exceeded 300,000. Zhuhai Bus A typical old supermarket in Zhuhai Fenghuang Rd (Chinese: Âá§Âá∞Ë∑Ø), Xiangzhou District Fenghuang Rd (Chinese: Âá§Âá∞Ë∑Ø), Xiangzhou District A combined scene of Chinese bike-sharing system and the small but busy shops along the street. Comebuy, one of the most popular boba shops in Zhuhai. Xiangzhou District (Chinese: È¶ôÊ¥≤Âå∫). An old small alley among the buildings. Xiangzhou District is one of the oldest area in Zhuhai, which originates from a tiny village 40 years ago. It is still the political, financial and cultural center of Zhuhai. Just like the countless other alleys, most of the food stalls here have a very long history, and any of them is able to satisfy your appetite. ","date":"2019-03-05","objectID":"/zhuhai/:1:0","tags":["photography","city representation"],"title":"Zhuhai, China ‚Äî Home, Sweet Home","uri":"/zhuhai/"},{"categories":["My Gallery"],"content":"Zhuhai, By the Sea Zhuhai, as a coastal city, has a really pretty seaside view. Although I cannot guarantee that it is as gorgeous and magnificent as those world-class famous tourism spots such as Maldives, Phuket, Miami, etc., it actually reflects the exact feeling of ‚Äúliving by the sea.‚Äù The city of Zhuhai is built on several fishing villages. Before the ‚Äúreform and opening-up‚Äù policy in China in 1978, Zhuhai is still a small fishing village. The dwellers at that time still made their living mainly by hunting fish. As Zhuhai develops, although more and more elements of modern cities have been added to this place (such as malls, amusement parks, etc.), what still remains unchanged is the way of living ‚Äî Zhuhai citizens still love to hang out by the sea. Lovers love to start their stories by the sea. Beaches are always crowded on weekends and holidays. Fish markets are always the busiest among all markets in the city. Even the landmark of Zhuhai ‚Äî is about fish! As a result, besides constructing more and more shopping malls in the city, the City Government of Zhuhai also focuses on improving its coastal line. So far, walking by the sea has absolutely become one of the most relaxing activities in the city life. Feel the wind breeze with the smell of the sea. Watch the boats come and go. Enjoy the sunrise and sunset on the beach. When it comes to the sea, Zhuhai turns into a romantic city. Qi'ao Island (Chinese: Ê∑áÊæ≥Â≤õ). A construction site on a pier. Qi‚Äôao Island (Chinese: Ê∑áÊæ≥Â≤õ). A construction site on a pier. After the successful tryout of Hengqin Free Trade Zone (Ê®™Áê¥Ëá™Ë¥∏Âå∫), Qi‚Äôao Island seems to be planned to become the second center of the Guangdong-Hong Kong-Macau Greater Bay Area (Á≤§Ê∏ØÊæ≥Â§ßÊπæÂå∫). Captures of the coastal line in a stormy afternoon. Xianglu Bay (Chinese: È¶ôÁÇâÊπæ), the most historical bay in the city. Two small fishing boats are parked in the water, with a traditional ship-shaped restaurant captured not far away. City view and the Xianglu Bay in the evening. The lights in the city were just lit up. ","date":"2019-03-05","objectID":"/zhuhai/:2:0","tags":["photography","city representation"],"title":"Zhuhai, China ‚Äî Home, Sweet Home","uri":"/zhuhai/"}]