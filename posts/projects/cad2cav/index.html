<!doctype html><html><head><title>CAD2CAV: Computer Aided Design for Cooperative Autonomous Vehicles</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/navigators/navbar.css><link rel=stylesheet href=/css/plyr.css><link rel=stylesheet href=/css/flag-icon.min.css><link rel=stylesheet href=/css/katex.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css><link rel=icon type=image/png href=/images/site/favicon-32x32-ZR_hu8be55ec36a526970238b8207e0c8eab8_2764_42x0_resize_box_2.png><meta property="og:title" content="CAD2CAV: Computer Aided Design for Cooperative Autonomous Vehicles"><meta property="og:description" content="CAD2CAV is a project focusing on multi-agent exploration in unknown environments. It attempts to build a complete system from perception to planning and control, exploring a designated unknown environments with multiple autonomous vehicles. It is built in xLab at the University of Pennsylvania, on multiple F1TENTH race cars.
Useful documents:
 GitHub link Spring 2021 Final Report (not complete)  Demo This is a sample scenario of a single F1TENTH race car exploring the 2nd floor of Levine Hall at the University of Pennsylvania, right outside xLab."><meta property="og:type" content="article"><meta property="og:url" content="https://zhihaoruan.xyz/posts/projects/cad2cav/"><meta property="article:published_time" content="2021-12-27T23:12:37+00:00"><meta property="article:modified_time" content="2021-12-27T23:12:37+00:00"><meta name=description content="CAD2CAV: Computer Aided Design for Cooperative Autonomous Vehicles"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/css/layouts/single.css><link rel=stylesheet href=/css/navigators/sidebar.css><link rel=stylesheet href=/css/style.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/>Ryan's Blog</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"></ul></div></div></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><a href=/posts/introduction/ title=Introduction>Introduction</a></li><li><i class="fas fa-plus-circle"></i><a href=/posts/lecturenotes/>Lecture Notes</a><ul><li><a href=/posts/lecturenotes/ve320-notes/ title="VE320 Course Final Summary">VE320 Course Final Summary</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/gallery/>My Personal Gallery</a><ul><li><a href=/posts/gallery/ann-arbor/ title="Ann Arbor">Ann Arbor</a></li><li><a href=/posts/gallery/blue-hour/ title="Blue Hour">Blue Hour</a></li><li><a href=/posts/gallery/covid-19/ title="Capturing COVID-19">Capturing COVID-19</a></li><li><a href=/posts/gallery/hong-kong/ title="Hong Kong">Hong Kong</a></li><li><a href=/posts/gallery/dublin/ title=Ireland>Ireland</a></li><li><a href=/posts/gallery/london/ title=London>London</a></li><li><a href=/posts/gallery/michigan/ title=Michigan>Michigan</a></li><li><a href=/posts/gallery/newcastle/ title=Newcastle>Newcastle</a></li><li><a href=/posts/gallery/zhuhai/ title=Zhuhai>Zhuhai</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/projects/>Projects</a><ul class=active><li><a class=active href=/posts/projects/cad2cav/ title=CAD2CAV>CAD2CAV</a></li><li><a href=/posts/projects/565-cuda-path-tracer/ title="CUDA Path Tracer">CUDA Path Tracer</a></li><li><a href=/posts/projects/373proj/ title="EECS 373 Project">EECS 373 Project</a></li><li><a href=/posts/projects/560proj/ title="Mini Minecraft">Mini Minecraft</a></li><li><a href=/posts/projects/450proj/ title="SJTU Undergraduate Design Expo">SJTU Undergraduate Design Expo</a></li><li><a href=/posts/projects/467proj-slam/ title="SLAM Project">SLAM Project</a></li><li><a href=/posts/projects/565-final-project/ title="Volume Rendered ReSTIR">Volume Rendered ReSTIR</a></li><li><a href=/posts/projects/565-vulkan-grass-rendering/ title="Vulkan Grass Rendering">Vulkan Grass Rendering</a></li><li><i class="fas fa-plus-circle"></i><a href=/posts/projects/f1tenth-labs/>F1Tenth Course Labs</a><ul><li><a href=/posts/projects/f1tenth-labs/f1tenth-lab1/ title="F1Tenth Course Lab 1">F1Tenth Course Lab 1</a></li><li><a href=/posts/projects/f1tenth-labs/f1tenth-lab2/ title="F1Tenth Course Lab 2">F1Tenth Course Lab 2</a></li><li><a href=/posts/projects/f1tenth-labs/f1tenth-lab3/ title="F1Tenth Course Lab 3">F1Tenth Course Lab 3</a></li><li><a href=/posts/projects/f1tenth-labs/f1tenth-lab4/ title="F1Tenth Course Lab 4">F1Tenth Course Lab 4</a></li><li><a href=/posts/projects/f1tenth-labs/f1tenth-lab6/ title="F1Tenth Course Lab 6">F1Tenth Course Lab 6</a></li></ul></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(https://zhihaoruan.xyz/images/posts/cad2cav/cad2cav-overview.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/author/avatar_hufae261693e8da39e936529a400eb43d8_104998_120x120_fit_q75_box.jpg alt="Author Image"><h5 class=author-name>Zhihao (Ryan) Ruan</h5><p>:date_full</p></div><div class=title><h1>CAD2CAV: Computer Aided Design for Cooperative Autonomous Vehicles</h1></div><div class=post-content id=post-content><p>CAD2CAV is a project focusing on multi-agent exploration in unknown environments. It attempts to build a complete system from perception to planning and control, exploring a designated unknown environments with multiple autonomous vehicles. It is built in <a href=https://mlab-upenn.github.io/lab_website/>xLab at the University of Pennsylvania</a>, on multiple <a href=https://f1tenth.readthedocs.io/en/latest/>F1TENTH race cars</a>.</p><p>Useful documents:</p><ul><li><a href=https://github.com/mlab-upenn/ISP2021-cad2cav>GitHub link</a></li><li><a href=/files/CAD2CAV_Project_Report.pdf>Spring 2021 Final Report (not complete)</a></li></ul><h2 id=demo>Demo</h2><p>This is a sample scenario of a single F1TENTH race car exploring the 2nd floor of Levine Hall at the University of Pennsylvania, right outside xLab.</p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/CMwnMAKMueA style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><h2 id=motivation>Motivation</h2><p>Autonomous robots have been widely used in a great number of aspects in our daily life. Specifically, mobile robots have demonstrated great help in unknown environment exploration, i.e., rescue robots exploring debris of a building after an earthquake, scientific robots exploring the world under the sea, etc. We are particularly interested in developing a fleet of mobile robots based on the dynamics of a normal self-driving vehicle (i.e., F1TENTH race cars) that helps people explore normal in-use buildings, or buildings that are still under construction.</p><figure><img src=f1tenth_NX.png alt="Sample image of an F1TENTH race car." width=400><figcaption><p>Sample image of an F1TENTH race car.</p></figcaption></figure><p>Normally, one would quickly think of SLAM (Simultaneous Localization and Mapping) when it comes to robotic exploration (for more details, see my <a href=https://zhihaoruan.xyz/posts/projects/467proj-slam/>SLAM project</a>). Yes, that is one of the most straightforward and simplest solution for a normal robotic exploration task. However, under the assumption of exploring a known building, we are actually given the access an extra layer of structural information for the target environment, i.e. the <em>floor plan</em> of the building. How to exploit that information and set up a multi-agent exploration task efficiently? That becomes the main goal for this project.</p><h2 id=technical-details>Technical Details</h2><h3 id=overview>Overview</h3><p>The general structure of this project is divided into 3 modules: a central ROS server, a building renderer/simulator, and the F1TENTH race car onboard computing platform.</p><figure><img src=cad2cav-overview.png alt="A diagram for the system design of CAD2CAV."><figcaption><p>A diagram for the system design of CAD2CAV.</p></figcaption></figure><p>The figure above demonstrates the general relationship among the 3 modules. Given the prior knowledge of the building&rsquo;s floor plan (which is assumed to come from <a href="https://www.autodesk.com/products/revit/overview?term=1-YEAR&tab=subscription">Autodesk Revit</a>), the building renderer would parse and construct a rendered 3D model our of it for visualization; the central ROS server would extract necessary structural information and construct 2D occupancy map for motion planning & mapping nodes. As soon as the central ROS server generates a balanced multi-agent routing scheme, it would pass the plan to each F1TENTH race car and the race car would then execute the drive node to drive the car and follow the high-level routing scheme in the real world.</p><p>As the race car is driving in the environment, its perception module would try to localize itself in the occupancy map and update the perceived objects in the map. The map then gets updated in the central ROS server and the server would also pass the updated map to the building renderer. The building renderer would finally render everything and visualize the building in the software.</p><h3 id=building-renderer>Building Renderer</h3><p>As mentioned above, we need a building renderer to render 3D models and visualize everything on the screen. Our current choice is to use Unreal Engine 4. UE4 has a popular plugin called Unreal Datasmith compatible with most of the mainstream model designing softwares, i.e., Autodesk Revit, Cinema 4D (C4D), Rhino, etc.</p><p>Users can install and try the plugin by following the tutorial of <a href=https://docs.unrealengine.com/4.27/en-US/WorkingWithContent/Importing/Datasmith/SoftwareInteropGuides/Revit/ExportingDatasmithContentfromRevit/>Exporting Datasmith Content from Revit</a> and <a href=https://docs.unrealengine.com/4.27/en-US/WorkingWithContent/Importing/Datasmith/HowTo/ImportingContent/>Importing Datasmith Content into Unreal Engine 4</a>. We also hosted a pre-imported model of Levine Hall 4th floor on <a href=https://github.com/shineyruan/unreal_levine_4>GitHub</a>.</p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/wf6D9ydkIpc style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><h4 id=other-options>Other Options</h4><p>We are also looking into trying out different options that are more friendly to robotics application development than UE4. One of them is the <a href=https://developer.nvidia.com/isaac-sim>Nvidia Isaac Sim</a> powered by the newly-announced Nvidia Omniverse platform.</p><h3 id=central-ros-server>Central ROS Server</h3><p>The central ROS server is responsible for: 1) parsing floor plan from Revit and constructing initial occupancy grid; 2) generating high-level balanced multi-agent routing scheme from occupancy map; 3) updating occupancy map from the perception data; 4) sending occupancy map information over to building renderer. Each of these tasks is a sub-problem in different fields of robotics, in the general sense.</p><h4 id=initial-map-construction-from-revit>Initial Map Construction from Revit</h4><p>Revit models are encoded in a unique format special to the software. Hence, the only way for us to extract information from a general Revit model is through the <a href=https://www.revitapidocs.com/2022/>Revit API</a>. Revit API is initially written in C#, while it has been recently transferred to Python by an open-source project called <a href=https://github.com/eirannejad/pyRevit>pyRevit</a>. By following the official installation tutorials of pyRevit, I have managed to create a pyRevit plugin for the project to specifically export the 2D geometry information of all the main elements in a building floor plan. The current supported elements and their geometric representations are:</p><ol><li>Walls, represented as 2D line segments;</li><li>Doors, represented as 2D points with orientation;</li><li>Windows, represented as 2D points with orientation.</li></ol><p>The current pyRevit plugin is hosted on <a href=https://github.com/shineyruan/cad2cav_revit_plugins>GitHub</a> and is written in IronPython 2.7.<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> It runs in Autodesk Revit 2022. Upon one button click, it would try to look for these elements in the model and print their geometric representation information in the console with the following specific format:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-txt data-lang=txt>
Type,x_1,y_1,z_1,x_2,y_2,z_2,Orientation,Width,Height
wall,-17.3718,12.2507,0.0,-16.7919,12.8305,0.0,0.0,0.0,0.0
door,-6.2356,2.5174,0.0,0.0,0.0,0.0,1.5708,0.864,2.032
window,-0.915,-3.4125,0.9,0.0,0.0,0.0,3.1416,1.05,1.35

</code></pre></div><p>Users are supposed to save the console outputs into a CSV file and put it manually in the ROS codebase.</p><p>In the CSV format:</p><ul><li><code>Type</code> is a string describing the object type;</li><li><code>x_1</code>, <code>y_1</code>, <code>z_1</code> is the 3D Cartesian coordinate of the 1<sup>st</sup> endpoint of the line segment, or the position of the oriented point;</li><li><code>x_2</code>, <code>y_2</code>, <code>z_2</code> is the 3D Cartesian coordinate of the 2<sup>nd</sup> endpoint of the line segment, or left blank if the type is an oriented point;</li><li><code>Orientation</code> is the orientation of the point, or left blank if the type is a line segment;</li><li><code>Width</code> and <code>Height</code> are the actual width and height of the doors and windows in the Revit model, and are left blank for the walls.</li></ul><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/6ka9pvTfP-U style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><p>After we have the necessary structural information in a floor plan contained in a CSV file, we can then proceed to load the file in ROS and construct an initial occupancy map out of it. Since all information are transformed into some basic geometric shapes, it is then very easy for us to draw these shapes on a 2D grid. The only algorithm we use here is the <a href=https://en.wikipedia.org/wiki/Bresenham%27s_line_algorithm>Bresenham&rsquo;s line algorithm</a>, which enables fast grid marching along a certain 2D line segment. We set everything cell in the grid that is part of the geometric shapes to be <code>OCCUPIED</code> and an initial map for the floor plan is then produced.</p><table><thead><tr><th style=text-align:center>Revit</th><th style=text-align:center>Occupancy Grid</th></tr></thead><tbody><tr><td style=text-align:center><img src=revit_floorplan_levine2.png alt></td><td style=text-align:center><img src=map_floorplan_levine2.png alt></td></tr></tbody></table><h4 id=waypoint-identification-and-registration>Waypoint Identification and Registration</h4><p>To generate a high-level routing scheme for multiple vehicles to explore, it is important to know which set of locations or areas that the exploration needs to cover. While we&rsquo;re still exploring other options<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>, for simplicity we provide an interface for users to select and register waypoints on the map in a pop-up window. As soon as the waypoints are registered, the server would construct a graph out of all the waypoints, and the routing algorithms will be performed with the scope of an abstract graph.</p><figure><img src=user_waypoint_registration.png alt="A screenshot of a sample user waypoint registration interface." width=600><figcaption><p>A screenshot of a sample user waypoint registration interface.</p></figcaption></figure><h4 id=graph-planning-and-routing>Graph Planning and Routing</h4><p>All the high-level routing algorithms are conducted on an abstract graph. In this task we&rsquo;re interested in assign an exploration route for each vehicle, such that all vehicles gets an exploration path of equal or similar cost to each other.</p><h5 id=the-capacitated-vehicle-routing-problem-cvrp>The Capacitated Vehicle Routing Problem (CVRP)</h5><p>Vehicle Routing Problem (VRP) is a famous set of combinatorial optimization problems widely used in operations research. It is essentially a generalization to the famous Travelling Salesman Problem (TSP), which only focuses on computing the cheapest transportation loop for one single agent. The VRP can be described as the following:<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup></p><blockquote><p>Concerning the service of a delivery company. Things are delivered from one or more <em>depots</em> which has a given set of home <em>vehicles</em> and operated by a set of <em>drivers</em> who can move on a given <em>road network</em> to a set of <em>customers</em>. Determine the set of <em>routes</em> (one route for each vehicle that must start and finish at its own depot) such that all customers' requirements and operational constraints are satisfied and the <em>global transportation cost</em> is minimized.</p></blockquote><p>From an optimization problem point of view, VRP essentially tries to minimize the transportation cost under some constraints. Notice that VRP itself does not specify any definition for the transportation costs in the first place. Here we choose to minimize the Euclidean distance as the objective function. The original VRP problem does not specify any constraints either. However, as the optimal solution for an unconstrained VRP would fall back to a TSP loop which is meaningless for our project, we introduce a capacity constraint on each vehicle, which forms the Capacitated VRP (CVRP) problem:</p><blockquote><p>Each vehicle has a fixed <em>capacity</em> defining the maximum number of nodes it could visit in one run. Once the vehicle has visited the maximum number of nodes after it leaves the depot, it must return to the depot immediately.</p></blockquote><p>The formulation of CVRP in our project would be simply minimizing the total Euclidean distance of the set of routes over the graph of waypoints in Revit coordinates. By tuning the capacity of the vehicle as a hyperparameter, we are able to generate a set of routes that covers all waypoints and minimizes the total travelled distance in the same time, for a fixed number of vehicles.</p><p>Solving the CVRP problem in our project involves the use of <a href=https://en.wikipedia.org/wiki/Ant_colony_optimization_algorithms>Any Colony Optimization</a> (ACO) algorithm. As the general CVRP problem is NP-hard, ACO is used as an approximation to the optimal solution that runs in polynomial time.</p><p>A general description of ACO algorithm can be described in the pseudo-code:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>procedure ACO_MetaHeuristic <span style=color:#f92672>is</span>
    <span style=color:#66d9ef>while</span> <span style=color:#f92672>not</span> terminated do
        generateSolutions()
        daemonActions()
        pheromoneUpdate()
    repeat
end procedure
</code></pre></div><p>The essence of ACO is to mimic a colony of exploring ants. During exploration, each ant would leave a pheromone trail behind its body, and other ants would smell it and intend to follow the path of the previous ants. Meanwhile, each ant also has an intention of exploring unvisited places. The combination of two different intentions forms an <a href=https://www.geeksforgeeks.org/epsilon-greedy-algorithm-in-reinforcement-learning/>$\varepsilon$-greedy strategy</a> for each ant, balancing exploration with exploitation.</p><p>Our implementation of ACO algorithm also involves a timeout variable for terminating the while loop, generating graph search solutions for each ant, determining the next action for the current ant, and updating the pheromone trails. Notice that in any CVRP solution, all vehicles must start off from one common depot.</p><table><thead><tr><th style=text-align:center>CVRP Solution for Vehicle 1</th><th style=text-align:center>CVRP Solution for Vehicle 2</th></tr></thead><tbody><tr><td style=text-align:center><img src=vrp-1.png alt></td><td style=text-align:center><img src=vrp-2.png alt></td></tr></tbody></table><h5 id=spectral-clustering>Spectral Clustering</h5><p>Another way to tackle this multi-agent routing task is to divide the entire graph into several partitions. In each partition, we find a TSP path that connects all nodes in the subgraph and minimizes the travel cost. <strong>The advantage of this method is that vehicles are <em>not</em> required to start off from one common depot anymore, which makes the algorithm suitable for replanning at any time during the exploration.</strong></p><p>Spectral clustering is an intuitive way to group graph nodes together based on spectral coordinates. This algorithm initially comes from the theory of unsupervised learning. The spectral coordinates of a graph is defined to be the row vectors of the eigenvector matrix of the Laplacian of the graph. We can obtain the spectral coordinates for each graph node in the following way:</p><ol><li>Construct an adjacency matrix $W$ for the graph;</li><li>Compute the degree matrix $D$ by summing up all elements per row of $W$ and make it a diagonal matrix.</li><li>Compute the graph Laplacian $L$ by $L=D-W$.</li><li>Compute the eigenvalues and eigenvectors of $L$, sort them in ascending order.</li><li>The spectral coordinates of each graph node is the corresponding row of the eigenvector matrix of $L$.</li></ol><figure><img src=spectral-clustering.png alt="Referenced from EECS 445, the University of Michigan" width=600><figcaption><p>Referenced from EECS 445, the University of Michigan</p></figcaption></figure><p>After obtaining the spectral coordinates for each node, we then apply a simple clustering algorithm on the spectral space of the nodes. One popular choice would be <a href=https://en.wikipedia.org/wiki/K-means_clustering>k-means clustering</a>. As for us, we replaced k-means with <a href=https://en.wikipedia.org/wiki/K-means%2B%2B>k-means++</a> as a quick and simple improvement, which improves the initialization of the centroids to make the clustering outcome more stable.</p><p>Spectral clustering tends to group similar nodes together. The definition of similarity lies in the definition of the weight of the edges. That is, the lower the edge weight is between two nodes, the higher chances that they are grouped together. This property of spectral clustering aligns perfectly with our desire, i.e., nodes in the neighborhood form a subpart of the graph. Hence, we could directly apply spectral clustering to our graph partitioning task and generate TSP loop within each graph partition.</p><p>The application of spectral clustering implicitly assume that the graph is complete, i.e., every two nodes are connected. Hence, it cannot generalize to all situations, especially those incomplete graphs generated from some special floor plans.</p><table><thead><tr><th style=text-align:center>Spectral Clustering for Vehicle 1</th><th style=text-align:center>Spectral Clustering for Vehicle 2</th></tr></thead><tbody><tr><td style=text-align:center><img src=spectral-1.png alt></td><td style=text-align:center><img src=spectral-2.png alt></td></tr></tbody></table><h5 id=multi-level-k-way-graph-partitioning>Multi-level $k$-Way Graph Partitioning</h5><p>Another way of partitioning the graph is to formulate a $k$-way graph partitioning problem. Traditional graph partitioning problems aims to find a minimum cut to divide the graph subject to the constraint that all resulting subparts are as evenly partitioned as possible. One handy multi-level $k$-way partitioning algorithm<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup> for this builds upon the <a href=https://en.wikipedia.org/wiki/Kernighan%E2%80%93Lin_algorithm>Kernighan-Lin heuristic</a>. For simplicity, we directly incorporated <a href=http://glaros.dtc.umn.edu/gkhome/metis/metis/overview>METIS</a> to perform the graph partitioning task. We then generate TSP loops in each graph subpart using <a href=https://developers.google.com/optimization>Google OR-Tools</a>.</p><table><thead><tr><th style=text-align:center>$k$-Way Graph Partitioning for Vehicle 1</th><th style=text-align:center>$k$-Way Graph Partitioning for Vehicle 2</th></tr></thead><tbody><tr><td style=text-align:center><img src=partitioning-1.png alt></td><td style=text-align:center><img src=partitioning-2.png alt></td></tr></tbody></table><h5 id=graph-planning-summary-and-demo>Graph Planning Summary and Demo</h5><p>We explored 3 different ways of distributing different routes to each vehicles based on abstract graphs in the graph planning task in our project. One of them requires a common starting point (initial depot) and the others are based on graph partitioning. All of these methods tries to find a set of routes that covers all the floor plan with minimum overlap and complete loops. Although we have implemented all 3 methods in our code base, our default and most up-to-date choice would be the application of METIS and multi-level $k$-way partitioning algorithm.</p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/ufkgAVptQT8 style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><h3 id=f1tenth-onboard-computing-platform>F1TENTH Onboard Computing Platform</h3><p>The latest version of F1TENTH race car is equipped with a nice Nvidia Xavier NX Development Board as the onboard computing platform, which contains both an ARMv8 CPU and a CUDA-enabled GPU. It runs a complete but special Ubuntu system developed and shipped by Nvidia. In general, it is capable of performing part of the computationally heavy tasks in our project. In our design, we run the low-level planning and control node as well as the localization and mapping node onboard.</p><h4 id=obstacle-avoiding-planning-and-control>Obstacle-Avoiding Planning and Control</h4><p>After we have a complete route for each vehicle, we need to design an algorithm for the vehicle to actually reach those waypoints in the real world. Hence, the algorithm is required to be fast, accurate, and responsive. In this task, we choose to apply the <a href=https://arxiv.org/abs/1306.3532>Fast Marching Tree (FMT*)</a> algorithm as a low-level planner to generate real-time trajectories for each vehicle. As a sampling-based planning algorithm, FMT* is fast enough to replan in a very short amount of time so that the vehicle is able to avoid obstacles.</p><p>As a general overview, FMT* combines the properties of two different sampling-based planning categories: <a href=https://en.wikipedia.org/wiki/Probabilistic_roadmap>PRM (Probabilistic Road Map)</a> and <a href=https://en.wikipedia.org/wiki/Rapidly-exploring_random_tree>RRT (Rapidly-exploring Random Trees)</a>. It first samples a set of points from the planning space (or configuration space), and tries to grow a tree from the start to the end in an RRT-style. It defines an unvisited set $V_\text{unvisited}$, an open set $V_\text{open}$ and a closed set $V_\text{closed}$. As the following figure shows, it involves the following steps:</p><ol><li>Add the starting point to $V_\text{closed}$.</li><li>Add all of the neighboring points of the starting point to $V_\text{open}$. All the other points are in $V_\text{unvisited}$.</li><li>Select the lowest cost node $z\in V_\text{open}$ and finds its neighbors $\mathcal{N}(z)\subseteq V_\text{unvisited}$.</li><li>Given a node $x\in\mathcal{N}(z)$, find the optimal 1-step connection to its neighboring nodes $\mathcal{N}(x)$ in $V_\text{open}$.</li><li>Check collision on all newly-added connections and apply penalties for violations (i.e., remove all the collision connections);</li><li>Add all successful connected nodes to $V_\text{open}$, and move $z$ to $V_\text{closed}$.</li><li>Repeat the operations from Step 3 until the goal is reached.</li></ol><figure><img src=fmt_star.png alt="FMT* algorithm cited from the paper." width=600><figcaption><p>FMT* algorithm cited from the paper.</p></figcaption></figure><p>FMT* works as a low-level planner on the vehicle and generates trajectory towards the next waypoint in the high-level route. For now, the trajectory consists of a set of more fine-grained 2D points. For future work orientation can be added to the trajectory point as an extra information so that the vehicle could be better controlled in terms of steering.</p><p>As for the control algorithm, we currently run the traditional pure-pursuit algorithm to drive the vehicle to follow the trajectory points. For a more detailed overview of the pure-pursuit algorithm please checkout <a href=https://zhihaoruan.xyz/posts/projects/f1tenth-labs/f1tenth-lab6/>my F1TENTH Lab 6 report</a>.</p><p>Future work on the control algorithm involves developing a <a href=https://en.wikipedia.org/wiki/Model_predictive_control>model predictive control (MPC)</a> algorithm, but for the sake of simplicity we still stick to pure-pursuit for the moment.</p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/XoU88reSlNA style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><h4 id=localization>Localization</h4><p>In a real-world experiment, each vehicle needs to know where it is in the map. Hence, it is very important for the vehicle to have a localization module onboard. Common localization algorithms in general robotics include all kinds of filtering algorithms, from Bayes filer, <a href=https://en.wikipedia.org/wiki/Kalman_filter>Kalman filter (KF)</a> and its variations (<a href=https://en.wikipedia.org/wiki/Extended_Kalman_filter>EKF</a>, <a href=https://groups.seas.harvard.edu/courses/cs281/papers/unscented.pdf>UKF</a>, etc.), and <a href=https://en.wikipedia.org/wiki/Particle_filter>particle filter</a>. We choose to use particle filter as a starting point and incorporated a CUDA-accelerated package of particle filter<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup> from the MIT race car team.</p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/58snKwkVv1g style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div></br><blockquote><p>Thanks to my project partner Shumin for her work on the localization module. <strong>The rest of this section is reposted from</strong> <a href=https://shumin326.github.io/shumin.github.io/jekyll/update/2021/12/21/IndependentStudy.html><strong>Shumin&rsquo;s article</strong></a>.</p></blockquote><h5 id=overview-1>Overview</h5><p>Mapping and localization algorithms require a manual walkthrough and capture of the environment. Upon loop closure they are ready to navigate with algorithms like SLAM. This project’s goal is to improve this process for indoor spaces with two steps:</p><ol><li>Cheap initial navigable map, e.g. floor plans. Use architectural maps to slow the cost and increase the speed of the initial map capture by eliminating the manual walkthrough. We aim to accomplish this by transforming the building’s blueprint into a 3D navigable world with occupancy map and semantic landmarks within which we do path planning and localization.</li><li>Landmark detection: We implemented a landmark detection model so that robot can recognize landmarks in real world navigation. Those landmarks are further utilized to improve localization performance.</li></ol><p>This independent study developed a complete pipeline for robots navigation under indoor environments, e.g. hospitals and offices, with only an architectural map.</p><p>The overall workflow is shown in the figure below. Given a building blueprint, e.g. floor plans, the Auto Mapping module converts the floor plan into an occupancy map and a list of landmarks with accurate location and orientation. The planning module determines a navigation strategy for robots to navigate and has the potential to apply onto multi-agent navigation applications. It first lets the user manually determine a set of sparse waypoints in order to make sure that LiDAR scans cover every corner of the building. Then it uses a Pure Pursuit controller and FMT* path planner to navigate among those waypoints. Graph partitioning algorithm and closed TSP solution are used to make sure all waypoints are visited only once. The landmark detection module detects landmarks and also shares this information in the format of labeled bounding boxes with the localization module. The Localization module uses particle filter to fuse sensor data, i.e. lidar scan, odometry and landmarks, and obtain an estimated pose.</p><figure><img src=https://i.ibb.co/5cpPZ1V/system-overview.png alt=system-overview></figure><h5 id=localization-1>Localization</h5><p>The localization part is the main focus of this project. It aims at providing a robust and accurate pose estimate for other modules (planning and control, etc) to function properly. The localization module takes as input the laser scans, noisy odometry and landmarks, and fuses them with a particle filter (alg. 1). There are mainly three updates happening inside the loop:</p><ol><li>Motion model update using odometry. This step updates the particle poses by dead reckoning the odometry data and will have accumulated error overtime. Since the odometry used in onboard is quite noisy, the motion model update was manually added some gaussian noise.</li><li>Sensor update with laser scan. This step updates the particle weights by comparing the obtained ranges from laser scan with the ray marching ranges computed by RangeLibc and the reference map.</li><li>Sensor update with landmarks. This step updates the particle weights by doing a landmark projection and matching(shown in Alg.2) algorithm which compares the observed landmarks with reference landmarks. More details will be provided in the next section.</li></ol><figure><img src=https://i.ibb.co/68YNY9z/algo1.png alt=algo1></figure><h5 id=landmark-detection>Landmark Detection</h5><p>The landmark detection is developed using a well known, high performance and highly customizable object detection model: YOLO. Since our indoor environment is vastly different from its training dataset environments where images are collected mostly outdoors and from a human perspective, a customized dataset was created for fine-tuning the YOLO model. The trained model can perform very robust and fast indoor landmark detection which is exactly what we needed for this project.</p><figure><img src=https://i.ibb.co/KWFN2sT/landmark-detection.png alt=landmark-detection></figure><h5 id=landmark-projection-and-matching>Landmark Projection and Matching</h5><p>The detected landmarks as bounding boxes can be used to further enhance the localization performance because we already have certain landmarks positions and orientations embedded in the floor maps, e.g. doors and windows. Thus, this section introduces the landmark projection and matching algorithm (alg. 2) that uses reference landmarks of floor maps and detect landmark in real time to update particle weights and improve the robustness of localization module especially at low-feature places such as the narrow hallway.</p><figure><img src=https://i.ibb.co/GJHNcCd/alg2.png alt=alg2></figure><p>Determining the “seeable” landmarks is to check whether a landmark satisfies the three conditions (also shown in the figure below).</p><ol><li>It’s not blocked by any obstacle</li><li>It’s within camera’s sight distance</li><li>It’s within the camera’s field of view (FOV).</li></ol><p>The first requirement is ensured by comparing the ray marching result from robot to the landmark and the actual distance between them two. If they’re equal(with some tolerance) then the landmark is not blocked by obstacles. The second and third requirements are ensured by computing the distance and angles between robot and landmark and compare with sight distance as well as camera FOV.</p><figure><img src=https://i.ibb.co/3SqYh4n/seeable.png alt=seeable></figure><p>Seeable landmarks are in world frame but the detected landmarks are in camera frame, so we need to transform the seeable landmarks from world to camera frame. For simplicity, the frames are denoted as world ($w$), Robot ($R$) and Camera ($C$):</p><p>$$
L^C=T^C_R\cdot T^R_w\cdot L^w
$$</p><p>Where $L^w$ is the landmark in world frame and $L^C$ is the converted landmarks in the camera frame. The transformation matrices can be obtained using particle pose $\pi=(x,y,\theta)$ and camera extrinsic matrix $E$ and intrinsic matrix $I$.</p><p>$$
T^C_R=E\cdot I
$$</p><p>$$
T^R_W=\begin{bmatrix}
R_{z,\theta} & d \\ \mathbf{0} & 1
\end{bmatrix},\quad d=[x,y,0]^T
$$</p><p>Up to now we’ve found the set of expected landmarks to be seen in the camera frame and we’re ready to compare them with detected landmarks $L$. Since the number of two sets are not necessarily equal, use $k$-nearest neighbor (kNN) to find matches and compute the average distance $d$. Then update the particle weight $w$:</p><p>$$
w=1/(1+d)
$$</p><p>Note that in order to make the computation less stressful so it can be run in real time, the particle filter algorithm is vectorized instead of doing a loop for each particle, the latter is just to make the algorithm easier to understand.</p><h5 id=code-and-demo-videos>Code and Demo Videos</h5><p>The <a href=https://github.com/mlab-upenn>mLab github</a> contains the repos for all the modules including localization, landmark detection, as well as planning module and Auto-mapping module. They&rsquo;re for the whole project &ldquo;CAD2CAV: From Building Blueprints to Scalable Multi-robot navigation&rdquo;.</p><p>in specific, the repos for my independent study project are:</p><ol><li><a href=https://github.com/shineyruan/particle_filter>Localization module</a>.</li><li><a href=https://github.com/Shumin326/darknet_ros>Landmark detection module</a>. It also contains instructions on fine tuning as well as a link to my model.</li></ol><p>The demo videos can be viewed in the repos(listed above) README as well as this <a href="https://www.youtube.com/playlist?list=PL8Fip0E9YRSuZok9umO4fMpGWx_UsfoaL">youtube video list</a>.</p><h2 id=references>References</h2><ul><li><a href=https://github.com/eirannejad/pyRevit>eirannejad/pyRevit</a>.</li><li><a href=https://github.com/mit-racecar/particle_filter>mit-racecar/particle_filter</a>.</li><li><a href=https://github.com/google/or-tools>google/or-tools</a>.</li><li><a href=https://github.com/AprilRobotics/apriltag>AprilRobotics/apriltag</a>.</li><li><a href=https://github.com/ben-strasser/fast-cpp-csv-parser>ben-strasser/fast-cpp-csv-parser</a>.</li><li><a href=https://github.com/leggedrobotics/darknet_ros>leggedrobotics/darknet_ros</a>.</li><li>A. Y. Ng, M. I. Jordan, and Y. Weiss, &ldquo;On spectral clustering: Analysis and an algorithm,&rdquo; in Proceedings of the 14th International Conference on Neural Information Processing Systems: Natural and Synthetic, ser. NIPS’01. Cambridge, MA, USA: MIT Press, 2001, p. 849–856.</li><li>B. Lau, C. Sprunk, and W. Burgard, &ldquo;Improved updating of euclidean distance maps and Voronoi diagrams,&rdquo; in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems, Taipei, Taiwan, 2010. [Online]. Available: <a href=http://ais.informatik.uni-freiburg.de/publications/papers/lau10iros.pdf>http://ais.informatik.uni-freiburg.de/publications/papers/lau10iros.pdf</a></li><li>B. W. Kernighan and S. Lin, &ldquo;An efficient heuristic procedure for partitioning graphs,&rdquo; The Bell System Technical Journal, vol. 49, no. 2, pp. 291–307, 1970.</li><li>C. Johnson, &ldquo;Topological mapping and navigation in real-world environments,&rdquo; Ph.D. dissertation, University of Michigan, 2018. Available: <a href=https://deepblue.lib.umich.edu/handle/2027.42/144014>https://deepblue.lib.umich.edu/handle/2027.42/144014</a></li><li>D. Arthur and S. Vassilvitskii, &ldquo;K-means++: The advantages of careful seeding,&rdquo; in Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, ser. SODA ’07. USA: Society for Industrial and Applied Mathematics, 2007, p. 1027–1035.</li><li>E. Olson, &ldquo;AprilTag: A robust and flexible visual fiducial system,&rdquo; 2011 IEEE International Conference on Robotics and Automation, 2011, pp. 3400-3407, doi: 10.1109/ICRA.2011.5979561.</li><li>G. Karypis and V. Kumar, &ldquo;Multilevel k-way partitioning scheme for irregular graphs,&rdquo; Journal of Parallel and Distributed Computing, vol. 48, no. 1, pp. 96–129, 1998. [Online]. Available: <a href=https://www.sciencedirect.com/science/article/pii/S0743731597914040>https://www.sciencedirect.com/science/article/pii/S0743731597914040</a></li><li>J. Wang and E. Olson, &ldquo;AprilTag 2: Efficient and robust fiducial detection,&rdquo; 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2016, pp. 4193-4198, doi: 10.1109/IROS.2016.7759617.</li><li>L. Janson, E. Schmerling, A. Clark, and M. Pavone, &ldquo;Fast marching tree: a fast marching sampling-based method for optimal motion planning in many dimensions,&rdquo; 2015.</li><li>S. L. Bowman, N. Atanasov, K. Daniilidis and G. J. Pappas, &ldquo;Probabilistic data association for semantic SLAM,&rdquo; 2017 IEEE International Conference on Robotics and Automation (ICRA), 2017, pp. 1722-1729, doi: 10.1109/ICRA.2017.7989203.</li><li>Yang, Zhong & Yao, Baozhen. (2009). &ldquo;An Improved Ant Colony Optimization for Vehicle Routing Problem.&rdquo; European Journal of Operational Research. 196. 171-176. 10.1016/j.ejor.2008.02.028.</li></ul><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p><a href=https://ironpython.net/>IronPython</a> is the Python extension for Microsoft .Net environment. It is used by pyRevit as the default Python wrapper over C# Revit API. <a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2 role=doc-endnote><p>Other options include the algorithm of extracting a Voronoi skeleton out of a general occupancy map. This work was initially proposed by Lau et al. in the IROS 2010 paper <a href=http://ais.informatik.uni-freiburg.de/publications/papers/lau10iros.pdf>&ldquo;Improved updating of euclidean distance maps and Voronoi diagrams&rdquo;</a>. <a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3 role=doc-endnote><p>From Wikipedia: <a href=https://en.wikipedia.org/wiki/Vehicle_routing_problem>Vehicle Routing Problem</a>. <a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4 role=doc-endnote><p>See <a href=http://glaros.dtc.umn.edu/gkhome/metis/metis/overview>METIS</a> and <a href=https://www.sciencedirect.com/science/article/pii/S0743731597914040>&ldquo;Multilevel k-way partitioning scheme for irregular graphs&rdquo;</a>. <a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5 role=doc-endnote><p>See <a href=https://github.com/mit-racecar/particle_filter>mit-racecar/particle_filter</a>. <a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div><div class="col-md-6 btn-improve-page"><a href=https://github.com/shineyruan/shineyruan.github.io/edit/main/content/posts/Projects/cad2cav/index.md title="Improve this page" target=_blank rel=noopener><i class="fas fa-code-branch"></i>Improve this page</a></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/posts/gallery/zhuhai/ title="Zhuhai, China — Home, Sweet Home" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i>Prev</div><div class=next-prev-text>Zhuhai, China — Home, Sweet Home</div></a></div><div class="col-md-6 next-article"><a href=/posts/projects/565-cuda-path-tracer/ title="CUDA Path Tracer with À-Trous Denoiser" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>CUDA Path Tracer with À-Trous Denoiser</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#demo>Demo</a></li><li><a href=#motivation>Motivation</a></li><li><a href=#technical-details>Technical Details</a><ul><li><a href=#overview>Overview</a></li><li><a href=#building-renderer>Building Renderer</a><ul><li><a href=#other-options>Other Options</a></li></ul></li><li><a href=#central-ros-server>Central ROS Server</a><ul><li><a href=#initial-map-construction-from-revit>Initial Map Construction from Revit</a></li><li><a href=#waypoint-identification-and-registration>Waypoint Identification and Registration</a></li><li><a href=#graph-planning-and-routing>Graph Planning and Routing</a><ul><li><a href=#the-capacitated-vehicle-routing-problem-cvrp>The Capacitated Vehicle Routing Problem (CVRP)</a></li><li><a href=#spectral-clustering>Spectral Clustering</a></li><li><a href=#multi-level-k-way-graph-partitioning>Multi-level $k$-Way Graph Partitioning</a></li><li><a href=#graph-planning-summary-and-demo>Graph Planning Summary and Demo</a></li></ul></li></ul></li><li><a href=#f1tenth-onboard-computing-platform>F1TENTH Onboard Computing Platform</a><ul><li><a href=#obstacle-avoiding-planning-and-control>Obstacle-Avoiding Planning and Control</a></li><li><a href=#localization>Localization</a><ul><li><a href=#overview-1>Overview</a></li><li><a href=#localization-1>Localization</a></li><li><a href=#landmark-detection>Landmark Detection</a></li><li><a href=#landmark-projection-and-matching>Landmark Projection and Matching</a></li><li><a href=#code-and-demo-videos>Code and Demo Videos</a></li></ul></li></ul></li></ul></li><li><a href=#references>References</a></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://zhihaoruan.xyz#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://zhihaoruan.xyz#skills>Skills</a></li><li class=nav-item><a class=smooth-scroll href=https://zhihaoruan.xyz#experiences>Experiences</a></li><li class=nav-item><a class=smooth-scroll href=https://zhihaoruan.xyz#education>Education</a></li><li class=nav-item><a class=smooth-scroll href=https://zhihaoruan.xyz#projects>Projects</a></li><li class=nav-item><a class=smooth-scroll href=https://zhihaoruan.xyz#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:shineyruan@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span><span>shineyruan@gmail.com</span></a></li><li><a href=https://github.com/shineyruan target=_blank rel=noopener><span><i class="fab fa-github"></i></span><span>shineyruan</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_2.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2018 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=/js/jquery-3.4.1.min.js></script><script type=text/javascript src=/js/popper.min.js></script><script type=text/javascript src=/js/bootstrap.min.js></script><script type=text/javascript src=/js/navbar.js></script><script type=text/javascript src=/js/plyr.js></script><script type=text/javascript src=/js/main.js></script><script type=text/javascript defer src=/js/katex.min.js></script><script type=text/javascript defer src=/js/auto-render.min.js onload=renderMathInElement(document.body);></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=/js/single.js></script><script>hljs.initHighlightingOnLoad();</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:'$$',right:'$$',display:true},{left:'$',right:'$',display:false},{left:'\\(',right:'\\)',display:false},{left:'\\[',right:'\\]',display:true}],throwOnError:false});});</script></body></html>