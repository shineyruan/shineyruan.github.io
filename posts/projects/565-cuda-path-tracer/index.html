<!doctype html><html><head><title>CUDA Path Tracer with À-Trous Denoiser</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/navigators/navbar.css><link rel=stylesheet href=/css/plyr.css><link rel=stylesheet href=/css/flag-icon.min.css><link rel=stylesheet href=/google-fonts/Mulish/mulish.css><link rel=stylesheet href=/fontawesome/css/all.min.css><link rel=icon type=image/png href=/images/site/favicon-32x32-ZR_hu8be55ec36a526970238b8207e0c8eab8_2764_42x0_resize_box_3.png><meta property="og:title" content="CUDA Path Tracer with À-Trous Denoiser"><meta property="og:description" content="Tested on: Ubuntu 20.04 LTS, Ryzen 3700X @ 2.22GHz 48GB, RTX 2060 Super @ 7976MB CUDA Path Tracer Highlights Finished path tracing core features:
diffuse shaders perfect specular reflection 1st-bounce ray intersection caching radix sort by material type path continuation/termination by Thrust stream compaction Finished Advanced Features:
Refraction with Fresnel effects using Schlick&rsquo;s approximation Stochastic sampled anti-aliasing Physically-based depth of field OBJ mesh loading with tinyobjloader Background: Ray Tracing Ray tracing is a technique commonly used in rendering."><meta property="og:type" content="article"><meta property="og:url" content="https://zhihaoruan.xyz/posts/projects/565-cuda-path-tracer/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-12-27T00:43:29+00:00"><meta property="article:modified_time" content="2021-12-27T00:43:29+00:00"><meta name=description content="CUDA Path Tracer with À-Trous Denoiser"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/css/layouts/single.css><link rel=stylesheet href=/css/navigators/sidebar.css><link rel=stylesheet href=/css/style.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/>Ryan's Blog</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"></ul></div></div></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts/ data-filter=all>Posts</a></li><div class=subtree><li><a href=/posts/introduction/ title=Introduction>Introduction</a></li><li><i class="fas fa-plus-circle"></i><a href=/posts/lecturenotes/>Lecture Notes</a><ul><li><a href=/posts/lecturenotes/ve320-notes/ title="VE320 Course Final Summary">VE320 Course Final Summary</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/posts/gallery/>My Personal Gallery</a><ul><li><a href=/posts/gallery/ann-arbor/ title="Ann Arbor">Ann Arbor</a></li><li><a href=/posts/gallery/blue-hour/ title="Blue Hour">Blue Hour</a></li><li><a href=/posts/gallery/covid-19/ title="Capturing COVID-19">Capturing COVID-19</a></li><li><a href=/posts/gallery/hong-kong/ title="Hong Kong">Hong Kong</a></li><li><a href=/posts/gallery/dublin/ title=Ireland>Ireland</a></li><li><a href=/posts/gallery/london/ title=London>London</a></li><li><a href=/posts/gallery/michigan/ title=Michigan>Michigan</a></li><li><a href=/posts/gallery/newcastle/ title=Newcastle>Newcastle</a></li><li><a href=/posts/gallery/zhuhai/ title=Zhuhai>Zhuhai</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/posts/projects/>Projects</a><ul class=active><li><a href=/posts/projects/cad2cav/ title=CAD2CAV>CAD2CAV</a></li><li><a class=active href=/posts/projects/565-cuda-path-tracer/ title="CUDA Path Tracer">CUDA Path Tracer</a></li><li><a href=/posts/projects/373proj/ title="EECS 373 Project">EECS 373 Project</a></li><li><a href=/posts/projects/560proj/ title="Mini Minecraft">Mini Minecraft</a></li><li><a href=/posts/projects/450proj/ title="SJTU Undergraduate Design Expo">SJTU Undergraduate Design Expo</a></li><li><a href=/posts/projects/467proj-slam/ title="SLAM Project">SLAM Project</a></li><li><a href=/posts/projects/565-final-project/ title="Volume Rendered ReSTIR">Volume Rendered ReSTIR</a></li><li><a href=/posts/projects/565-vulkan-grass-rendering/ title="Vulkan Grass Rendering">Vulkan Grass Rendering</a></li><li><i class="fas fa-plus-circle"></i><a href=/posts/projects/f1tenth-labs/>F1Tenth Course Labs</a><ul><li><a href=/posts/projects/f1tenth-labs/f1tenth-lab1/ title="F1Tenth Course Lab 1">F1Tenth Course Lab 1</a></li><li><a href=/posts/projects/f1tenth-labs/f1tenth-lab2/ title="F1Tenth Course Lab 2">F1Tenth Course Lab 2</a></li><li><a href=/posts/projects/f1tenth-labs/f1tenth-lab3/ title="F1Tenth Course Lab 3">F1Tenth Course Lab 3</a></li><li><a href=/posts/projects/f1tenth-labs/f1tenth-lab4/ title="F1Tenth Course Lab 4">F1Tenth Course Lab 4</a></li><li><a href=/posts/projects/f1tenth-labs/f1tenth-lab6/ title="F1Tenth Course Lab 6">F1Tenth Course Lab 6</a></li></ul></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/images/posts/565-cuda-path-tracer/cornell.2021-10-09_18-44-15z.5000samp.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/author/avatar_hufae261693e8da39e936529a400eb43d8_104998_120x120_fit_q75_box.jpg alt="Author Image"><h5 class=author-name>Zhihao (Ryan) Ruan</h5><p>Monday, December 27, 2021</p></div><div class=title><h1>CUDA Path Tracer with À-Trous Denoiser</h1></div><div class=post-content id=post-content><ul><li>Tested on: Ubuntu 20.04 LTS, Ryzen 3700X @ 2.22GHz 48GB, RTX 2060 Super @ 7976MB</li></ul><h2 id=cuda-path-tracer>CUDA Path Tracer</h2><p><img src=cornell.2021-10-09_18-44-15z.5000samp.png alt></p><h3 id=highlights>Highlights</h3><p>Finished path tracing core features:</p><ul><li>diffuse shaders</li><li>perfect specular reflection</li><li>1st-bounce ray intersection caching</li><li>radix sort by material type</li><li>path continuation/termination by Thrust stream compaction</li></ul><p>Finished Advanced Features:</p><ul><li>Refraction with Fresnel effects using Schlick&rsquo;s approximation</li><li>Stochastic sampled anti-aliasing</li><li>Physically-based depth of field</li><li>OBJ mesh loading with <a href=https://github.com/tinyobjloader/tinyobjloader>tinyobjloader</a></li></ul><h3 id=background-ray-tracing>Background: Ray Tracing</h3><p>Ray tracing is a technique commonly used in rendering. Essentially it mimics the actual physical behavior of light: shoots a ray from every pixel in an image and calculates the final color of the ray by bouncing it off on every surface it hits in the world, until it reaches the light source. In practice a maximum number of depth (bouncing times) would be specified, so that we would not have to deal with infinitely bouncing rays.</p><p>Ray tracing is one of the applications considered as &ldquo;embarrassingly parallel&rdquo;, given the fact that each ray is completely independent of other rays. Hence, it is best to run on an GPU which is capable of providing hundreds of thousands of threads for parallel algorithms. This project aims at developing a CUDA-based application for customized ray tracing, and a detailed instruction can be found <a href=INSTRUCTION.md>here</a>.</p><h4 id=bsdf-bidirectional-scattering-distribution-functions>BSDF: Bidirectional Scattering Distribution Functions</h4><p>BSDF is a collection of models that approximates the light behavior in real world. It is commonly known to consist of BRDF (Reflection) models and BTDF (Transmission) models. Some typical reflection models include:</p><ul><li><em>Ideal specular</em> (perfect mirror, <code>glm::reflect</code>)</li><li><em>Ideal diffuse.</em> It is a model that the direction of the reflected ray is randomly sampled from the incident hemisphere, along with other advanced sampling methods.</li><li><em>Microfacet,</em> such as <a href=https://media.disneyanimation.com/uploads/production/publication_asset/48/asset/s2012_pbs_disney_brdf_notes_v3.pdf>Disney model</a>.</li><li><em>Glossy specular.</em></li><li><em>Subsurface scattering.</em></li><li>&mldr;</li></ul><p>Some typical transmission models include:</p><ul><li><em>Fresnel effect refraction.</em> It consists of a regular transmission model based on <a href=https://en.wikipedia.org/wiki/Snell%27s_law>Snell&rsquo;s law</a> and a partially reflective model based on <a href=https://en.wikipedia.org/wiki/Fresnel_equations>Fresnel effect</a> and its <a href=https://en.wikipedia.org/wiki/Schlick%27s_approximation>Schlick&rsquo;s approximations</a>.</li><li>&mldr;</li></ul><h4 id=cuda-optimization-for-ray-tracing>CUDA Optimization for Ray Tracing</h4><p>In order to better utilize CUDA hardware for ray tracing, it is not suggested to parallelize each pixel, as it would lead to a huge amount of divergence.</p><p><img src=ray-tracing-divergence.png alt></p><p>Instead, one typical optimization people use is to parallelize each <em>ray</em>, and uses <strong>stream compaction</strong> to remove those rays that terminates early. By this means we could better recycle the early ending warps for ray tracing other pixels.</p><p><img src=ray-tracing-parallel-rays.png alt></p><h3 id=results-and-demos>Results and Demos</h3><h4 id=ray-refraction-for-glass-like-materials>Ray Refraction for Glass-like Materials</h4><table><thead><tr><th style=text-align:center>Perfect Specular Reflection</th><th style=text-align:center>Glass-like Refraction</th></tr></thead><tbody><tr><td style=text-align:center><img src=cornell.2021-10-04_02-10-06z.5000samp.png alt></td><td style=text-align:center><img src=cornell.2021-10-04_01-57-31z.5000samp.png alt></td></tr></tbody></table><h4 id=stochastic-sampled-anti-aliasing>Stochastic Sampled Anti-Aliasing</h4><table><thead><tr><th style=text-align:center>1x Anti-Aliasing (Feature OFF)</th><th style=text-align:center>4x Anti-Aliasing</th></tr></thead><tbody><tr><td style=text-align:center><img src=cornell.2021-10-04_01-14-01z.5000samp-antialias-1x.png alt></td><td style=text-align:center><img src=cornell.2021-10-04_01-07-24z.5000samp-antialias-4x.png alt></td></tr></tbody></table><h4 id=physically-based-depth-of-field>Physically-Based Depth of Field</h4><table><thead><tr><th style=text-align:center>Pinhole Camera Model (Feature OFF)</th><th style=text-align:center>Thin-Lens Camera Model</th></tr></thead><tbody><tr><td style=text-align:center><img src=cornell.2021-10-05_02-45-59z.5000samp.png alt></td><td style=text-align:center><img src=cornell.2021-10-05_02-40-08z.5000samp.png alt></td></tr></tbody></table><h4 id=mesh-loading>Mesh Loading</h4><p>Mesh loading has not been fully supported due to an incorrect normal vector parsing issue in <a href=https://github.com/tinyobjloader/tinyobjloader>tinyobjloader</a>. The runtime is also unoptimized and takes an unreasonable amount of time to run.</p><p>TODO:</p><ul><li>Bounding volume culling with AABB box/OBB box.</li></ul><h3 id=performance-analysis>Performance Analysis</h3><p>Throughout the project two optimizations were done:</p><ol><li><strong>Cache ray first bounce.</strong> For every iteration in a rendering process, the first rays that shoot from camera to the first hit surface is always the same. Therefore we can cache the first shooting ray during the first iteration and reuse the results in all the following iterations.</li><li><strong>Radix-sort hit attributes by material type.</strong> The calculation of resulting colors for each ray depends on the material of the object it hits, and thus we can sort the rays based on material type before shading to enforce CUDA memory coalescence.</li></ol><p><img src=sort-material-type.png alt></p><p>The following experiments are conducted on a Cornell box scene as shown in <a href=scenes/cornell_profiling.txt><code>cornell_profiling.txt</code></a> with varying iterations from 300 to 2000.</p><p><img src=performance-analysis.png alt></p><p>From the performance analysis we can see that caching the first bounce for the 1st iteration has a slight improvement on the performance, while radix-sorting the material type before shading have a great negative impact on the performance. This is possibly due to the reason that radix-sorting itself takes a lot amount of time in each iteration.</p><h2 id=cuda-denoiser>CUDA Denoiser</h2><h3 id=physically-based-ray-traced-pbrt-image-with-à-trous-denoising>Physically-Based Ray Traced (PBRT) Image with À-Trous Denoising</h3><table><thead><tr><th style=text-align:center>PBRT, 5000 iterations</th><th style=text-align:center>PBRT, 5000 iterations, 4x anti-aliasing</th></tr></thead><tbody><tr><td style=text-align:center><img src=cornell.2021-10-21_22-45-59z.5000samp.original.png alt></td><td style=text-align:center><img src=cornell.2021-10-21_22-47-57z.5000samp.original.png alt></td></tr></tbody></table><table><thead><tr><th style=text-align:center>PBRT, 100 iterations</th><th style=text-align:center>PBRT, 100 iterations, À-Trous Denoising</th></tr></thead><tbody><tr><td style=text-align:center><img src=cornell.2021-10-21_21-56-47z.100samp.original.png alt></td><td style=text-align:center><img src=cornell.2021-10-21_22-57-08z.100samp.denoised.png alt></td></tr></tbody></table><h3 id=highlights-1>Highlights</h3><p>Implemented <a href=https://jo.dreggn.org/home/2010_atrous.pdf>Edge-Avoiding À-Trous Wavelet Transform</a> denoising techniques with 5x5 Gaussian blur kernel.</p><h3 id=introduction>Introduction</h3><p>Physically-Based Ray Tracing (PBRT) is considered as one of the best methods that produce the most photorealistic images. The essence of PBRT is to approximate <a href=https://en.wikipedia.org/wiki/Rendering_equation>the rendering equation</a> with Monte-Carlo integration, sampling multiple rays from each pixel of an image and bouncing them off multiple times from various surfaces of different kinds and different textures according to the ray directions, accumulating the colors along the way.</p><p>However, in reality it is very hard to run PBRT in real time, as we often need a large amount of rays to obtain a reasonable good approximation for the rendering equation. Hence, people come up with multiple ways of applying denoising techniques on partially ray-traced images, hoping that with denoising we could get reasonably good photorealistic images while terminating PBRT early. In this project, we explored <a href=https://jo.dreggn.org/home/2010_atrous.pdf>Edge-Avoiding À-Trous Wavelet Transform</a> for image denoising. For more details, please checkout <a href=INSTRUCTION.md>the project instruction</a>.</p><p>The word &ldquo;À-Trous&rdquo; with meaning &ldquo;with holes&rdquo; comes from <a href=https://en.wikipedia.org/wiki/Stationary_wavelet_transform><em>Algorithme À-Trous</em></a>, which is a stationary wavelet transform commonly used in computer graphics to approximate the effect of a Gaussian filter with much faster performance. It starts from a fix-sized Gaussian filter, performs convolution on the image while expanding out each element of the filter at every iteration, filling all missing entries with 0s. The &ldquo;Edge-Avoiding&rdquo; part of the algorithm incorporates the use of a pixel-wise <em>GBuffer</em>, storing positions and normals of the first hit for each ray. When the image is denoised, information of the GBuffer will be used to avoid blurring edges in the image, while the noisy surfaces are smoothed.</p><p><img src=a-trous.png alt></p><table><thead><tr><th style=text-align:center>Pure À-Trous Filtering</th><th style=text-align:center>À-Trous Filtering with Edge-Avoiding GBuffer</th></tr></thead><tbody><tr><td style=text-align:center><img src=cornell.2021-10-22_00-46-51z.100samp.denoised.png alt></td><td style=text-align:center><img src=cornell.2021-10-22_00-47-47z.100samp.denoised.png alt></td></tr></tbody></table><h3 id=qualitative-analysis>Qualitative Analysis</h3><h4 id=visual-results-vs-filter-size>Visual Results vs. Filter Size</h4><p>The following experiments are run with <code>c_phi=132.353, n_phi=0.245, p_phi=1.324</code>.</p><table><thead><tr><th style=text-align:center>Filter Size = 6</th><th style=text-align:center>Filter Size = 15</th><th style=text-align:center>Filter Size = 32</th><th style=text-align:center>Filter Size = 100</th></tr></thead><tbody><tr><td style=text-align:center><img src=denoise.filtersize.6.png alt></td><td style=text-align:center><img src=denoise.filtersize.15.png alt></td><td style=text-align:center><img src=denoise.filtersize.32.png alt></td><td style=text-align:center><img src=denoise.filtersize.100.png alt></td></tr></tbody></table><p>From the results we can see that the visual results does not vary uniformly with the filter size. When the filter reaches some size threshold, it is no longer the filter size that stops the smoothing process but the weights of the GBuffer instead.</p><h4 id=visual-results-vs-material-type>Visual Results vs. Material Type</h4><p>The following experiments are run with <code>c_phi=132.353, n_phi=0.245, p_phi=1.324, filter_size=100</code>.</p><table><thead><tr><th style=text-align:center>Diffuse</th><th style=text-align:center>Reflective</th><th style=text-align:center>Refractive</th></tr></thead><tbody><tr><td style=text-align:center><img src=denoise-diffuse.png alt></td><td style=text-align:center><img src=denoise-specular.png alt></td><td style=text-align:center><img src=denoise-refraction.png alt></td></tr></tbody></table><p>From the results we can see that the filter works best with diffuse materials, reasonably well with refractive materials, and the worst with reflective materials. This is mainly because the current implementation only caches the position & normal vectors of the first hit, while this property does not really apply to reflective materials (colors on a pure-reflective material depend heavily on the material properties from the 2nd hit). As a result, the filter is blurring the reflection on the reflective materials.</p><h4 id=visual-results-vs-light-conditions>Visual Results vs. Light Conditions</h4><p>The following experiments are run with <code>c_phi=132.353, n_phi=0.245, p_phi=1.324, filter_size=100</code>.</p><table><thead><tr><th style=text-align:center>Cornell Box</th><th style=text-align:center>Cornell Box with Large Lights</th></tr></thead><tbody><tr><td style=text-align:center><img src=denoise-cornell.png alt></td><td style=text-align:center><img src=denoise-cornell-ceiling-light.png alt></td></tr></tbody></table><p>From the results we can see that the filter works better in brighter lighting conditions. This is because in brighter lighting conditions the color tends to be more similar locally, while with point lights the color differs much from its adjacent pixels, making it more difficult to smooth.</p><h3 id=quantitative-analysis>Quantitative Analysis</h3><h4 id=denoising-time>Denoising Time</h4><p>For a standard scene as shown in <a href=#physically-based-ray-traced-pbrt-image-with-%C3%A0-trous-denoising>Physically-Based Ray Traced (PBRT) Image with À-Trous Denoising</a>, the denoising time for a 800x800 image with <code>c_phi=132.353, n_phi=0.245, p_phi=1.324, filter_size=100</code> is:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>----- Begin image denoising -----
</span></span><span style=display:flex><span>   elapsed time: 31.2983ms    <span style=color:#f92672>(</span>CUDA Measured<span style=color:#f92672>)</span>
</span></span></code></pre></div><p>which is a reasonably fast denoising time.</p><h4 id=number-of-iterations-needed-for-a-smooth-image>Number of Iterations Needed for a Smooth Image</h4><p>The following experiments are run with <code>c_phi=132.353, n_phi=0.245, p_phi=1.324, filter_size=100</code>.</p><table><thead><tr><th style=text-align:center>Iteration = 1</th><th style=text-align:center>Iteration = 10</th><th style=text-align:center>Iteration = 25</th></tr></thead><tbody><tr><td style=text-align:center><img src=denoise-iter-1.png alt></td><td style=text-align:center><img src=denoise-iter-10.png alt></td><td style=text-align:center><img src=denoise-iter-25.png alt></td></tr></tbody></table><table><thead><tr><th style=text-align:center>Iteration = 50</th><th style=text-align:center>Iteration = 100</th></tr></thead><tbody><tr><td style=text-align:center><img src=denoise-iter-50.png alt></td><td style=text-align:center><img src=denoise-iter-100.png alt></td></tr></tbody></table><p>From the results we can see that subjectively, with the parameters specified above, we can get a reasonably smooth image with number of iterations at least 50.</p><h4 id=denoising-runtime-vs-resolution-filter-size>Denoising Runtime vs. Resolution, Filter Size</h4><p>The following experiments are run with <code>c_phi=132.353, n_phi=0.245, p_phi=1.324</code>. For varying resolution, <code>filter_size = 100</code>; for varying filter size, <code>resolution = 800x800</code>.</p><table><thead><tr><th style=text-align:center>Runtime vs. Resolution</th><th style=text-align:center>Runtime vs. Filter Size</th></tr></thead><tbody><tr><td style=text-align:center><img src=denoise_runtime_resolution.png alt></td><td style=text-align:center><img src=denoise_runtime_filter_size.png alt></td></tr></tbody></table><p>From the results we can see that the runtime increases quadratically with resolution as the number of pixels increases quadratically with resolution; the runtime increases linearly with filter size, as increasing filter size would only increase the number of iterations that À-Trous wavelet transform needs to run.</p><h2 id=references>References</h2><ul><li>[PBRT] Physically Based Rendering, Second Edition: From Theory To Implementation. Pharr, Matt and Humphreys, Greg. 2010.</li><li>Antialiasing and Raytracing. Chris Cooksey and Paul Bourke, <a href=http://paulbourke.net/miscellaneous/aliasing/>http://paulbourke.net/miscellaneous/aliasing/</a></li><li><a href=http://graphics.ucsd.edu/courses/cse168_s14/>Sampling notes</a> from Steve Rotenberg and Matteo Mannino, University of California, San Diego, CSE168: Rendering Algorithms</li><li>Path Tracer Readme Samples (non-exhaustive list):<ul><li><a href=https://github.com/byumjin/Project3-CUDA-Path-Tracer>https://github.com/byumjin/Project3-CUDA-Path-Tracer</a></li><li><a href=https://github.com/lukedan/Project3-CUDA-Path-Tracer>https://github.com/lukedan/Project3-CUDA-Path-Tracer</a></li><li><a href=https://github.com/botforge/CUDA-Path-Tracer>https://github.com/botforge/CUDA-Path-Tracer</a></li><li><a href=https://github.com/taylornelms15/Project3-CUDA-Path-Tracer>https://github.com/taylornelms15/Project3-CUDA-Path-Tracer</a></li><li><a href=https://github.com/emily-vo/cuda-pathtrace>https://github.com/emily-vo/cuda-pathtrace</a></li><li><a href=https://github.com/ascn/toki>https://github.com/ascn/toki</a></li><li><a href=https://github.com/gracelgilbert/Project3-CUDA-Path-Tracer>https://github.com/gracelgilbert/Project3-CUDA-Path-Tracer</a></li><li><a href=https://github.com/vasumahesh1/Project3-CUDA-Path-Tracer>https://github.com/vasumahesh1/Project3-CUDA-Path-Tracer</a></li></ul></li><li><a href=https://jo.dreggn.org/home/2010_atrous.pdf>Edge-Avoiding A-Trous Wavelet Transform for fast Global Illumination Filtering</a></li><li><a href=https://research.nvidia.com/publication/2017-07_Spatiotemporal-Variance-Guided-Filtering%3A>Spatiotemporal Variance-Guided Filtering</a></li><li><a href=http://jcgt.org/published/0003/02/01/paper.pdf>A Survey of Efficient Representations for Independent Unit Vectors</a></li><li>ocornut/imgui - <a href=https://github.com/ocornut/imgui>https://github.com/ocornut/imgui</a></li></ul></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div><div class="col-md-6 btn-improve-page"><a href=https://github.com/shineyruan/shineyruan.github.io/edit/main/content/posts/Projects/565-cuda-path-tracer/index.md title="Improve this page" target=_blank rel=noopener><i class="fas fa-code-branch"></i>
Improve this page</a></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/posts/projects/cad2cav/ title="CAD2CAV: Computer Aided Design for Cooperative Autonomous Vehicles" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>CAD2CAV: Computer Aided Design for Cooperative Autonomous Vehicles</div></a></div><div class="col-md-6 next-article"><a href=/posts/projects/373proj/ title="An Interactive Game — Step On the White Tiles!" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>An Interactive Game — Step On the White Tiles!</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#cuda-path-tracer>CUDA Path Tracer</a><ul><li><a href=#highlights>Highlights</a></li><li><a href=#background-ray-tracing>Background: Ray Tracing</a><ul><li><a href=#bsdf-bidirectional-scattering-distribution-functions>BSDF: Bidirectional Scattering Distribution Functions</a></li><li><a href=#cuda-optimization-for-ray-tracing>CUDA Optimization for Ray Tracing</a></li></ul></li><li><a href=#results-and-demos>Results and Demos</a><ul><li><a href=#ray-refraction-for-glass-like-materials>Ray Refraction for Glass-like Materials</a></li><li><a href=#stochastic-sampled-anti-aliasing>Stochastic Sampled Anti-Aliasing</a></li><li><a href=#physically-based-depth-of-field>Physically-Based Depth of Field</a></li><li><a href=#mesh-loading>Mesh Loading</a></li></ul></li><li><a href=#performance-analysis>Performance Analysis</a></li></ul></li><li><a href=#cuda-denoiser>CUDA Denoiser</a><ul><li><a href=#physically-based-ray-traced-pbrt-image-with-à-trous-denoising>Physically-Based Ray Traced (PBRT) Image with À-Trous Denoising</a></li><li><a href=#highlights-1>Highlights</a></li><li><a href=#introduction>Introduction</a></li><li><a href=#qualitative-analysis>Qualitative Analysis</a><ul><li><a href=#visual-results-vs-filter-size>Visual Results vs. Filter Size</a></li><li><a href=#visual-results-vs-material-type>Visual Results vs. Material Type</a></li><li><a href=#visual-results-vs-light-conditions>Visual Results vs. Light Conditions</a></li></ul></li><li><a href=#quantitative-analysis>Quantitative Analysis</a><ul><li><a href=#denoising-time>Denoising Time</a></li><li><a href=#number-of-iterations-needed-for-a-smooth-image>Number of Iterations Needed for a Smooth Image</a></li><li><a href=#denoising-runtime-vs-resolution-filter-size>Denoising Runtime vs. Resolution, Filter Size</a></li></ul></li></ul></li><li><a href=#references>References</a></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://zhihaoruan.xyz#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://zhihaoruan.xyz#skills>Skills</a></li><li class=nav-item><a class=smooth-scroll href=https://zhihaoruan.xyz#experiences>Experiences</a></li><li class=nav-item><a class=smooth-scroll href=https://zhihaoruan.xyz#education>Education</a></li><li class=nav-item><a class=smooth-scroll href=https://zhihaoruan.xyz#projects>Projects</a></li><li class=nav-item><a class=smooth-scroll href=https://zhihaoruan.xyz#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:shineyruan@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>shineyruan@gmail.com</span></a></li><li><a href=https://github.com/shineyruan target=_blank rel=noopener><span><i class="fab fa-github"></i></span> <span>shineyruan</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hugo-toha/toha target=_blank rel=noopener><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2018 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=/js/jquery-3.4.1.min.js></script>
<script type=text/javascript src=/js/popper.min.js></script>
<script type=text/javascript src=/js/bootstrap.min.js></script>
<script type=text/javascript src=/js/navbar.js></script>
<script type=text/javascript src=/js/plyr.js></script>
<script type=text/javascript src=/js/main.js></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script>
<script src=/js/single.js></script>
<script>hljs.initHighlightingOnLoad()</script></body></html>